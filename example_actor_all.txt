========================= INSTRUCTION =========================

I have graph, it is saved on the dataset file: actor, 
which is an actor-only induced subgraph of the film-director-actor-writer network. 
In this graph-based dataset, nodes represent actors, and edges represent 
the co-occurrence of two actors on the same Wikipedia page. 
Node features correspond to keywords extracted from the actors' Wikipedia pages. 
The goal is to use graph neural networks to classify nodes into one of five categories 
based on the words present in an actor's Wikipedia page.

========================= INSTRUCTION END =========================


========================= MANAGER AGENT =========================

{'Learning_tasks_on_graph': 'Node-level', 'Learning_task_types': 'Classification', 'Evaluation_metric': 'Accuracy', 'Preference': 'None', 'Data': 'actor'}

========================= MANAGER AGENT END =========================


========================= KNOWLEDGE AGENT =========================

[Loaded existing prior knowledge] Loaded 376 files into cache overall.
[Loaded existing experiment knowledge] Loaded 53 existing experiment knowledge base files into cache.

========================= KNOWLEDGE AGENT END =========================


========================= DATA AGENT =========================

Data Analysis: 
{
    "num_nodes": 7600,
    "num_edges": 33544,
    "num_node_features": 931,
    "num_classes": 5,
    "avg_node_degree": 3.9498684406280518,
    "max_node_degree": 73.0,
    "min_node_degree": 0.0,
    "diameter": 12,
    "graph_density": 0.0010395758444670697,
    "avg_shortest_path_length": 4.109739162049862
}

Information from PyG document:
 Below are the names and descriptions of all feature engineering functions:
name: OneHotDegree and description: Adds the node degree as one hot encodings to the node features (functional name: one_hot_degree).
name: LineGraph and description: Converts a graph to its corresponding line-graph (functional name: line_graph).
name: VirtualNode and description: Appends a virtual node to the given homogeneous graph that is connected to all other nodes, as described in the "Neural Message Passing for Quantum Chemistry" paper (functional name: virtual_node).
name: TwoHop and description: Adds the two hop edges to the edge indices (functional name: two_hop).
name: AddRandomMetaPaths and description: Adds additional edge types similar to AddMetaPaths.
name: AddMetaPaths and description: Adds additional edge types to a HeteroData object between the source node type and the destination node type of a given metapath, as described in the "Heterogenous Graph Attention Networks" paper (functional name: add_metapaths).
name: AddRemainingSelfLoops and description: Adds remaining self-loops to the given homogeneous or heterogeneous graph (functional name: add_remaining_self_loops).
name: RemoveIsolatedNodes and description: Removes isolated nodes from the graph (functional name: remove_isolated_nodes).
name: LocalDegreeProfile and description: Appends the Local Degree Profile (LDP) from the "A Simple yet Effective Baseline for Non-attribute Graph Classification" paper (functional name: local_degree_profile).
name: RemoveSelfLoops and description: Removes all self-loops in the given homogeneous or heterogeneous graph (functional name: remove_self_loops).
name: AddRandomWalkPE and description: Adds the random walk positional encoding from the "Graph Neural Networks with Learnable Structural and Positional Representations" paper to the given graph (functional name: add_random_walk_pe).
name: TargetIndegree and description: Saves the globally normalized degree of target nodes (functional name: target_indegree).
name: KNNGraph and description: Creates a k-NN graph based on node positions data.pos (functional name: knn_graph).
name: AddSelfLoops and description: Adds self-loops to the given homogeneous or heterogeneous graph (functional name: add_self_loops).
name: GCNNorm and description: Applies the GCN normalization from the "Semi-supervised Classification with Graph Convolutional Networks" paper (functional name: gcn_norm).
name: LaplacianLambdaMax and description: Computes the highest eigenvalue of the graph Laplacian given by torch_geometric.utils.get_laplacian() (functional name: laplacian_lambda_max).
name: ToUndirected and description: Converts a homogeneous or heterogeneous graph to an undirected graph such that \((j,i) \in \mathcal{E}\) for every edge \((i,j) \in \mathcal{E}\) (functional name: to_undirected).
name: ToDense and description: Converts a sparse adjacency matrix to a dense adjacency matrix with shape [num_nodes, num_nodes, *] (functional name: to_dense).
name: RootedEgoNets and description: Collects rooted \(k\)-hop EgoNets for each node in the graph, as described in the "From Stars to Subgraphs: Uplifting Any GNN with Local Structure Awareness" paper.
name: LargestConnectedComponents and description: Selects the subgraph that corresponds to the largest connected components in the graph (functional name: largest_connected_components).
name: GDC and description: Processes the graph via Graph Diffusion Convolution (GDC) from the "Diffusion Improves Graph Learning" paper (functional name: gdc).
name: RootedRWSubgraph and description: Collects rooted random-walk based subgraphs for each node in the graph, as described in the "From Stars to Subgraphs: Uplifting Any GNN with Local Structure Awareness" paper.
name: RemoveDuplicatedEdges and description: Removes duplicated edges from a given homogeneous or heterogeneous graph.
name: RadiusGraph and description: Creates edges based on node positions data.pos to all points within a given distance (functional name: radius_graph).
name: SIGN and description: The Scalable Inception Graph Neural Network module (SIGN) from the "SIGN: Scalable Inception Graph Neural Networks" paper (functional name: sign), which precomputes the fixed representations.
name: HalfHop and description: The graph upsampling augmentation from the "Half-Hop: A Graph Upsampling Approach for Slowing Down Message Passing" paper.
name: FeaturePropagation and description: The feature propagation operator from the "On the Unreasonable Effectiveness of Feature propagation in Learning on Graphs with Missing Node Features" paper (functional name: feature_propagation).
name: AddLaplacianEigenvectorPE and description: Adds the Laplacian eigenvector positional encoding from the "Benchmarking Graph Neural Networks" paper to the given graph (functional name: add_laplacian_eigenvector_pe).

Cases retrieved from prior knowledge base:
{
    "Task Description": "Leaderboards for Node Property Prediction",
    "Dataset Name": "ogbn-mag",
    "Method": "LEGNN",
    "Test Accuracy": "0.5276 \u00b1 0.0014",
    "Validation Accuracy": "0.5443 \u00b1 0.0009",
    "Parameters": "5,147,997",
    "Hardware": "NVIDIA Tesla T4 (15 GB)",
    "Paper Summary": "The paper presents the **Label-Enhanced Graph Neural Network (LEGNN)** designed specifically for semi-supervised node classification tasks in graphs. The authors aimed to better leverage the rich information contained in label data to improve model performance significantly.\n\n### Key Methodological Aspects:\n\n1. **Heterogeneous Graph Construction**:\n   - The framework creates a heterogeneous graph by integrating label nodes alongside the existing nodes. \n   - Each label acts as a virtual center for the nodes associated with it, creating connections among intra-class nodes to strengthen their relationships.\n   - An adjacency matrix is enhanced by establishing connections based on the label matrix, thus fostering more inter-class interaction.\n\n2. **Heterogeneous Message Passing**:\n   - A unique message-passing mechanism is introduced that differentiates between nodes and label types, allowing for simultaneous learning of node and label representations.\n   - The node feature matrix \\(X\\) is aligned with a label feature matrix \\(E\\) to ensure effective interactions during the learning phase.\n   - The process involves computing representations for both nodes and labels using layer-specific transformations, allowing the model to learn intricate dependencies and semantics associated with labels.\n\n3. **Training Node Selection**:\n   - To mitigate the label leakage problem, the training methodology selectively connects labeled nodes with their corresponding labels without establishing these connections for all labeled nodes altogether.\n   - A pre-defined selection rate is utilized to randomly choose a subset of labeled nodes to prevent trivial label prediction and improve generalization.\n\n4. **Adaptive Self-Training Strategy**:\n   - An adaptive self-training approach is designed to iteratively augment the training set with pseudo-labeled nodes deemed reliable based on their predicted probabilities.\n   - The strategy quantifies the reliability of pseudo labels through a training confidence measure, which assesses the model's performance on the training set over epochs.\n   - Pseudo-labeled nodes with higher probabilities and training confidence are prioritized, while the importance of each pseudo-labeled node is assessed to optimize its contribution during model training.\n\n5. **Model Learning Process**:\n   - Predictions are made via a softmax function applied to the derived representations from both heterogeneous nodes and labels.\n   - The training process utilizes a loss function that incorporates both labeled and pseudo-labeled data, thereby enhancing the model's learning capacity in a semi-supervised context.\n\nThrough these design choices, LEGNN aims to capitalize on the extensive information encapsulated in labels to bolster the performance of GNNs in node classification tasks, ensuring that the representations of nodes in the same class are smooth and coherent while maintaining interpretable label semantics."
}

Cases retrieved from prior knowledge base:
{
    "Task Description": "Leaderboards for Node Property Prediction",
    "Dataset Name": "ogbn-arxiv",
    "Method": "SimTeG+TAPE+RevGAT",
    "Test Accuracy": "0.7803 \u00b1 0.0007",
    "Validation Accuracy": "0.7846 \u00b1 0.0004",
    "Parameters": "1,386,219,488",
    "Hardware": "4 * A100-XMS4 (40GB GPU)",
    "Paper Summary": "The paper presents SimTeG, a novel approach for Textual Graph (TG) representation learning, designed to enhance the integration of language models (LMs) with graph neural networks (GNNs). The core methodology comprises two main stages:\n\n1. **Parameter-Efficient Fine-Tuning (PEFT) of the Language Model**: \n   - The process begins with the supervised fine-tuning of a pre-trained LM on the textual data of a TG, where task-specific labels guide the training. This is done to effectively adapt the LM to the downstream task, such as node classification. \n   - The fine-tuned LM generates node embeddings from its last hidden states, providing features that represent the text associated with each node in the graph.\n\n2. **Graph Neural Network Training**:\n   - The next stage involves training a GNN, such as GraphSAGE, using the embeddings produced from the fine-tuned LM, along with the graph's structural information. The GNN further processes these embeddings for the same downstream task, essentially integrating the textual features captured from the LM with the graph's structure.\n\nThe paper emphasizes the simplicity of this approach, as it does not introduce complex models or architectures but rather effectively utilizes existing frameworks. The design flexibility allows the usage of any combination of LMs and GNNs, enabling customization based on specific dataset or task needs. Additionally, the method copes with the prevalent issue of overfitting in LMs by employing PEFT, which mitigates overfitting and helps maintain a well-regularized feature space for the subsequent GNN training.\n\nOverall, SimTeG's methodology underscores the significance of leveraging LMs for generating robust textual embeddings that can be effectively utilized by GNNs to improve performance on textual graph-related tasks."
}

Cases retrieved from prior knowledge base:
{
    "name": "GeneralConv",
    "description": "A general GNN layer adapted from the \"Design Space for Graph Neural Networks\" paper.",
    "link": "../generated/torch_geometric.nn.conv.GeneralConv.html#torch_geometric.nn.conv.GeneralConv",
    "paper_link": "https://arxiv.org/abs/2011.08843",
    "paper_name": "\"Design Space for Graph Neural Networks\"",
    "Model design and experimental setup": "{    \"GNN_Design\": {        \"agg_ops\": \"AGG \u2208 {MEAN, MAX, SUM}, with SUM found most empirically successful.\",        \"skip_connections\": \"Two types: residual connections (SKIP-SUM) and dense connections (SKIP-CAT).\",        \"layer_info_fusion\": \"Layers are modular with options for pre/post-process using Multilayer Perceptron (MLP) layers.\",        \"num_layers\": \"Message passing layers considered: {2, 4, 6, 8}.\",        \"hyperparameters\": \"Batch size \u2208 {16, 32, 64}, learning rate \u2208 {0.1, 0.01, 0.001}, optimizers: SGD and ADAM, training epochs \u2208 {100, 200, 400}.\",        \"activation\": \"ACT \u2208 {RELU, PRELU, SWISH}, with PRELU as the most favorable choice.\"    },    \"Experimental_Setup\": {        \"datasets\": [            \"12 synthetic node classification, 8 synthetic graph classification, 6 real-world node classification, 6 real-world graph classification\"        ],        \"dataset_summary\": \"Synthetic tasks generated with small-world and scale-free graphs, covering a range of graph statistics and feature/label combinations. Real-world tasks span citation networks and bioinformatics datasets.\",        \"baseline\": \"Standard GNN models like GCN with different numbers of message passing layers.\",        \"performance_comparisons\": \"Designs from the proposed GNN design space showed superiority over standard GCNs in 24 out of 32 tasks evaluated, achieving state-of-the-art in new, challenging tasks.\"    }}",
    "Paper Summary": "The paper \"Design Space for Graph Neural Networks\" explores a systematic and scalable framework for designing and evaluating Graph Neural Networks (GNNs). Here\u2019s a summary of the methods focused on model design aspects:\n\n### GNN Design Space\n\nThe authors propose a general design space for GNNs that is characterized by three principal components:\n\n1. **Intra-layer Design**: This aspect involves various design dimensions applicable within individual layers of GNNs, including:\n   - **Batch Normalization (BN)**: Whether to include BN (True/False).\n   - **Dropout**: A dropout rate which can vary from no drop to significant drop rates (0.3, 0.6).\n   - **Activation Functions**: The choice among activation functions such as ReLU, PReLU, and Swish.\n   - **Aggregation Functions**: Different methods to combine inputs from neighbors, including Mean, Max, and Sum.\n\n   Each GNN layer is mathematically expressed, and ranges are specified for each dimension.\n\n2. **Inter-layer Design**: This dimension discusses how multiple layers are organized:\n   - **Layer Connectivity**: Options for stacking layers or using skip connections (e.g., SKIP-SUM, SKIP-CAT).\n   - **Number of Layers**: Choices for how many message-passing layers to include (e.g., 2, 4, 6, 8).\n   - **Pre-processing and Post-processing Layers**: This includes adding layers before and after the main message-passing layers.\n\n3. **Training Configurations**: Different configurations for training the GNN are addressed, such as:\n   - **Batch Size**: Values such as 16, 32, or 64.\n   - **Learning Rate**: Ranges from 0.1 to 0.001.\n   - **Optimizers**: Including SGD and ADAM.\n   - **Training Epochs**: Range from 100 to 400 epochs.\n\nOverall, the proposed GNN design space encompasses 12 design dimensions, leading to a combinatorial possibility of 315,000 unique designs.\n\n### GNN Task Space\n\nAdditionally, the authors present a GNN task space inspired by the relationship between diverse tasks. A task similarity metric is developed to evaluate how different tasks relate to one another. This allows for:\n- **Identification of Similar Tasks**: The similarity between tasks is quantified using Kendall's rank correlation on the performance of various fixed GNN architectures across those tasks.\n- **Transferability of Designs**: This metric helps determine which GNN designs can be effectively transferred between similar tasks.\n\n### Design Space Evaluation\n\nWith an extensive design and task space, the authors introduce a controlled random search evaluation method to gain insights into the various design choices without exhaustively searching through all combinations. This method is crucial due to the combinatorial explosion of design-task combinations.\n\nSignificantly, this evaluation aims to comprehend design trade-offs by exploring a substantial portion of the design space efficiently. The controlled random search allows researchers to gather insights on specific questions like the usefulness of batch normalization in GNNs by controlling for other variables.\n\n### GraphGym Platform\n\nThe framework culminates in a platform called **GraphGym**, which is modularized to facilitate exploration of different GNN designs and tasks. It supports:\n- **Modular Implementation**: Users can modify or add new design dimensions.\n- **Standardized Evaluation**: A consistent evaluation pipeline exists for GNNs that allows researchers to select datasets, metrics, and how to report performance.\n- **Reproducibility and Scalability**: Experiments are described in configuration files to promote reproducibility and ease in comparing results across different models and tasks.\n\nThis systematic approach aims to provide guidelines that can help researchers design better GNN architectures and understand task interrelationships, ultimately improving the efficiency of GNN development for various applications."
}

Cases retrieved from prior knowledge base:
{
    "name": "GatedGraphConv",
    "description": "The gated graph convolution operator from the \"Gated Graph Sequence Neural Networks\" paper.",
    "link": "../generated/torch_geometric.nn.conv.GatedGraphConv.html#torch_geometric.nn.conv.GatedGraphConv",
    "paper_link": "https://arxiv.org/abs/1511.05493",
    "paper_name": "\"Gated Graph Sequence Neural Networks\"",
    "Model design and experimental setup": "{    \"GNN_Design\": {        \"agg_ops\": \"The model uses a propagation step with a gated recurrence model using Gated Recurrent Units (GRUs). Aggregation is performed using edge-specific transformations in a matrix form that is structured to handle directed graph edges and edge types.\",        \"skip_connections\": \"No explicit mention of skip connections in the GRU propagation model, which inherently manages state updates.\",        \"layer_info_fusion\": \"Information is propagated across layers using matrix multiplications and GRU-style updates, enabling information aggregation across nodes influenced by edge connections.\",        \"num_layers\": \"The model unfolds the recurrence for a fixed number of T steps, treating each step as a layer in its propagation model.\",        \"hyperparameters\": \"Parameter tying is used based on edge types. The model's specific layer dimensions such as node vector size (D) are set differently based on the task.\",        \"activation\": \"Uses tanh and sigmoid activation functions in the GRU update steps to manage information flow and gating mechanisms.\"    },    \"Experimental_Setup\": {        \"datasets\": \"Includes bAbI tasks, generated datasets for shortest paths, Eulerian circuits, and synthetic datasets for program verification.\",        \"dataset_summary\": \"bAbI tasks are designed for AI reasoning. The shortest path and Eulerian circuit tasks involve graph algorithm learning. The program verification data involves graphs representing program heap states.\",        \"baseline\": \"RNN and LSTM models are baseline comparisons. Previously developed models for program verification are also compared.\",        \"performance_comparisons\": \"GGS-NNs performs well on bAbI tasks, achieving state-of-the-art performance especially in tasks like Path Finding (Task 19). It outperforms RNNs and LSTMs in both accuracy and required training instances on sequential graph tasks.\"    }}",
    "Paper Summary": "The paper \"Gated Graph Sequence Neural Networks\" presents a novel architecture designed for learning from graph-structured data. The focus of the methods discussed revolves around the design and extension of Graph Neural Networks (GNNs) to create Gated Graph Neural Networks (GG-NNs) and Gated Graph Sequence Neural Networks (GGS-NNs). Below is a summary of the key aspects of model design discussed in the paper.\n\n### 1. Graph Neural Network (GNN) Framework\n- **Graph Representation**: GNNs are constructed to operate on graph structures represented by directed graphs \\( G = (V, E) \\), where \\( V \\) consists of nodes and \\( E \\) represents edges between nodes.\n- **Node Representations**: Each node \\( v \\) has an associated vector \\( h_v \\) that serves as its representation or embedding.\n- **Propagation Step**: A core part of GNNs involves iteratively updating node representations through a recurrence relation:\n  \\[\n  h_v(t) = f^*(l_v, l_{CO(v)}, l_{NBR(v)}, h_{NBR(v)}(t-1))\n  \\]\n  This function \\( f^* \\) synthesizes information from the labels and prior states of connected nodes to derive the new state.\n\n### 2. Gated Graph Neural Networks (GG-NNs)\n- **Integration of Gated Units**: GG-NNs incorporate Gated Recurrent Units (GRUs) into the propagation step, enabling the model to effectively capture sequential dependencies.\n- **Initialization of Node Representations**: Node representations begin by including node annotations to highlight importance or specificity for certain nodes.\n- **Parameter Sharing and Sparsity**: The model utilizes a structured sparsity framework in updating nodes, ensuring that parameters dynamically adapt based on the structure of connections (edges) in the graph.\n\n### 3. Gated Graph Sequence Neural Networks (GGS-NNs)\n- **Sequential Predictions**: The GGS-NN architecture extends GG-NNs to handle sequential outputs, allowing multiple GG-NNs to operate in succession:\n  - Two types of networks are utilized: \\( F^o(k) \\) for output prediction and \\( F^X(k) \\) for carrying forward the state representation to subsequent steps.\n- **Output Model**: The output model predicts an output vector for each node based on its hidden representation and the associated node annotations.\n- **Effects of Node Annotations**: Node annotations continuously influence the learning and representation process throughout the sequence, adapting the internal state in relation to previously produced outputs or indicators of node relevance.\n\n### 4. Training and Learning\n- **Training Settings**: There are two major approaches to training GGS-NNs:\n  1. **Specifying Intermediate Annotations**: This uses domain-specific knowledge to provide intermediate outputs to the model.\n  2. **End-to-End Training**: Here, the model learns to optimize the whole process without any pre-defined annotations, effectively learning from the raw input graph and the target sequence.\n\n### Conclusions\nThe paper emphasizes that the GGS-NN architecture unifies the strengths of graph-based learning with sequence prediction capabilities, marking a significant advance in the field of machine learning applied to graph-structured data. The design integrates structured propagation models and the use of node annotations to maintain and enhance the information flow across the network. Overall, the innovations in model design presented serve as a foundation for further applications in various domains involving complex data representations."
}

Cases retrieved from prior knowledge base:
{
    "Task Description": "Leaderboards for Link Property Prediction",
    "Dataset Name": "ogbl-ddi",
    "Method": "GIDN@YITU",
    "Test Accuracy": "0.9542 \u00b1 0.0000",
    "Validation Accuracy": "0.8258 \u00b1 0.0000",
    "Parameters": "3,506,691",
    "Hardware": "DepGraph@SCTS/CGCL",
    "Paper Summary": "The paper introduces a novel model called the **Graph Inception Diffusion Network (GIDN)**, designed for efficient link prediction in knowledge graphs. The core idea behind GIDN is to enhance graph diffusion processes across various feature spaces while minimizing computational complexity.\n\n### Model Design Aspects:\n\n1. **Graph Diffusion Mechanism:**\n   - GIDN encapsulates graph diffusion to represent information about nodes' neighborhoods more effectively than the original graph structure.\n   - It uses a matrix representation for each target prediction, capturing the proximity information of nodes.\n   - The model incorporates small-hop nodes and learnable generalized weighting coefficients, allowing for multi-layer generalized graph diffusion in diverse feature spaces.\n\n2. **Inception Module:**\n   - To address the computational challenges of deeper models, GIDN utilizes the inception module. \n   - This module allows the model to capture a rich set of features without the excessive computation typically associated with deeper networks.\n   - It enhances adaptability for training with larger datasets while managing computational resources efficiently.\n\n3. **Data Augmentation:**\n   - The method also includes data augmentation strategies focusing on nodes and edges within the graph structure.\n   - A notable technique involves using random walks to remove edges between nodes with differing labels while forming connections among nodes with the same labels.\n\n4. **Computational Efficiency:**\n   - GIDN is designed to strike a balance between accuracy and computational speed, leveraging lower complexity in its calculations, which is vital for handling large-scale knowledge graphs.\n\nOverall, GIDN's architecture emphasizes efficient computation and effective feature extraction while maintaining the ability to model complex relationships in knowledge graphs."
}

 [Data Agent Output]
The Feature Engineering Selected:
 ['GCNNorm', 'AddSelfLoops', 'RemoveIsolatedNodes']

========================= DATA AGENT END =========================


========================= CONFIGURATION AGENT =========================

Information from PyG document:
 Below are the names and descriptions of all Aggregation functions, which play an important role in the message passing framework of Graph Neural Networks:
name: SignedConv and description: The signed graph convolutional operator from the "Signed Graph Convolutional Network" paper.
name: ClusterGCNConv and description: The ClusterGCN graph convolutional operator from the "Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks" paper.
name: GATConv and description: The graph attentional operator from the "Graph Attention Networks" paper.
name: DirGNNConv and description: A generic wrapper for computing graph convolution on directed graphs as described in the "Edge Directionality Improves Learning on Heterophilic Graphs" paper.
name: NNConv and description: The continuous kernel-based convolutional operator from the "Neural Message Passing for Quantum Chemistry" paper.
name: GeneralConv and description: A general GNN layer adapted from the "Design Space for Graph Neural Networks" paper.
name: DNAConv and description: The dynamic neighborhood aggregation operator from the "Just Jump: Towards Dynamic Neighborhood Aggregation in Graph Neural Networks" paper.
name: GCN2Conv and description: The graph convolutional operator with initial residual connections and identity mapping (GCNII) from the "Simple and Deep Graph Convolutional Networks" paper.
name: GATv2Conv and description: The GATv2 operator from the "How Attentive are Graph Attention Networks?" paper, which fixes the static attention problem of the standard GATConv layer.
name: CuGraphRGCNConv and description: The relational graph convolutional operator from the "Modeling Relational Data with Graph Convolutional Networks" paper.
name: GENConv and description: The GENeralized Graph Convolution (GENConv) from the "DeeperGCN: All You Need to Train Deeper GCNs" paper.
name: FiLMConv and description: The FiLM graph convolutional operator from the "GNN-FiLM: Graph Neural Networks with Feature-wise Linear Modulation" paper.
name: AntiSymmetricConv and description: The anti-symmetric graph convolutional operator from the "Anti-Symmetric DGN: a stable architecture for Deep Graph Networks" paper.
name: RGATConv and description: The relational graph attentional operator from the "Relational Graph Attention Networks" paper.
name: HEATConv and description: The heterogeneous edge-enhanced graph attentional operator from the "Heterogeneous Edge-Enhanced Graph Attention Network For Multi-Agent Trajectory Prediction" paper.
name: LEConv and description: The local extremum graph neural network operator from the "ASAP: Adaptive Structure Aware Pooling for Learning Hierarchical Graph Representations" paper.
name: GraphConv and description: The graph neural network operator from the "Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks" paper.
name: HGTConv and description: The Heterogeneous Graph Transformer (HGT) operator from the "Heterogeneous Graph Transformer" paper.
name: CuGraphGATConv and description: The graph attentional operator from the "Graph Attention Networks" paper.
name: ChebConv and description: The chebyshev spectral graph convolutional operator from the "Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering" paper.
name: MFConv and description: The graph neural network operator from the "Convolutional Networks on Graphs for Learning Molecular Fingerprints" paper.
name: SuperGATConv and description: The self-supervised graph attentional operator from the "How to Find Your Friendly Neighborhood: Graph Attention Design with Self-Supervision" paper.
name: SplineConv and description: The spline-based convolutional operator from the "SplineCNN: Fast Geometric Deep Learning with Continuous B-Spline Kernels" paper.
name: SGConv and description: The simple graph convolutional operator from the "Simplifying Graph Convolutional Networks" paper.
name: CuGraphSAGEConv and description: The GraphSAGE operator from the "Inductive Representation Learning on Large Graphs" paper.
name: SAGEConv and description: The GraphSAGE operator from the "Inductive Representation Learning on Large Graphs" paper.
name: GCNConv and description: The graph convolutional operator from the "Semi-supervised Classification with Graph Convolutional Networks" paper.
name: EGConv and description: The Efficient Graph Convolution from the "Adaptive Filters and Aggregator Fusion for Efficient Graph Convolutions" paper.
name: PointGNNConv and description: The PointGNN operator from the "Point-GNN: Graph Neural Network for 3D Object Detection in a Point Cloud" paper.
name: PANConv and description: The path integral based convolutional operator from the "Path Integral Based Convolution and Pooling for Graph Neural Networks" paper.
name: ResGatedGraphConv and description: The residual gated graph convolutional operator from the "Residual Gated Graph ConvNets" paper.
name: ARMAConv and description: The ARMA graph convolutional operator from the "Graph Neural Networks with Convolutional ARMA Filters" paper.
name: FusedGATConv and description: The fused graph attention operator from the "Understanding GNN Computational Graph: A Coordinated Computation, IO, and Memory Perspective" paper.
name: RGCNConv and description: The relational graph convolutional operator from the "Modeling Relational Data with Graph Convolutional Networks" paper.
name: GPSConv and description: The general, powerful, scalable (GPS) graph transformer layer from the "Recipe for a General, Powerful, Scalable Graph Transformer" paper.
name: PointTransformerConv and description: The Point Transformer layer from the "Point Transformer" paper.
name: CGConv and description: The crystal graph convolutional operator from the "Crystal Graph Convolutional Neural Networks for an Accurate and Interpretable Prediction of Material Properties" paper.
name: AGNNConv and description: The graph attentional propagation layer from the "Attention-based Graph Neural Network for Semi-Supervised Learning" paper.
name: FeaStConv and description: The (translation-invariant) feature-steered convolutional operator from the "FeaStNet: Feature-Steered Graph Convolutions for 3D Shape Analysis" paper.
name: PDNConv and description: The pathfinder discovery network convolutional operator from the "Pathfinder Discovery Networks for Neural Message Passing" paper.
name: TAGConv and description: The topology adaptive graph convolutional networks operator from the "Topology Adaptive Graph Convolutional Networks" paper.
name: MixHopConv and description: The Mix-Hop graph convolutional operator from the "MixHop: Higher-Order Graph Convolutional Architectures via Sparsified Neighborhood Mixing" paper.
name: APPNP and description: The approximate personalized propagation of neural predictions layer from the "Predict then Propagate: Graph Neural Networks meet Personalized PageRank" paper.
name: LGConv and description: The Light Graph Convolution (LGC) operator from the "LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation" paper.
name: XConv and description: The convolutional operator on \(\mathcal{X}\)-transformed points from the "PointCNN: Convolution On X-Transformed Points" paper.
name: PPFConv and description: The PPFNet operator from the "PPFNet: Global Context Aware Local Features for Robust 3D Point Matching" paper.
name: WLConvContinuous and description: The Weisfeiler Lehman operator from the "Wasserstein Weisfeiler-Lehman Graph Kernels" paper.
name: GravNetConv and description: The GravNet operator from the "Learning Representations of Irregular Particle-detector Geometry with Distance-weighted Graph Networks" paper, where the graph is dynamically constructed using nearest neighbors.
name: GINEConv and description: The modified GINConv operator from the "Strategies for Pre-training Graph Neural Networks" paper.
name: DynamicEdgeConv and description: The dynamic edge convolutional operator from the "Dynamic Graph CNN for Learning on Point Clouds" paper (see torch_geometric.nn.conv.EdgeConv), where the graph is dynamically constructed using nearest neighbors in the feature space.
name: HypergraphConv and description: The hypergraph convolutional operator from the "Hypergraph Convolution and Hypergraph Attention" paper.
name: EdgeConv and description: The edge convolutional operator from the "Dynamic Graph CNN for Learning on Point Clouds" paper.
name: PNAConv and description: The Principal Neighbourhood Aggregation graph convolution operator from the "Principal Neighbourhood Aggregation for Graph Nets" paper.
name: GMMConv and description: The gaussian mixture model convolutional operator from the "Geometric Deep Learning on Graphs and Manifolds using Mixture Model CNNs" paper.
name: WLConv and description: The Weisfeiler Lehman (WL) operator from the "A Reduction of a Graph to a Canonical Form and an Algebra Arising During this Reduction" paper.
name: FAConv and description: The Frequency Adaptive Graph Convolution operator from the "Beyond Low-Frequency Information in Graph Convolutional Networks" paper.
name: HeteroConv and description: A generic wrapper for computing graph convolution on heterogeneous graphs.
name: TransformerConv and description: The graph transformer operator from the "Masked Label Prediction: Unified Message Passing Model for Semi-Supervised Classification" paper.
name: GatedGraphConv and description: The gated graph convolution operator from the "Gated Graph Sequence Neural Networks" paper.
name: PointNetConv and description: The PointNet set layer from the "PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation" and "PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space" papers.
name: HANConv and description: The Heterogenous Graph Attention Operator from the "Heterogenous Graph Attention Network" paper.
name: SimpleConv and description: A simple message passing operator that performs (non-trainable) propagation.
name: SSGConv and description: The simple spectral graph convolutional operator from the "Simple Spectral Graph Convolution" paper.
name: FastRGCNConv and description: See RGCNConv.
name: GINConv and description: The graph isomorphism operator from the "How Powerful are Graph Neural Networks?" paper.
name: MessagePassing and description: Base class for creating message passing layers.

Cases retrieved from prior knowledge base:
{
    "name": "LEConv",
    "description": "The local extremum graph neural network operator from the \"ASAP: Adaptive Structure Aware Pooling for Learning Hierarchical Graph Representations\" paper.",
    "link": "../generated/torch_geometric.nn.conv.LEConv.html#torch_geometric.nn.conv.LEConv",
    "paper_link": "https://arxiv.org/abs/1911.07979",
    "paper_name": "\"ASAP: Adaptive Structure Aware Pooling for Learning Hierarchical Graph Representations\"",
    "Model design and experimental setup": "{    \"GNN_Design\": {        \"agg_ops\": \"ASAP uses Local Extrema Convolution (LEConv) as the aggregation operator. LEConv captures functions of local extremas in a graph substructure by considering both global and local importance through self-loops and neighbor aggregations.\",        \"skip_connections\": \"The paper does not explicitly mention the use of skip connections in the architecture.\",        \"layer_info_fusion\": \"Information is fused across layers using LEConv which computes a fitness vector for clusters based on both the cluster's features and the differences between the features of neighboring nodes.\",        \"num_layers\": \"ASAP uses three layers of GCN, each followed by a pooling layer.\",        \"hyperparameters\": {            \"k\": \"0.5\",            \"h\": \"1\",            \"dropout\": \"0\",            \"learning_rate\": \"0.001\",            \"hidden_dimension\": \"64\",            \"l2_regularization\": \"5e-4\"        },        \"activation\": \"ReLU (used within LEConv and GCN layers)\"    },    \"Experimental_Setup\": {        \"datasets\": [\"D&D\", \"PROTEINS\", \"NCI1\", \"NCI109\", \"FRANKENSTEIN\"],        \"dataset_summary\": {            \"D&D\": \"Contains proteins as graphs with 1178 graphs, 2 classes, average 284.32 nodes, and 715.66 edges per graph.\",            \"PROTEINS\": \"Contains proteins as graphs with 1113 graphs, 2 classes, average 39.06 nodes, and 72.82 edges per graph.\",            \"NCI1\": \"Anticancer activity classification dataset with 4110 graphs, 2 classes, average 29.87 nodes, and 32.30 edges per graph.\",            \"NCI109\": \"Anticancer activity classification dataset with 4127 graphs, 2 classes, average 29.68 nodes, and 32.13 edges per graph.\",            \"FRANKENSTEIN\": \"Molecule mutagen classification dataset with 4337 graphs, 2 classes, average 16.90 nodes, and 17.88 edges per graph.\"        },        \"baseline\": [\"DiffPool\", \"TopK\", \"SAGPool\", \"Set2Set\", \"Global-Attention\", \"SortPool\"],        \"performance_comparisons\": {            \"D&D\": {                \"ASAP\": \"76.87\u00b10.70\",                \"DiffPool\": \"66.95\u00b12.41\",                \"TopK\": \"75.01\u00b10.86\",                \"SAGPool\": \"76.45\u00b10.97\",                \"Set2Set\": \"71.60\u00b10.87\",                \"Global-Attention\": \"71.38\u00b10.78\",                \"SortPool\": \"71.87\u00b10.96\"            },            \"PROTEINS\": {                \"ASAP\": \"74.19\u00b10.79\",                \"DiffPool\": \"68.20\u00b12.02\",                \"TopK\": \"71.10\u00b10.90\",                \"SAGPool\": \"71.86\u00b10.97\",                \"Set2Set\": \"72.16\u00b10.43\",                \"Global-Attention\": \"71.87\u00b10.60\",                \"SortPool\": \"73.91\u00b10.72\"            },            \"NCI1\": {                \"ASAP\": \"71.48\u00b10.42\",                \"DiffPool\": \"62.32\u00b11.90\",                \"TopK\": \"67.02\u00b12.25\",                \"SAGPool\": \"67.45\u00b11.11\",                \"Set2Set\": \"66.97\u00b10.74\",                \"Global-Attention\": \"69.00\u00b10.49\",                \"SortPool\": \"68.74\u00b11.07\"            },            \"NCI109\": {                \"ASAP\": \"70.07\u00b10.55\",                \"DiffPool\": \"61.98\u00b11.98\",                \"TopK\": \"66.12\u00b11.60\",                \"SAGPool\": \"67.86\u00b11.41\",                \"Set2Set\": \"61.04\u00b12.69\",                \"Global-Attention\": \"67.87\u00b10.40\",                \"SortPool\": \"68.59\u00b10.67\"            },            \"FRANKENSTEIN\": {                \"ASAP\": \"66.26\u00b10.47\",                \"DiffPool\": \"60.60\u00b11.62\",                \"TopK\": \"61.46\u00b10.84\",                \"SAGPool\": \"61.73\u00b10.76\",                \"Set2Set\": \"61.46\u00b10.47\",                \"Global-Attention\": \"61.31\u00b10.41\",                \"SortPool\": \"63.44\u00b10.65\"            }        }    }}",
    "Paper Summary": "The paper presents a novel graph pooling method known as ASAP (Adaptive Structure Aware Pooling), designed for efficiently learning hierarchical graph representations. The primary focus of the proposed method is on the model design aspects that enhance graph representation learning. Here\u2019s a detailed summary of the methods discussed in the paper concerning the model design.\n\n### Model Design of ASAP\n\n1. **Overview**: ASAP employs a novel self-attention mechanism combined with a modified Graph Neural Network (GNN) to capture the significance of nodes in a graph and to create a pooled graph that effectively retains structural information.\n\n2. **Adaptive Structure-Aware Pooling**:\n   - ASAP operates by initially constructing local clusters from nodes based on a fixed receptive field, effectively creating clusters from a node\u2019s h-hop neighborhood.\n   - Each node is regarded as a medoid of its cluster, and the members of the cluster are determined using a probabilistic self-attention mechanism.\n\n3. **Master2Token (M2T) Framework**:\n   - The M2T attention mechanism provides a way to summarize the cluster by generating a master representation from all nodes within the cluster.\n   - It captures the intra-cluster information more effectively than previous self-attention implementations, allowing the model to generate a more representative attention score for each node within the cluster.\n\n4. **Cluster Assignment Matrix**:\n   - A sparse cluster assignment matrix is computed using the attention scores generated by the M2T framework, which signifies the degree of membership of each node in its respective cluster.\n   - This assignment matrix is critical in maintaining the sparsity of the pooled graph, allowing the model to handle larger graphs efficiently.\n\n5. **Local Extremum Convolution (LEConv)**:\n   - LEConv is introduced as a specialized convolutional operator designed to score clusters based on their local importance. It helps in capturing local extremes, providing a fitness score for each cluster.\n   - The scores from LEConv are used to rank clusters, and a fraction of the top-scoring clusters is selected for pooling, creating a reduced graph representation.\n\n6. **Cluster Formation and Connection**:\n   -  The pooled graph is created by selecting the top clusters based on the fitness scores and recalculating edge weights to ensure meaningful connections between clusters.\n   - This is achieved through a careful consideration of the graph structure that maintains connectivity across the new pooled graph.\n\n7. **Pooled Graph Representation**:\n   - The final pooled graph retains the node features and adjacency information, allowing for improved learning in subsequent layers of the GNN.\n   - The connection strength between clusters in the pooled graph is adapted based on both the membership of constituent nodes and the edge weights between them.\n\nOverall, the architecture is designed to address the shortcomings of existing pooling methods, allowing for more effective modeling of hierarchical relationships within graphs, leading to improved performance in graph classification tasks. The use of self-attention and learnable convolutional operations in a sparse manner allows ASAP to scale effectively with larger graphs while capturing rich structural information."
}

Cases retrieved from prior knowledge base:
{
    "name": "GATv2Conv",
    "description": "The GATv2 operator from the \"How Attentive are Graph Attention Networks?\" paper, which fixes the static attention problem of the standard GATConv layer.",
    "link": "../generated/torch_geometric.nn.conv.GATv2Conv.html#torch_geometric.nn.conv.GATv2Conv",
    "paper_link": "https://arxiv.org/abs/2105.14491",
    "paper_name": "\"How Attentive are Graph Attention Networks?\"",
    "Model design and experimental setup": "{    \"GNN_Design\": {        \"agg_ops\": \"Dynamic attention mechanism using a learned weight matrix after a nonlinearity\",        \"skip_connections\": \"Residual connections are used in all models\",        \"layer_info_fusion\": \"Edges are used to aggregate information across layers, using a dynamic scoring function\",        \"num_layers\": \"Varies per task, generally 2-6 layers depending on dataset\",        \"hyperparameters\": \"Hidden size (64-256), learning rate (0.0005-0.01), sampling method (full batch, GraphSAINT, Neighbor Sampling)\",        \"activation\": \"LeakyReLU\"    },    \"Experimental_Setup\": {        \"datasets\": [\"ogbn-arxiv\", \"ogbn-mag\", \"ogbn-products\", \"ogbn-proteins\", \"ogbl-collab\", \"ogbl-citation2\", \"QM9\", \"VARMISUSE\"],        \"dataset_summary\": {            \"ogbn-arxiv\": \"Node prediction dataset with 169,343 nodes and 1,166,243 edges\",            \"ogbn-mag\": \"Node prediction dataset with 1,939,743 nodes and 21,111,007 edges\",            \"ogbn-products\": \"Node prediction dataset with 2,449,029 nodes and 61,859,140 edges\",            \"ogbn-proteins\": \"Node prediction dataset with 132,534 nodes and 39,561,252 edges\",            \"ogbl-collab\": \"Link prediction dataset with 235,868 nodes and 1,285,465 edges\",            \"ogbl-citation2\": \"Link prediction dataset with 2,927,963 nodes and 30,561,187 edges\",            \"QM9\": \"Graph prediction dataset for quantum chemical properties\",            \"VARMISUSE\": \"Node-pointing problem with semantic interactions in code\"        },        \"baseline\": [\"GCN\", \"GIN\", \"GraphSAGE\", \"GAT\"],        \"performance_comparisons\": {            \"node_prediction\": \"GATv2 outperformed GAT and other baselines across all datasets, showing better accuracy\",            \"link_prediction\": \"GATv2 achieved higher mean reciprocal rank and Hits@50 compared to GAT\",            \"graph_prediction\": \"GATv2 had lower average error rates in QM9 compared to GAT\",            \"robustness\": \"GATv2 demonstrated more robustness to noise compared to GAT\",            \"synthetic_benchmark\": \"GATv2 achieved 100% accuracy in the DICTIONARY-LOOKUP problem, whereas GAT could not fit the training data\"        }    }}",
    "Paper Summary": "The paper introduces enhancements to the Graph Attention Network (GAT) architecture in light of its limitations regarding attention mechanisms. Here are the key points focused on the model design aspects:\n\n1. **Static vs. Dynamic Attention**: The authors identify that the standard GAT uses a static attention mechanism, meaning that the ranking of attention scores across all nodes is invariant to the specific query node. This can hinder the expressiveness of the model and limits its capability to solve certain tasks that require dynamic attention, where the relevance of neighbors varies depending on the query.\n\n2. **Definition of Attention Types**: The paper formally defines two types of attention:\n   - **Static Attention**: A family of scoring functions that produces the same ranking of attention scores for all nodes, regardless of the query.\n   - **Dynamic Attention**: A more flexible mechanism where attention scores can vary based on the query, allowing for variable ranking of the importance of neighbors.\n\n3. **Proposal of GATv2**: To overcome the limitations of static attention, the authors propose GATv2. The model design of GATv2 involves:\n   - **Modification of Operation Order**: GATv2 modifies the order of operations in the attention computation. Instead of applying linear transformations before non-linear activation functions, GATv2 applies the non-linearities following the concatenation of input features. This structural change allows for a more expressive attention function, enabling the computation of dynamic attention.\n   - **Universal Approximator**: The modified architecture essentially transforms GATv2 into a universal approximator for attention functions, which is theoretically more powerful than its predecessor.\n\n4. **Formulation of Attention Mechanism**: GATv2\u2019s attention mechanism is defined mathematically as:\n   - \\( e(h_i, h_j) = a^T \\text{LeakyReLU}(W \\cdot [h_i \\parallel h_j]) \\)\n   This contrasts with GAT's formulation, where linear transformations are applied before combining the features and applying the activation function, making GATv2 more expressive in capturing the relationships between nodes.\n\n5. **Model Complexity**: GATv2 maintains the same time complexity as GAT, i.e., \\( O(|V|d d' + |E|d') \\), which is efficient and suitable for graph-based tasks. The parameterization is adjusted to ensure equivalent or fewer parameters while enabling enhanced performance.\n\n6. **Empirical Insights**: Through theoretical claims and derivations, GATv2 demonstrates improved performance across various benchmarks when compared to GAT, showcasing the advantages of implementing a dynamic attention mechanism. \n\nIn summary, GATv2 significantly improves upon GAT by enabling dynamic attention through a relatively simple modification in the order of operations within the attention computation process, while maintaining computational efficiency and expressiveness necessary for various graph-based tasks."
}

This is a case related to the handled dataset:
{
    "Task Description": "Leaderboards for Node Property Prediction",
    "Dataset Name": "ogbn-proteins",
    "Dataset Link": "../nodeprop/#ogbn-proteins",
    "Rank": 14,
    "Method": "DeeperGCN",
    "External Data": "No",
    "Test Accuracy": "0.8580 \u00b1 0.0017",
    "Validation Accuracy": "0.9106 \u00b1 0.0016",
    "Contact": "mailto:guohao.li@kaust.edu.sa",
    "Paper Link": "https://arxiv.org/abs/2006.07739",
    "Code Link": "https://github.com/lightaime/deep_gcns_torch/tree/master/examples/ogb",
    "Parameters": "2,374,568",
    "Hardware": "NVIDIA Tesla V100 (32GB GPU)",
    "Date": "Jun 16, 2020",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Node_Property_Prediction/ogbn-proteins/DeeperGCN.pdf",
    "Paper Summary": "The paper \"DeeperGCN: All You Need to Train Deeper GCNs\" introduces several key methods to enhance the training of deep Graph Convolutional Networks (GCNs). Below are the main model design aspects discussed in the paper:\n\n### 1. **Generalized Aggregation Functions:**\n   - The authors propose a **Generalized Aggregation Function** that accommodates various types of message aggregation operations (e.g., mean, max) within GCNs. This function is designed to be **permutation invariant** and **differentiable**, allowing it to be adapted and learned during the training process. \n   - Two specific forms of aggregation\u2014**SoftMax Aggregation** and **Power Mean Aggregation**\u2014are introduced, which can interpolate between mean and max based on control parameters.\n\n### 2. **Modified Residual Connections:**\n   - A **pre-activation version of residual connections** is proposed for GCN architectures. This design changes the order of operations, activating nodes after normalization, which has been shown to improve representational power compared to traditional residual connections.\n   - The residual connection structure is specifically designed to help maintain performance even with increased model depth.\n\n### 3. **Message Normalization Layer (MsgNorm):**\n   - The authors highlight the significance of **normalization techniques** in deep GCNs, introducing a **MsgNorm layer**. This layer improves model performance by normalizing the features of the aggregated messages before they are used to update node features. This is achieved through a learnable scaling factor applied to the normalized message.\n   - The MsgNorm layer aims to combat issues related to over-smoothing in deep networks, particularly in instances where aggregation methods may not perform well.\n\n### 4. **Architectural Framework - GENeralized Aggregation Networks (GEN):**\n   - The proposed architecture combines the generalized message aggregators, pre-activation residual connections, and the MsgNorm layer into a unified framework, termed **GEN**. \n   - This framework is designed to be flexible and robust for training deeper GCNs efficiently, addressing challenges associated with vanishing gradients, over-smoothing, and overfitting when increasing model depth.\n\n### Summary: \nThese design aspects collectively aim to enhance the capability of GCNs to learn effective representations from large-scale graph datasets. By integrating generalized aggregation methods, modified residual connections, and a dedicated normalization layer, the proposed model design facilitates the training of significantly deeper GCNs, ultimately leading to improved performance across a range of tasks involving graph data."
}

This is a case related to the handled dataset:
{
    "Task Description": "Leaderboards for Node Property Prediction",
    "Dataset Name": "ogbn-products",
    "Dataset Link": "../nodeprop/#ogbn-products",
    "Rank": 1,
    "Method": "GLEM+GIANT+SAGN+SCR",
    "External Data": "Yes",
    "Test Accuracy": "0.8737 \u00b1 0.0006",
    "Validation Accuracy": "0.9400 \u00b1 0.0003",
    "Contact": "mailto:andy.zhaoja@gmail.com",
    "Paper Link": "https://arxiv.org/abs/2210.14709",
    "Code Link": "https://github.com/AndyJZhao/GLEM",
    "Parameters": "139,792,525",
    "Hardware": "Tesla V100 (32GB)",
    "Date": "Oct 27, 2022",
    "Local Paper PDF Path": "knowledge_base/Leaderboards_for_Node_Property_Prediction/ogbn-products/GLEM+GIANT+SAGN+SCR.pdf",
    "Paper Summary": "The paper introduces a novel approach called Graph and Language Learning by Expectation Maximization (GLEM) to tackle the challenge of learning representations on text-attributed graphs (TAGs). Central to GLEM is the idea of combining large language models (LMs) and graph neural networks (GNNs) using a variational Expectation-Maximization (EM) framework. Here's a breakdown of the model design aspects discussed in the article:\n\n### Method Overview\n\n1. **Dual Components**:\n   - **Language Model (LM)**: Encodes the textual attributes associated with each node, capturing local semantic information.\n   - **Graph Neural Network (GNN)**: Models the structural interactions between nodes, leveraging both the text and label information of surrounding nodes to predict node labels.\n\n2. **Variational EM Framework**:\n   - The GLEM employs a variational EM strategy that alternates between:\n     - **E-step (Expectation step)**: Updates the LM by predicting labels (including pseudo-labels inferred by the GNN) conditioned on the node\u2019s textual description.\n     - **M-step (Maximization step)**: Updates the GNN using node representations generated by the LM, while utilizing both the observed labels and GNN-predicted pseudo-labels.\n\n3. **Objective Functions**:\n   - The framework aims to maximize the log-likelihood of the observed node labels while utilizing a lower bound of the evidence.\n   - The model parameterizations incorporate:\n     - For the LM, a mean-field approximation to represent the label distribution as independent of other nodes but dependent on its text attributes.\n     - For the GNN, a conditional distribution is defined that factors in the graph structure and surrounding node attributes (text and labels).\n\n4. **Pseudo-Labels**:\n   - GLEM uses pseudo-labeling where the LM generates predictions for unlabeled nodes, which are then employed as targets in GNN updates, fostering collaboration between the two models.\n\n5. **Training Procedure**:\n   - **E-step**: The LM is tuned to mimic label distributions inferred by the GNN while also learning from actual labels of labeled nodes.\n   - **M-step**: The GNN is trained to capitalize on the textual embeddings provided by the LM and the inferred labels to optimize the prediction of node classifications.\n\n6. **Scalability**:\n   - The EM framework allows GLEM to adaptively scale while maintaining efficiency, avoiding the memory overhead associated with joint training of LMs and GNNs, making it feasible for large TAG datasets.\n\nOverall, GLEM synergistically connects LMs and GNNs through a structured training paradigm that prioritizes the interaction between textual and structural information, enabling effective representation learning in large text-attributed graphs."
}

This is a case related to the handled dataset:
{
    "name": "FAConv",
    "description": "The Frequency Adaptive Graph Convolution operator from the \"Beyond Low-Frequency Information in Graph Convolutional Networks\" paper.",
    "link": "../generated/torch_geometric.nn.conv.FAConv.html#torch_geometric.nn.conv.FAConv",
    "paper_link": "https://arxiv.org/abs/2101.00797",
    "paper_name": "\"Beyond Low-Frequency Information in Graph Convolutional Networks\"",
    "Local Paper PDF Path": "knowledge_base\\pyg_info\\nn\\FAConv.pdf",
    "Model design and experimental setup": {
        "GNN_Design": {
            "agg_ops": "Low-pass and high-pass filters to adaptively aggregate different frequency signals using a self-gating mechanism.",
            "skip_connections": "Adaptive integration of raw features with aggregated signals helps retain additional information.",
            "layer_info_fusion": "Layers aggregate low-frequency signals from similar-class neighbors and high-frequency signals from different-class neighbors.",
            "num_layers": "Configurable; experiments showed performance analysis across different depths.",
            "hyperparameters": "Includes a scaling parameter \u03b5, dropout, learning rate, and weight decay. Exact settings vary per dataset.",
            "activation": "Uses a tanh function within the self-gating mechanism."
        },
        "Experimental_Setup": {
            "datasets": "Cora, Citeseer, Pubmed, Chameleon, Squirrel, and Actor networks.",
            "dataset_summary": "Assortative: Cora, Citeseer, Pubmed with publication citations. Disassortative: Chameleon, Squirrel with Wikipedia pages and Actor co-occurrence network.",
            "baseline": "Comparison with models like SGC, GCN, GWNN, GIN, GAT, MoNet, APPNP, and Geom-GCN.",
            "performance_comparisons": "FAGCN showed superior performance across multiple datasets, outperforming other models in both assortative and disassortative network analysis."
        }
    },
    "Paper Summary": "The paper \"Beyond Low-frequency Information in Graph Convolutional Networks\" discusses the design of a novel graph convolutional network model called Frequency Adaptation Graph Convolutional Networks (FAGCN). This model specifically aims to adaptively integrate both low-frequency and high-frequency signals from node features to enhance learning effectiveness in various network types.\n\n### Key Methods Discussed in the Paper:\n\n1. **Separation of Frequency Signals:**\n   - FAGCN employs enhanced low-pass and high-pass filters to separate low-frequency and high-frequency signals from raw node features. The filters are mathematically defined and help in delineating the different contributions each signal type makes to the node representations.\n\n2. **Self-Gating Mechanism:**\n   - A self-gating mechanism is developed within FAGCN to adaptively determine the importance or proportion of low-frequency and high-frequency signals during the message passing phases. This is realized through coefficients that learn from both node features and their neighbors, allowing the model to dynamically adjust the filtering based on the network context.\n\n3. **Adaptive Aggregation:**\n   - FAGCN aggregates signals from neighboring nodes using different coefficients for low-frequency and high-frequency contributions, which prevents the over-smoothing problem often encountered in existing GNNs. The mathematical formulation includes normalizing the contributions based on the node's degree to manage representation sizes effectively.\n\n4. **Architecture Framework:**\n   - The architecture of FAGCN uses a multi-layer perceptron (MLP) for non-linear transformations and propagates aggregated representations through several layers. This modular design allows for flexible adaptation to signal types without prior knowledge of network structure (assortative or disassortative).\n\n5. **Expressive Power:**\n   - The paper theorizes that FAGCN generalizes existing GNNs and possesses superior expressive power due to its ability to adjust the filtering and aggregation of low- and high-frequency signals, thus allowing for better performance across different network types.\n\nIn summary, FAGCN represents a significant advancement in graph neural network design, focusing on the dual adaptation of frequency signals to optimize node representation learning while mitigating issues like over-smoothing. This adaptive mechanism allows for more nuanced learning strategies tailored to the characteristics of real-world networks."
}

 [Configuration Agent Output]
Constructed Search Space: {'AGGREGATION': ['LEConv', 'GATv2Conv', 'GCNConv', 'GATConv', 'ASAPConv'], 'ACTIVATION': ['relu', 'elu'], 'FUSION': ['sum', 'mean']}
Selected Search Algorithm: {'algorithm': 'Differentiable Search'}

========================= CONFIGURATION AGENT END =========================


========================= EVALUATION AGENT =========================

************************* START SEARCH *************************
searched archs for actor...
Experiment dir : logs/search-actor-20241021-212050
*************log_filename=logs/search-actor-20241021-212050/log.txt************
[Epoch: 10] Train Loss: 1.5404, Acc: 0.2708 | Val Loss: 1.4772, Acc: 0.3243 | Test Loss: 1.4823, Acc: 0.3204
[Epoch: 20] Train Loss: 1.3217, Acc: 0.3904 | Val Loss: 1.4403, Acc: 0.3421 | Test Loss: 1.4436, Acc: 0.3408
[Epoch: 30] Train Loss: 1.1877, Acc: 0.4450 | Val Loss: 1.4355, Acc: 0.3447 | Test Loss: 1.4603, Acc: 0.3553
[Epoch: 40] Train Loss: 1.0648, Acc: 0.5105 | Val Loss: 1.4886, Acc: 0.3724 | Test Loss: 1.4887, Acc: 0.3730
[Epoch: 50] Train Loss: 0.9920, Acc: 0.5588 | Val Loss: 1.6419, Acc: 0.3750 | Test Loss: 1.6493, Acc: 0.3697
[Epoch: 60] Train Loss: 0.9127, Acc: 0.5864 | Val Loss: 1.7675, Acc: 0.3743 | Test Loss: 1.7904, Acc: 0.3559
[Epoch: 70] Train Loss: 0.8406, Acc: 0.6272 | Val Loss: 1.9276, Acc: 0.3717 | Test Loss: 1.9234, Acc: 0.3513
[Epoch: 80] Train Loss: 0.8070, Acc: 0.6419 | Val Loss: 2.0973, Acc: 0.3763 | Test Loss: 2.0856, Acc: 0.3474
[Epoch: 90] Train Loss: 0.7685, Acc: 0.6656 | Val Loss: 2.2118, Acc: 0.3724 | Test Loss: 2.1778, Acc: 0.3605
[Epoch: 100] Train Loss: 0.7556, Acc: 0.6741 | Val Loss: 2.2991, Acc: 0.3691 | Test Loss: 2.2855, Acc: 0.3480
[Epoch: 110] Train Loss: 0.7278, Acc: 0.6842 | Val Loss: 2.4255, Acc: 0.3809 | Test Loss: 2.4231, Acc: 0.3487
[Epoch: 120] Train Loss: 0.7031, Acc: 0.7011 | Val Loss: 2.5347, Acc: 0.3704 | Test Loss: 2.5158, Acc: 0.3592
[Epoch: 130] Train Loss: 0.6950, Acc: 0.6963 | Val Loss: 2.6693, Acc: 0.3842 | Test Loss: 2.6314, Acc: 0.3632
[Epoch: 140] Train Loss: 0.6695, Acc: 0.7103 | Val Loss: 2.7646, Acc: 0.3750 | Test Loss: 2.7089, Acc: 0.3566
[Epoch: 150] Train Loss: 0.6547, Acc: 0.7200 | Val Loss: 2.8830, Acc: 0.3691 | Test Loss: 2.8183, Acc: 0.3618
[Epoch: 160] Train Loss: 0.6251, Acc: 0.7454 | Val Loss: 2.9648, Acc: 0.3724 | Test Loss: 2.9017, Acc: 0.3638
[Epoch: 170] Train Loss: 0.6308, Acc: 0.7296 | Val Loss: 3.0703, Acc: 0.3704 | Test Loss: 2.9936, Acc: 0.3566
[Epoch: 180] Train Loss: 0.6162, Acc: 0.7368 | Val Loss: 3.0963, Acc: 0.3724 | Test Loss: 3.0068, Acc: 0.3559
[Epoch: 190] Train Loss: 0.6179, Acc: 0.7395 | Val Loss: 3.1241, Acc: 0.3711 | Test Loss: 3.0395, Acc: 0.3586
[Epoch: 200] Train Loss: 0.5993, Acc: 0.7436 | Val Loss: 3.1822, Acc: 0.3678 | Test Loss: 3.0976, Acc: 0.3592
searched res for actor saved in exp_res/actor-searched_res-20241021-212059-eps0.0-reg0.0005.txt
************************* SEARCH DONE *************************

************************* START TUNE *************************
*************** [process 1-th/1] ***************
{'activation': 'elu', 'dropout': 6, 'hidden_size': 512, 'learning_rate': 0.0038820032612969474, 'model': 'f2gnn', 'optimizer': 'adam', 'weight_decay': 0.0005944875859904801}
Current Hyperparameter: {'activation': 'elu', 'dropout': 6, 'hidden_size': 512, 'learning_rate': 0.0038820032612969474, 'model': 'f2gnn', 'optimizer': 'adam', 'weight_decay': 0.0005944875859904801}
[Epoch:0] Train Loss:1.7091 | Valid Acc:0.2388 | Test Acc:0.2388
[Epoch:10] Train Loss:1.7255 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:20] Train Loss:1.5990 | Valid Acc:0.2145 | Test Acc:0.2145
[Epoch:30] Train Loss:1.5812 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:40] Train Loss:1.5774 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:50] Train Loss:1.5748 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:60] Train Loss:1.5734 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:70] Train Loss:1.5735 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:80] Train Loss:1.5738 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:90] Train Loss:1.5738 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:100] Train Loss:1.5720 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:110] Train Loss:1.5740 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:120] Train Loss:1.5728 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:130] Train Loss:1.5709 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:140] Train Loss:1.5726 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:150] Train Loss:1.5715 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:160] Train Loss:1.5692 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:170] Train Loss:1.5673 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:180] Train Loss:1.5616 | Valid Acc:0.2967 | Test Acc:0.3112
[Epoch:190] Train Loss:1.5346 | Valid Acc:0.3013 | Test Acc:0.3033
[Epoch:200] Train Loss:1.4853 | Valid Acc:0.3007 | Test Acc:0.3026
[Epoch:210] Train Loss:1.4472 | Valid Acc:0.2632 | Test Acc:0.2671
[Epoch:220] Train Loss:1.5963 | Valid Acc:0.2388 | Test Acc:0.2388
[Epoch:230] Train Loss:1.5705 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:240] Train Loss:1.5706 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:250] Train Loss:1.5609 | Valid Acc:0.2434 | Test Acc:0.2447
[Epoch:260] Train Loss:1.5124 | Valid Acc:0.3217 | Test Acc:0.3158
[Epoch:270] Train Loss:1.4423 | Valid Acc:0.3105 | Test Acc:0.2954
[Epoch:280] Train Loss:1.4044 | Valid Acc:0.3645 | Test Acc:0.3388
[Epoch:290] Train Loss:1.3855 | Valid Acc:0.3559 | Test Acc:0.3401
[Epoch:300] Train Loss:1.3691 | Valid Acc:0.3684 | Test Acc:0.3461
[Epoch:310] Train Loss:1.5563 | Valid Acc:0.2289 | Test Acc:0.2316
[Epoch:320] Train Loss:1.4471 | Valid Acc:0.2901 | Test Acc:0.2862
[Epoch:330] Train Loss:1.3989 | Valid Acc:0.3257 | Test Acc:0.3178
[Epoch:340] Train Loss:1.3650 | Valid Acc:0.3046 | Test Acc:0.2967
[Epoch:350] Train Loss:1.3450 | Valid Acc:0.3151 | Test Acc:0.3105
[Epoch:360] Train Loss:1.3288 | Valid Acc:0.3217 | Test Acc:0.3145
[Epoch:370] Train Loss:1.3190 | Valid Acc:0.3316 | Test Acc:0.3224
[Epoch:380] Train Loss:1.3032 | Valid Acc:0.3336 | Test Acc:0.3250
[Epoch:390] Train Loss:1.2917 | Valid Acc:0.3480 | Test Acc:0.3434
[Epoch:400] Train Loss:1.2853 | Valid Acc:0.3487 | Test Acc:0.3401
[Epoch:410] Train Loss:1.2780 | Valid Acc:0.3342 | Test Acc:0.3197
[Epoch:420] Train Loss:1.2477 | Valid Acc:0.3362 | Test Acc:0.3171
[Epoch:430] Train Loss:1.2254 | Valid Acc:0.3684 | Test Acc:0.3684
[Epoch:440] Train Loss:1.2305 | Valid Acc:0.3099 | Test Acc:0.2961
[Epoch:450] Train Loss:1.2348 | Valid Acc:0.3572 | Test Acc:0.3658
[Epoch:460] Train Loss:1.2005 | Valid Acc:0.3408 | Test Acc:0.3395
[Epoch:470] Train Loss:1.1880 | Valid Acc:0.3592 | Test Acc:0.3730
[Epoch:480] Train Loss:1.1703 | Valid Acc:0.3625 | Test Acc:0.3783
[Epoch:490] Train Loss:1.1771 | Valid Acc:0.3586 | Test Acc:0.3789
[Epoch:500] Train Loss:1.1466 | Valid Acc:0.3553 | Test Acc:0.3737
[Epoch:510] Train Loss:1.1439 | Valid Acc:0.3605 | Test Acc:0.3822
[Epoch:520] Train Loss:1.1422 | Valid Acc:0.3526 | Test Acc:0.3750
[Epoch:530] Train Loss:1.1308 | Valid Acc:0.3618 | Test Acc:0.3770
[Epoch:540] Train Loss:1.1165 | Valid Acc:0.3664 | Test Acc:0.3829
[Epoch:550] Train Loss:1.1111 | Valid Acc:0.3592 | Test Acc:0.3783
[Epoch:560] Train Loss:1.0935 | Valid Acc:0.3632 | Test Acc:0.3789
[Epoch:570] Train Loss:1.0892 | Valid Acc:0.3658 | Test Acc:0.3737
[Epoch:580] Train Loss:1.0805 | Valid Acc:0.3664 | Test Acc:0.3875
[Epoch:590] Train Loss:1.0746 | Valid Acc:0.3645 | Test Acc:0.3737
[Epoch:600] Train Loss:1.0665 | Valid Acc:0.3743 | Test Acc:0.3783
[Epoch:610] Train Loss:1.0582 | Valid Acc:0.3678 | Test Acc:0.3717
[Epoch:620] Train Loss:1.0398 | Valid Acc:0.3743 | Test Acc:0.3737
[Epoch:630] Train Loss:1.0378 | Valid Acc:0.3770 | Test Acc:0.3783
[Epoch:640] Train Loss:1.0295 | Valid Acc:0.3750 | Test Acc:0.3849
[Epoch:650] Train Loss:1.0336 | Valid Acc:0.3743 | Test Acc:0.3776
[Epoch:660] Train Loss:1.0321 | Valid Acc:0.3836 | Test Acc:0.3822
[Epoch:670] Train Loss:1.0167 | Valid Acc:0.3803 | Test Acc:0.3757
[Epoch:680] Train Loss:1.0155 | Valid Acc:0.3809 | Test Acc:0.3888
[Epoch:690] Train Loss:1.0109 | Valid Acc:0.3862 | Test Acc:0.3822
[Epoch:700] Train Loss:0.9974 | Valid Acc:0.3803 | Test Acc:0.3816
[Epoch:710] Train Loss:1.0036 | Valid Acc:0.3816 | Test Acc:0.3855
[Epoch:720] Train Loss:1.0055 | Valid Acc:0.3829 | Test Acc:0.3836
[Epoch:730] Train Loss:0.9974 | Valid Acc:0.3855 | Test Acc:0.3862
[Epoch:740] Train Loss:0.9941 | Valid Acc:0.3783 | Test Acc:0.3862
[Epoch:750] Train Loss:0.9864 | Valid Acc:0.3809 | Test Acc:0.3849
[Epoch:760] Train Loss:0.9928 | Valid Acc:0.3836 | Test Acc:0.3862
[Epoch:770] Train Loss:1.0009 | Valid Acc:0.3842 | Test Acc:0.3842
[Epoch:780] Train Loss:0.9968 | Valid Acc:0.3849 | Test Acc:0.3875
[Epoch:790] Train Loss:0.9933 | Valid Acc:0.3836 | Test Acc:0.3875
Best results: epoch=689, valid loss=1.5480, valid acc=0.3875, test loss:1.5241,test acc=0.3842
{'activation': 'relu', 'dropout': 5, 'hidden_size': 64, 'learning_rate': 0.009375524420326943, 'model': 'f2gnn', 'optimizer': 'adam', 'weight_decay': 0.00012355679425179737}
Current Hyperparameter: {'activation': 'relu', 'dropout': 5, 'hidden_size': 64, 'learning_rate': 0.009375524420326943, 'model': 'f2gnn', 'optimizer': 'adam', 'weight_decay': 0.00012355679425179737}
[Epoch:0] Train Loss:1.6286 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:10] Train Loss:1.5787 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:20] Train Loss:1.5505 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:30] Train Loss:1.4338 | Valid Acc:0.3671 | Test Acc:0.3770
[Epoch:40] Train Loss:1.3378 | Valid Acc:0.3757 | Test Acc:0.3763
[Epoch:50] Train Loss:1.2825 | Valid Acc:0.3539 | Test Acc:0.3658
[Epoch:60] Train Loss:1.2355 | Valid Acc:0.3921 | Test Acc:0.3901
[Epoch:70] Train Loss:1.2070 | Valid Acc:0.3651 | Test Acc:0.3783
[Epoch:80] Train Loss:1.1667 | Valid Acc:0.3678 | Test Acc:0.3546
[Epoch:90] Train Loss:1.1140 | Valid Acc:0.3684 | Test Acc:0.3632
[Epoch:100] Train Loss:1.0578 | Valid Acc:0.3783 | Test Acc:0.3678
[Epoch:110] Train Loss:1.0301 | Valid Acc:0.3875 | Test Acc:0.3671
[Epoch:120] Train Loss:0.9824 | Valid Acc:0.3678 | Test Acc:0.3566
[Epoch:130] Train Loss:0.9127 | Valid Acc:0.3717 | Test Acc:0.3651
[Epoch:140] Train Loss:0.8689 | Valid Acc:0.3743 | Test Acc:0.3553
[Epoch:150] Train Loss:0.8261 | Valid Acc:0.3711 | Test Acc:0.3461
[Epoch:160] Train Loss:0.8044 | Valid Acc:0.3612 | Test Acc:0.3467
[Epoch:170] Train Loss:0.7777 | Valid Acc:0.3592 | Test Acc:0.3572
[Epoch:180] Train Loss:0.7589 | Valid Acc:0.3500 | Test Acc:0.3533
[Epoch:190] Train Loss:0.7141 | Valid Acc:0.3500 | Test Acc:0.3579
[Epoch:200] Train Loss:0.7260 | Valid Acc:0.3612 | Test Acc:0.3467
[Epoch:210] Train Loss:0.6888 | Valid Acc:0.3572 | Test Acc:0.3605
[Epoch:220] Train Loss:0.6701 | Valid Acc:0.3533 | Test Acc:0.3618
[Epoch:230] Train Loss:0.6561 | Valid Acc:0.3520 | Test Acc:0.3645
[Epoch:240] Train Loss:0.6592 | Valid Acc:0.3500 | Test Acc:0.3566
[Epoch:250] Train Loss:0.6300 | Valid Acc:0.3520 | Test Acc:0.3539
[Epoch:260] Train Loss:0.6354 | Valid Acc:0.3559 | Test Acc:0.3520
[Epoch:270] Train Loss:0.6183 | Valid Acc:0.3612 | Test Acc:0.3572
[Epoch:280] Train Loss:0.6267 | Valid Acc:0.3566 | Test Acc:0.3625
[Epoch:290] Train Loss:0.6184 | Valid Acc:0.3664 | Test Acc:0.3546
[Epoch:300] Train Loss:0.6078 | Valid Acc:0.3572 | Test Acc:0.3572
[Epoch:310] Train Loss:0.6006 | Valid Acc:0.3487 | Test Acc:0.3618
[Epoch:320] Train Loss:0.6027 | Valid Acc:0.3553 | Test Acc:0.3487
[Epoch:330] Train Loss:0.6111 | Valid Acc:0.3579 | Test Acc:0.3480
[Epoch:340] Train Loss:0.6016 | Valid Acc:0.3566 | Test Acc:0.3572
[Epoch:350] Train Loss:0.6114 | Valid Acc:0.3553 | Test Acc:0.3428
[Epoch:360] Train Loss:0.5701 | Valid Acc:0.3461 | Test Acc:0.3454
[Epoch:370] Train Loss:0.5769 | Valid Acc:0.3447 | Test Acc:0.3428
[Epoch:380] Train Loss:0.5724 | Valid Acc:0.3500 | Test Acc:0.3414
[Epoch:390] Train Loss:0.5488 | Valid Acc:0.3454 | Test Acc:0.3474
[Epoch:400] Train Loss:0.5655 | Valid Acc:0.3461 | Test Acc:0.3507
[Epoch:410] Train Loss:0.5530 | Valid Acc:0.3474 | Test Acc:0.3493
[Epoch:420] Train Loss:0.5491 | Valid Acc:0.3447 | Test Acc:0.3447
[Epoch:430] Train Loss:0.5441 | Valid Acc:0.3467 | Test Acc:0.3454
[Epoch:440] Train Loss:0.5452 | Valid Acc:0.3461 | Test Acc:0.3500
[Epoch:450] Train Loss:0.5447 | Valid Acc:0.3559 | Test Acc:0.3586
[Epoch:460] Train Loss:0.5314 | Valid Acc:0.3507 | Test Acc:0.3513
[Epoch:470] Train Loss:0.5275 | Valid Acc:0.3500 | Test Acc:0.3487
[Epoch:480] Train Loss:0.5250 | Valid Acc:0.3461 | Test Acc:0.3408
[Epoch:490] Train Loss:0.5320 | Valid Acc:0.3454 | Test Acc:0.3474
[Epoch:500] Train Loss:0.5333 | Valid Acc:0.3461 | Test Acc:0.3414
[Epoch:510] Train Loss:0.5054 | Valid Acc:0.3520 | Test Acc:0.3454
[Epoch:520] Train Loss:0.5081 | Valid Acc:0.3421 | Test Acc:0.3414
[Epoch:530] Train Loss:0.5099 | Valid Acc:0.3474 | Test Acc:0.3388
[Epoch:540] Train Loss:0.5222 | Valid Acc:0.3467 | Test Acc:0.3368
[Epoch:550] Train Loss:0.5157 | Valid Acc:0.3513 | Test Acc:0.3388
[Epoch:560] Train Loss:0.5123 | Valid Acc:0.3493 | Test Acc:0.3382
[Epoch:570] Train Loss:0.5148 | Valid Acc:0.3454 | Test Acc:0.3401
[Epoch:580] Train Loss:0.5055 | Valid Acc:0.3513 | Test Acc:0.3401
[Epoch:590] Train Loss:0.5002 | Valid Acc:0.3480 | Test Acc:0.3434
[Epoch:600] Train Loss:0.5070 | Valid Acc:0.3474 | Test Acc:0.3395
[Epoch:610] Train Loss:0.4987 | Valid Acc:0.3454 | Test Acc:0.3336
[Epoch:620] Train Loss:0.4898 | Valid Acc:0.3493 | Test Acc:0.3362
[Epoch:630] Train Loss:0.5148 | Valid Acc:0.3461 | Test Acc:0.3401
[Epoch:640] Train Loss:0.4942 | Valid Acc:0.3461 | Test Acc:0.3395
[Epoch:650] Train Loss:0.5165 | Valid Acc:0.3487 | Test Acc:0.3388
[Epoch:660] Train Loss:0.4950 | Valid Acc:0.3441 | Test Acc:0.3414
[Epoch:670] Train Loss:0.4918 | Valid Acc:0.3493 | Test Acc:0.3401
[Epoch:680] Train Loss:0.4907 | Valid Acc:0.3487 | Test Acc:0.3375
[Epoch:690] Train Loss:0.4953 | Valid Acc:0.3447 | Test Acc:0.3428
[Epoch:700] Train Loss:0.4851 | Valid Acc:0.3487 | Test Acc:0.3408
[Epoch:710] Train Loss:0.4909 | Valid Acc:0.3467 | Test Acc:0.3375
[Epoch:720] Train Loss:0.4992 | Valid Acc:0.3474 | Test Acc:0.3382
[Epoch:730] Train Loss:0.4808 | Valid Acc:0.3461 | Test Acc:0.3401
[Epoch:740] Train Loss:0.4993 | Valid Acc:0.3454 | Test Acc:0.3401
[Epoch:750] Train Loss:0.4834 | Valid Acc:0.3467 | Test Acc:0.3395
[Epoch:760] Train Loss:0.4893 | Valid Acc:0.3447 | Test Acc:0.3414
[Epoch:770] Train Loss:0.4914 | Valid Acc:0.3467 | Test Acc:0.3408
[Epoch:780] Train Loss:0.4871 | Valid Acc:0.3467 | Test Acc:0.3401
[Epoch:790] Train Loss:0.4944 | Valid Acc:0.3467 | Test Acc:0.3401
Best results: epoch=52, valid loss=1.3491, valid acc=0.3961, test loss:1.3590,test acc=0.4007
{'activation': 'elu', 'dropout': 5, 'hidden_size': 16, 'learning_rate': 0.00638179814855209, 'model': 'f2gnn', 'optimizer': 'adam', 'weight_decay': 0.0007958388830195125}
Current Hyperparameter: {'activation': 'elu', 'dropout': 5, 'hidden_size': 16, 'learning_rate': 0.00638179814855209, 'model': 'f2gnn', 'optimizer': 'adam', 'weight_decay': 0.0007958388830195125}
[Epoch:0] Train Loss:1.6201 | Valid Acc:0.2645 | Test Acc:0.2658
[Epoch:10] Train Loss:1.5743 | Valid Acc:0.3145 | Test Acc:0.3164
[Epoch:20] Train Loss:1.5610 | Valid Acc:0.2862 | Test Acc:0.2855
[Epoch:30] Train Loss:1.5127 | Valid Acc:0.2967 | Test Acc:0.3158
[Epoch:40] Train Loss:1.4261 | Valid Acc:0.3553 | Test Acc:0.3737
[Epoch:50] Train Loss:1.3496 | Valid Acc:0.3757 | Test Acc:0.3921
[Epoch:60] Train Loss:1.3094 | Valid Acc:0.3862 | Test Acc:0.3974
[Epoch:70] Train Loss:1.2691 | Valid Acc:0.3842 | Test Acc:0.3934
[Epoch:80] Train Loss:1.2431 | Valid Acc:0.3809 | Test Acc:0.3934
[Epoch:90] Train Loss:1.2216 | Valid Acc:0.3816 | Test Acc:0.3967
[Epoch:100] Train Loss:1.1947 | Valid Acc:0.3796 | Test Acc:0.3829
[Epoch:110] Train Loss:1.1702 | Valid Acc:0.3796 | Test Acc:0.3895
[Epoch:120] Train Loss:1.1624 | Valid Acc:0.3711 | Test Acc:0.3921
[Epoch:130] Train Loss:1.1695 | Valid Acc:0.3711 | Test Acc:0.3803
[Epoch:140] Train Loss:1.1280 | Valid Acc:0.3796 | Test Acc:0.3783
[Epoch:150] Train Loss:1.1254 | Valid Acc:0.3691 | Test Acc:0.3704
[Epoch:160] Train Loss:1.1087 | Valid Acc:0.3678 | Test Acc:0.3717
[Epoch:170] Train Loss:1.0933 | Valid Acc:0.3822 | Test Acc:0.3743
[Epoch:180] Train Loss:1.0908 | Valid Acc:0.3757 | Test Acc:0.3743
[Epoch:190] Train Loss:1.0852 | Valid Acc:0.3750 | Test Acc:0.3691
[Epoch:200] Train Loss:1.0826 | Valid Acc:0.3553 | Test Acc:0.3671
[Epoch:210] Train Loss:1.0599 | Valid Acc:0.3618 | Test Acc:0.3789
[Epoch:220] Train Loss:1.0653 | Valid Acc:0.3566 | Test Acc:0.3618
[Epoch:230] Train Loss:1.0525 | Valid Acc:0.3645 | Test Acc:0.3763
[Epoch:240] Train Loss:1.0413 | Valid Acc:0.3691 | Test Acc:0.3770
[Epoch:250] Train Loss:1.0262 | Valid Acc:0.3789 | Test Acc:0.3711
[Epoch:260] Train Loss:1.0119 | Valid Acc:0.3757 | Test Acc:0.3789
[Epoch:270] Train Loss:1.0120 | Valid Acc:0.3638 | Test Acc:0.3579
[Epoch:280] Train Loss:0.9899 | Valid Acc:0.3664 | Test Acc:0.3697
[Epoch:290] Train Loss:1.0004 | Valid Acc:0.3605 | Test Acc:0.3783
[Epoch:300] Train Loss:0.9879 | Valid Acc:0.3605 | Test Acc:0.3717
[Epoch:310] Train Loss:0.9549 | Valid Acc:0.3599 | Test Acc:0.3849
[Epoch:320] Train Loss:0.9729 | Valid Acc:0.3618 | Test Acc:0.3724
[Epoch:330] Train Loss:0.9520 | Valid Acc:0.3586 | Test Acc:0.3757
[Epoch:340] Train Loss:0.9627 | Valid Acc:0.3618 | Test Acc:0.3763
[Epoch:350] Train Loss:0.9523 | Valid Acc:0.3526 | Test Acc:0.3704
[Epoch:360] Train Loss:0.9458 | Valid Acc:0.3618 | Test Acc:0.3711
[Epoch:370] Train Loss:0.9853 | Valid Acc:0.3638 | Test Acc:0.3625
[Epoch:380] Train Loss:0.9931 | Valid Acc:0.3730 | Test Acc:0.3704
[Epoch:390] Train Loss:0.9639 | Valid Acc:0.3664 | Test Acc:0.3724
[Epoch:400] Train Loss:0.9528 | Valid Acc:0.3645 | Test Acc:0.3757
[Epoch:410] Train Loss:0.9430 | Valid Acc:0.3632 | Test Acc:0.3691
[Epoch:420] Train Loss:0.9417 | Valid Acc:0.3645 | Test Acc:0.3671
[Epoch:430] Train Loss:0.9307 | Valid Acc:0.3684 | Test Acc:0.3697
[Epoch:440] Train Loss:0.9312 | Valid Acc:0.3664 | Test Acc:0.3691
[Epoch:450] Train Loss:0.9217 | Valid Acc:0.3717 | Test Acc:0.3704
[Epoch:460] Train Loss:0.9162 | Valid Acc:0.3664 | Test Acc:0.3658
[Epoch:470] Train Loss:0.9172 | Valid Acc:0.3658 | Test Acc:0.3757
[Epoch:480] Train Loss:0.9003 | Valid Acc:0.3691 | Test Acc:0.3776
[Epoch:490] Train Loss:0.9135 | Valid Acc:0.3664 | Test Acc:0.3724
[Epoch:500] Train Loss:0.8897 | Valid Acc:0.3664 | Test Acc:0.3671
[Epoch:510] Train Loss:0.8949 | Valid Acc:0.3658 | Test Acc:0.3717
[Epoch:520] Train Loss:0.8949 | Valid Acc:0.3671 | Test Acc:0.3796
[Epoch:530] Train Loss:0.8898 | Valid Acc:0.3704 | Test Acc:0.3737
[Epoch:540] Train Loss:0.8821 | Valid Acc:0.3684 | Test Acc:0.3684
[Epoch:550] Train Loss:0.8885 | Valid Acc:0.3717 | Test Acc:0.3730
[Epoch:560] Train Loss:0.8823 | Valid Acc:0.3664 | Test Acc:0.3717
[Epoch:570] Train Loss:0.8832 | Valid Acc:0.3684 | Test Acc:0.3711
[Epoch:580] Train Loss:0.8789 | Valid Acc:0.3678 | Test Acc:0.3704
[Epoch:590] Train Loss:0.8842 | Valid Acc:0.3691 | Test Acc:0.3691
[Epoch:600] Train Loss:0.8717 | Valid Acc:0.3697 | Test Acc:0.3684
[Epoch:610] Train Loss:0.8631 | Valid Acc:0.3664 | Test Acc:0.3664
[Epoch:620] Train Loss:0.8682 | Valid Acc:0.3638 | Test Acc:0.3678
[Epoch:630] Train Loss:0.8634 | Valid Acc:0.3625 | Test Acc:0.3717
[Epoch:640] Train Loss:0.8605 | Valid Acc:0.3625 | Test Acc:0.3664
[Epoch:650] Train Loss:0.8716 | Valid Acc:0.3658 | Test Acc:0.3697
[Epoch:660] Train Loss:0.8617 | Valid Acc:0.3645 | Test Acc:0.3717
[Epoch:670] Train Loss:0.8662 | Valid Acc:0.3658 | Test Acc:0.3704
[Epoch:680] Train Loss:0.8720 | Valid Acc:0.3658 | Test Acc:0.3711
[Epoch:690] Train Loss:0.8560 | Valid Acc:0.3658 | Test Acc:0.3664
[Epoch:700] Train Loss:0.8596 | Valid Acc:0.3664 | Test Acc:0.3711
[Epoch:710] Train Loss:0.8654 | Valid Acc:0.3684 | Test Acc:0.3684
[Epoch:720] Train Loss:0.8563 | Valid Acc:0.3671 | Test Acc:0.3697
[Epoch:730] Train Loss:0.8593 | Valid Acc:0.3658 | Test Acc:0.3691
[Epoch:740] Train Loss:0.8525 | Valid Acc:0.3651 | Test Acc:0.3704
[Epoch:750] Train Loss:0.8594 | Valid Acc:0.3658 | Test Acc:0.3691
[Epoch:760] Train Loss:0.8541 | Valid Acc:0.3664 | Test Acc:0.3691
[Epoch:770] Train Loss:0.8568 | Valid Acc:0.3671 | Test Acc:0.3691
[Epoch:780] Train Loss:0.8558 | Valid Acc:0.3664 | Test Acc:0.3691
[Epoch:790] Train Loss:0.8560 | Valid Acc:0.3664 | Test Acc:0.3691
Best results: epoch=57, valid loss=1.3543, valid acc=0.3947, test loss:1.3644,test acc=0.3974
{'activation': 'elu', 'dropout': 6, 'hidden_size': 128, 'learning_rate': 0.004961381919489586, 'model': 'f2gnn', 'optimizer': 'adagrad', 'weight_decay': 0.00043511078956465086}
Current Hyperparameter: {'activation': 'elu', 'dropout': 6, 'hidden_size': 128, 'learning_rate': 0.004961381919489586, 'model': 'f2gnn', 'optimizer': 'adagrad', 'weight_decay': 0.00043511078956465086}
[Epoch:0] Train Loss:1.7480 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:10] Train Loss:1.5889 | Valid Acc:0.2355 | Test Acc:0.2362
[Epoch:20] Train Loss:1.5784 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:30] Train Loss:1.5710 | Valid Acc:0.2750 | Test Acc:0.2684
[Epoch:40] Train Loss:1.5680 | Valid Acc:0.3046 | Test Acc:0.2868
[Epoch:50] Train Loss:1.5625 | Valid Acc:0.2546 | Test Acc:0.2599
[Epoch:60] Train Loss:1.5451 | Valid Acc:0.3211 | Test Acc:0.3151
[Epoch:70] Train Loss:1.5371 | Valid Acc:0.2553 | Test Acc:0.2605
[Epoch:80] Train Loss:1.5079 | Valid Acc:0.2908 | Test Acc:0.2947
[Epoch:90] Train Loss:1.4866 | Valid Acc:0.2836 | Test Acc:0.2836
[Epoch:100] Train Loss:1.4602 | Valid Acc:0.3309 | Test Acc:0.3184
[Epoch:110] Train Loss:1.4456 | Valid Acc:0.3026 | Test Acc:0.3053
[Epoch:120] Train Loss:1.4332 | Valid Acc:0.3296 | Test Acc:0.3309
[Epoch:130] Train Loss:1.4161 | Valid Acc:0.3230 | Test Acc:0.3224
[Epoch:140] Train Loss:1.4063 | Valid Acc:0.3355 | Test Acc:0.3270
[Epoch:150] Train Loss:1.3901 | Valid Acc:0.3329 | Test Acc:0.3263
[Epoch:160] Train Loss:1.3775 | Valid Acc:0.3441 | Test Acc:0.3414
[Epoch:170] Train Loss:1.3813 | Valid Acc:0.3118 | Test Acc:0.3151
[Epoch:180] Train Loss:1.3550 | Valid Acc:0.3474 | Test Acc:0.3507
[Epoch:190] Train Loss:1.3565 | Valid Acc:0.3349 | Test Acc:0.3349
[Epoch:200] Train Loss:1.3554 | Valid Acc:0.3336 | Test Acc:0.3375
[Epoch:210] Train Loss:1.3356 | Valid Acc:0.3513 | Test Acc:0.3539
[Epoch:220] Train Loss:1.3323 | Valid Acc:0.3520 | Test Acc:0.3605
[Epoch:230] Train Loss:1.3422 | Valid Acc:0.3230 | Test Acc:0.3243
[Epoch:240] Train Loss:1.3287 | Valid Acc:0.3934 | Test Acc:0.3908
[Epoch:250] Train Loss:1.3253 | Valid Acc:0.3342 | Test Acc:0.3382
[Epoch:260] Train Loss:1.3159 | Valid Acc:0.3796 | Test Acc:0.3789
[Epoch:270] Train Loss:1.3134 | Valid Acc:0.3737 | Test Acc:0.3737
[Epoch:280] Train Loss:1.3155 | Valid Acc:0.3638 | Test Acc:0.3743
[Epoch:290] Train Loss:1.3129 | Valid Acc:0.3513 | Test Acc:0.3684
[Epoch:300] Train Loss:1.3012 | Valid Acc:0.3862 | Test Acc:0.3803
[Epoch:310] Train Loss:1.3000 | Valid Acc:0.3730 | Test Acc:0.3757
[Epoch:320] Train Loss:1.2969 | Valid Acc:0.3921 | Test Acc:0.3803
[Epoch:330] Train Loss:1.3011 | Valid Acc:0.3842 | Test Acc:0.3776
[Epoch:340] Train Loss:1.2899 | Valid Acc:0.3895 | Test Acc:0.3816
[Epoch:350] Train Loss:1.2948 | Valid Acc:0.3934 | Test Acc:0.3862
[Epoch:360] Train Loss:1.2842 | Valid Acc:0.3993 | Test Acc:0.3980
[Epoch:370] Train Loss:1.2888 | Valid Acc:0.3993 | Test Acc:0.3974
[Epoch:380] Train Loss:1.2862 | Valid Acc:0.4020 | Test Acc:0.3901
[Epoch:390] Train Loss:1.2872 | Valid Acc:0.3993 | Test Acc:0.3888
[Epoch:400] Train Loss:1.2802 | Valid Acc:0.4026 | Test Acc:0.3914
[Epoch:410] Train Loss:1.2768 | Valid Acc:0.4033 | Test Acc:0.3934
[Epoch:420] Train Loss:1.2780 | Valid Acc:0.4013 | Test Acc:0.3967
[Epoch:430] Train Loss:1.2745 | Valid Acc:0.4000 | Test Acc:0.3974
[Epoch:440] Train Loss:1.2790 | Valid Acc:0.3980 | Test Acc:0.3974
[Epoch:450] Train Loss:1.2697 | Valid Acc:0.3993 | Test Acc:0.3993
[Epoch:460] Train Loss:1.2738 | Valid Acc:0.4007 | Test Acc:0.4046
[Epoch:470] Train Loss:1.2821 | Valid Acc:0.4000 | Test Acc:0.3888
[Epoch:480] Train Loss:1.2629 | Valid Acc:0.3967 | Test Acc:0.3954
[Epoch:490] Train Loss:1.2668 | Valid Acc:0.3934 | Test Acc:0.3947
[Epoch:500] Train Loss:1.2673 | Valid Acc:0.3954 | Test Acc:0.4007
[Epoch:510] Train Loss:1.2592 | Valid Acc:0.3928 | Test Acc:0.4000
[Epoch:520] Train Loss:1.2636 | Valid Acc:0.3928 | Test Acc:0.4007
[Epoch:530] Train Loss:1.2686 | Valid Acc:0.3934 | Test Acc:0.3987
[Epoch:540] Train Loss:1.2683 | Valid Acc:0.3908 | Test Acc:0.4066
[Epoch:550] Train Loss:1.2636 | Valid Acc:0.3941 | Test Acc:0.4020
[Epoch:560] Train Loss:1.2606 | Valid Acc:0.3921 | Test Acc:0.4020
[Epoch:570] Train Loss:1.2602 | Valid Acc:0.3928 | Test Acc:0.4007
[Epoch:580] Train Loss:1.2601 | Valid Acc:0.3941 | Test Acc:0.4013
[Epoch:590] Train Loss:1.2674 | Valid Acc:0.3941 | Test Acc:0.4026
[Epoch:600] Train Loss:1.2616 | Valid Acc:0.3914 | Test Acc:0.4079
[Epoch:610] Train Loss:1.2605 | Valid Acc:0.3928 | Test Acc:0.4020
[Epoch:620] Train Loss:1.2608 | Valid Acc:0.3921 | Test Acc:0.4020
[Epoch:630] Train Loss:1.2617 | Valid Acc:0.3908 | Test Acc:0.4053
[Epoch:640] Train Loss:1.2586 | Valid Acc:0.3941 | Test Acc:0.4053
[Epoch:650] Train Loss:1.2554 | Valid Acc:0.3928 | Test Acc:0.4053
[Epoch:660] Train Loss:1.2550 | Valid Acc:0.3921 | Test Acc:0.4039
[Epoch:670] Train Loss:1.2620 | Valid Acc:0.3908 | Test Acc:0.4020
[Epoch:680] Train Loss:1.2588 | Valid Acc:0.3928 | Test Acc:0.4033
[Epoch:690] Train Loss:1.2567 | Valid Acc:0.3928 | Test Acc:0.4046
[Epoch:700] Train Loss:1.2612 | Valid Acc:0.3928 | Test Acc:0.4053
[Epoch:710] Train Loss:1.2586 | Valid Acc:0.3941 | Test Acc:0.4053
[Epoch:720] Train Loss:1.2595 | Valid Acc:0.3941 | Test Acc:0.4079
[Epoch:730] Train Loss:1.2549 | Valid Acc:0.3934 | Test Acc:0.4079
[Epoch:740] Train Loss:1.2502 | Valid Acc:0.3947 | Test Acc:0.4072
[Epoch:750] Train Loss:1.2663 | Valid Acc:0.3941 | Test Acc:0.4072
[Epoch:760] Train Loss:1.2480 | Valid Acc:0.3947 | Test Acc:0.4066
[Epoch:770] Train Loss:1.2648 | Valid Acc:0.3941 | Test Acc:0.4072
[Epoch:780] Train Loss:1.2593 | Valid Acc:0.3941 | Test Acc:0.4072
[Epoch:790] Train Loss:1.2576 | Valid Acc:0.3941 | Test Acc:0.4072
Best results: epoch=395, valid loss=1.3481, valid acc=0.4066, test loss:1.3584,test acc=0.3928
{'activation': 'elu', 'dropout': 4, 'hidden_size': 512, 'learning_rate': 0.009325756755386189, 'model': 'f2gnn', 'optimizer': 'adagrad', 'weight_decay': 0.0007380856388062606}
Current Hyperparameter: {'activation': 'elu', 'dropout': 4, 'hidden_size': 512, 'learning_rate': 0.009325756755386189, 'model': 'f2gnn', 'optimizer': 'adagrad', 'weight_decay': 0.0007380856388062606}
[Epoch:0] Train Loss:1.6612 | Valid Acc:0.2388 | Test Acc:0.2388
[Epoch:10] Train Loss:1.5971 | Valid Acc:0.2388 | Test Acc:0.2388
[Epoch:20] Train Loss:1.5858 | Valid Acc:0.2388 | Test Acc:0.2388
[Epoch:30] Train Loss:1.5812 | Valid Acc:0.2388 | Test Acc:0.2388
[Epoch:40] Train Loss:1.5777 | Valid Acc:0.2388 | Test Acc:0.2388
[Epoch:50] Train Loss:1.5765 | Valid Acc:0.2388 | Test Acc:0.2388
[Epoch:60] Train Loss:1.5743 | Valid Acc:0.2388 | Test Acc:0.2388
[Epoch:70] Train Loss:1.5778 | Valid Acc:0.2388 | Test Acc:0.2388
[Epoch:80] Train Loss:1.5746 | Valid Acc:0.2388 | Test Acc:0.2388
[Epoch:90] Train Loss:1.5725 | Valid Acc:0.2388 | Test Acc:0.2388
[Epoch:100] Train Loss:1.5700 | Valid Acc:0.2388 | Test Acc:0.2388
[Epoch:110] Train Loss:1.5720 | Valid Acc:0.2388 | Test Acc:0.2388
[Epoch:120] Train Loss:1.5690 | Valid Acc:0.2388 | Test Acc:0.2388
[Epoch:130] Train Loss:1.5693 | Valid Acc:0.2388 | Test Acc:0.2395
[Epoch:140] Train Loss:1.5687 | Valid Acc:0.2388 | Test Acc:0.2401
[Epoch:150] Train Loss:1.5661 | Valid Acc:0.2395 | Test Acc:0.2414
[Epoch:160] Train Loss:1.5629 | Valid Acc:0.2395 | Test Acc:0.2421
[Epoch:170] Train Loss:1.5602 | Valid Acc:0.2414 | Test Acc:0.2441
[Epoch:180] Train Loss:1.5555 | Valid Acc:0.2428 | Test Acc:0.2474
[Epoch:190] Train Loss:1.5478 | Valid Acc:0.2454 | Test Acc:0.2513
[Epoch:200] Train Loss:1.5453 | Valid Acc:0.2507 | Test Acc:0.2539
[Epoch:210] Train Loss:1.5425 | Valid Acc:0.2599 | Test Acc:0.2592
[Epoch:220] Train Loss:1.5283 | Valid Acc:0.2625 | Test Acc:0.2645
[Epoch:230] Train Loss:1.5198 | Valid Acc:0.2704 | Test Acc:0.2691
[Epoch:240] Train Loss:1.5069 | Valid Acc:0.2743 | Test Acc:0.2737
[Epoch:250] Train Loss:1.4969 | Valid Acc:0.2822 | Test Acc:0.2809
[Epoch:260] Train Loss:1.4894 | Valid Acc:0.2868 | Test Acc:0.2875
[Epoch:270] Train Loss:1.4852 | Valid Acc:0.2882 | Test Acc:0.2875
[Epoch:280] Train Loss:1.4764 | Valid Acc:0.2895 | Test Acc:0.2868
[Epoch:290] Train Loss:1.4678 | Valid Acc:0.2928 | Test Acc:0.2888
[Epoch:300] Train Loss:1.4669 | Valid Acc:0.3013 | Test Acc:0.2849
[Epoch:310] Train Loss:1.4497 | Valid Acc:0.3125 | Test Acc:0.2888
[Epoch:320] Train Loss:1.4531 | Valid Acc:0.3059 | Test Acc:0.2763
[Epoch:330] Train Loss:1.4385 | Valid Acc:0.2941 | Test Acc:0.2625
[Epoch:340] Train Loss:1.4366 | Valid Acc:0.2888 | Test Acc:0.2579
[Epoch:350] Train Loss:1.4315 | Valid Acc:0.2842 | Test Acc:0.2664
[Epoch:360] Train Loss:1.4320 | Valid Acc:0.2882 | Test Acc:0.2684
[Epoch:370] Train Loss:1.4215 | Valid Acc:0.2875 | Test Acc:0.2612
[Epoch:380] Train Loss:1.4123 | Valid Acc:0.2789 | Test Acc:0.2618
[Epoch:390] Train Loss:1.4151 | Valid Acc:0.2809 | Test Acc:0.2625
[Epoch:400] Train Loss:1.4117 | Valid Acc:0.2836 | Test Acc:0.2546
[Epoch:410] Train Loss:1.4042 | Valid Acc:0.2816 | Test Acc:0.2526
[Epoch:420] Train Loss:1.3963 | Valid Acc:0.2796 | Test Acc:0.2572
[Epoch:430] Train Loss:1.4081 | Valid Acc:0.2743 | Test Acc:0.2572
[Epoch:440] Train Loss:1.3972 | Valid Acc:0.2803 | Test Acc:0.2553
[Epoch:450] Train Loss:1.3942 | Valid Acc:0.2809 | Test Acc:0.2579
[Epoch:460] Train Loss:1.3899 | Valid Acc:0.2816 | Test Acc:0.2546
[Epoch:470] Train Loss:1.3845 | Valid Acc:0.2789 | Test Acc:0.2579
[Epoch:480] Train Loss:1.3830 | Valid Acc:0.2803 | Test Acc:0.2533
[Epoch:490] Train Loss:1.3859 | Valid Acc:0.2789 | Test Acc:0.2526
[Epoch:500] Train Loss:1.3779 | Valid Acc:0.2829 | Test Acc:0.2533
[Epoch:510] Train Loss:1.3736 | Valid Acc:0.2816 | Test Acc:0.2526
[Epoch:520] Train Loss:1.3812 | Valid Acc:0.2822 | Test Acc:0.2526
[Epoch:530] Train Loss:1.3696 | Valid Acc:0.2809 | Test Acc:0.2533
[Epoch:540] Train Loss:1.3718 | Valid Acc:0.2803 | Test Acc:0.2533
[Epoch:550] Train Loss:1.3693 | Valid Acc:0.2849 | Test Acc:0.2546
[Epoch:560] Train Loss:1.3676 | Valid Acc:0.2816 | Test Acc:0.2513
[Epoch:570] Train Loss:1.3658 | Valid Acc:0.2803 | Test Acc:0.2526
[Epoch:580] Train Loss:1.3708 | Valid Acc:0.2836 | Test Acc:0.2566
[Epoch:590] Train Loss:1.3695 | Valid Acc:0.2822 | Test Acc:0.2546
[Epoch:600] Train Loss:1.3740 | Valid Acc:0.2822 | Test Acc:0.2546
[Epoch:610] Train Loss:1.3635 | Valid Acc:0.2829 | Test Acc:0.2566
[Epoch:620] Train Loss:1.3701 | Valid Acc:0.2829 | Test Acc:0.2566
[Epoch:630] Train Loss:1.3693 | Valid Acc:0.2822 | Test Acc:0.2572
[Epoch:640] Train Loss:1.3586 | Valid Acc:0.2842 | Test Acc:0.2579
[Epoch:650] Train Loss:1.3638 | Valid Acc:0.2836 | Test Acc:0.2579
[Epoch:660] Train Loss:1.3689 | Valid Acc:0.2829 | Test Acc:0.2579
[Epoch:670] Train Loss:1.3659 | Valid Acc:0.2829 | Test Acc:0.2572
[Epoch:680] Train Loss:1.3655 | Valid Acc:0.2829 | Test Acc:0.2572
[Epoch:690] Train Loss:1.3628 | Valid Acc:0.2836 | Test Acc:0.2599
[Epoch:700] Train Loss:1.3663 | Valid Acc:0.2836 | Test Acc:0.2592
[Epoch:710] Train Loss:1.3629 | Valid Acc:0.2829 | Test Acc:0.2572
[Epoch:720] Train Loss:1.3633 | Valid Acc:0.2829 | Test Acc:0.2592
[Epoch:730] Train Loss:1.3642 | Valid Acc:0.2829 | Test Acc:0.2592
[Epoch:740] Train Loss:1.3570 | Valid Acc:0.2836 | Test Acc:0.2592
[Epoch:750] Train Loss:1.3580 | Valid Acc:0.2829 | Test Acc:0.2592
[Epoch:760] Train Loss:1.3609 | Valid Acc:0.2829 | Test Acc:0.2592
[Epoch:770] Train Loss:1.3644 | Valid Acc:0.2829 | Test Acc:0.2592
[Epoch:780] Train Loss:1.3566 | Valid Acc:0.2829 | Test Acc:0.2592
[Epoch:790] Train Loss:1.3604 | Valid Acc:0.2829 | Test Acc:0.2592
Best results: epoch=314, valid loss=1.5417, valid acc=0.3178, test loss:1.5456,test acc=0.2882
{'activation': 'relu', 'dropout': 1, 'hidden_size': 32, 'learning_rate': 0.008465855346949748, 'model': 'f2gnn', 'optimizer': 'adam', 'weight_decay': 0.0009435520206887406}
Current Hyperparameter: {'activation': 'relu', 'dropout': 1, 'hidden_size': 32, 'learning_rate': 0.008465855346949748, 'model': 'f2gnn', 'optimizer': 'adam', 'weight_decay': 0.0009435520206887406}
[Epoch:0] Train Loss:1.5970 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:10] Train Loss:1.5370 | Valid Acc:0.2625 | Test Acc:0.2592
[Epoch:20] Train Loss:1.4364 | Valid Acc:0.3592 | Test Acc:0.3632
[Epoch:30] Train Loss:1.3136 | Valid Acc:0.3914 | Test Acc:0.3987
[Epoch:40] Train Loss:1.2648 | Valid Acc:0.3809 | Test Acc:0.3875
[Epoch:50] Train Loss:1.2218 | Valid Acc:0.3842 | Test Acc:0.3776
[Epoch:60] Train Loss:1.1705 | Valid Acc:0.3612 | Test Acc:0.3895
[Epoch:70] Train Loss:1.1513 | Valid Acc:0.3625 | Test Acc:0.3822
[Epoch:80] Train Loss:1.1111 | Valid Acc:0.3704 | Test Acc:0.3875
[Epoch:90] Train Loss:1.0564 | Valid Acc:0.3763 | Test Acc:0.3855
[Epoch:100] Train Loss:1.0252 | Valid Acc:0.3829 | Test Acc:0.3605
[Epoch:110] Train Loss:0.9524 | Valid Acc:0.3730 | Test Acc:0.3645
[Epoch:120] Train Loss:0.9054 | Valid Acc:0.3546 | Test Acc:0.3599
[Epoch:130] Train Loss:0.8657 | Valid Acc:0.3579 | Test Acc:0.3664
[Epoch:140] Train Loss:0.8211 | Valid Acc:0.3664 | Test Acc:0.3579
[Epoch:150] Train Loss:0.7657 | Valid Acc:0.3592 | Test Acc:0.3566
[Epoch:160] Train Loss:0.7279 | Valid Acc:0.3579 | Test Acc:0.3658
[Epoch:170] Train Loss:0.7072 | Valid Acc:0.3480 | Test Acc:0.3579
[Epoch:180] Train Loss:0.6670 | Valid Acc:0.3421 | Test Acc:0.3605
[Epoch:190] Train Loss:0.7441 | Valid Acc:0.3434 | Test Acc:0.3428
[Epoch:200] Train Loss:0.6443 | Valid Acc:0.3421 | Test Acc:0.3605
[Epoch:210] Train Loss:0.5978 | Valid Acc:0.3533 | Test Acc:0.3539
[Epoch:220] Train Loss:0.5798 | Valid Acc:0.3414 | Test Acc:0.3572
[Epoch:230] Train Loss:0.5614 | Valid Acc:0.3454 | Test Acc:0.3572
[Epoch:240] Train Loss:0.5439 | Valid Acc:0.3461 | Test Acc:0.3493
[Epoch:250] Train Loss:0.5406 | Valid Acc:0.3368 | Test Acc:0.3618
[Epoch:260] Train Loss:0.5418 | Valid Acc:0.3382 | Test Acc:0.3454
[Epoch:270] Train Loss:0.5543 | Valid Acc:0.3539 | Test Acc:0.3638
[Epoch:280] Train Loss:0.5096 | Valid Acc:0.3500 | Test Acc:0.3467
[Epoch:290] Train Loss:0.5022 | Valid Acc:0.3441 | Test Acc:0.3507
[Epoch:300] Train Loss:0.4902 | Valid Acc:0.3467 | Test Acc:0.3474
[Epoch:310] Train Loss:0.4864 | Valid Acc:0.3513 | Test Acc:0.3467
[Epoch:320] Train Loss:0.4728 | Valid Acc:0.3507 | Test Acc:0.3434
[Epoch:330] Train Loss:0.4642 | Valid Acc:0.3526 | Test Acc:0.3533
[Epoch:340] Train Loss:0.4634 | Valid Acc:0.3388 | Test Acc:0.3461
[Epoch:350] Train Loss:0.4651 | Valid Acc:0.3434 | Test Acc:0.3553
[Epoch:360] Train Loss:0.4627 | Valid Acc:0.3467 | Test Acc:0.3428
[Epoch:370] Train Loss:0.4531 | Valid Acc:0.3454 | Test Acc:0.3487
[Epoch:380] Train Loss:0.4474 | Valid Acc:0.3428 | Test Acc:0.3401
[Epoch:390] Train Loss:0.4341 | Valid Acc:0.3441 | Test Acc:0.3441
[Epoch:400] Train Loss:0.4406 | Valid Acc:0.3408 | Test Acc:0.3434
[Epoch:410] Train Loss:0.4313 | Valid Acc:0.3408 | Test Acc:0.3421
[Epoch:420] Train Loss:0.4338 | Valid Acc:0.3382 | Test Acc:0.3414
[Epoch:430] Train Loss:0.4235 | Valid Acc:0.3421 | Test Acc:0.3493
[Epoch:440] Train Loss:0.4216 | Valid Acc:0.3401 | Test Acc:0.3428
[Epoch:450] Train Loss:0.4298 | Valid Acc:0.3500 | Test Acc:0.3428
[Epoch:460] Train Loss:0.4289 | Valid Acc:0.3480 | Test Acc:0.3467
[Epoch:470] Train Loss:0.4253 | Valid Acc:0.3461 | Test Acc:0.3401
[Epoch:480] Train Loss:0.4212 | Valid Acc:0.3454 | Test Acc:0.3447
[Epoch:490] Train Loss:0.4112 | Valid Acc:0.3395 | Test Acc:0.3441
[Epoch:500] Train Loss:0.4091 | Valid Acc:0.3493 | Test Acc:0.3428
[Epoch:510] Train Loss:0.4042 | Valid Acc:0.3474 | Test Acc:0.3368
[Epoch:520] Train Loss:0.4073 | Valid Acc:0.3487 | Test Acc:0.3388
[Epoch:530] Train Loss:0.4048 | Valid Acc:0.3428 | Test Acc:0.3382
[Epoch:540] Train Loss:0.4059 | Valid Acc:0.3382 | Test Acc:0.3355
[Epoch:550] Train Loss:0.4084 | Valid Acc:0.3454 | Test Acc:0.3434
[Epoch:560] Train Loss:0.4045 | Valid Acc:0.3474 | Test Acc:0.3382
[Epoch:570] Train Loss:0.4077 | Valid Acc:0.3414 | Test Acc:0.3441
[Epoch:580] Train Loss:0.4045 | Valid Acc:0.3382 | Test Acc:0.3401
[Epoch:590] Train Loss:0.3949 | Valid Acc:0.3382 | Test Acc:0.3355
[Epoch:600] Train Loss:0.3994 | Valid Acc:0.3408 | Test Acc:0.3355
[Epoch:610] Train Loss:0.4038 | Valid Acc:0.3434 | Test Acc:0.3342
[Epoch:620] Train Loss:0.3994 | Valid Acc:0.3401 | Test Acc:0.3368
[Epoch:630] Train Loss:0.3904 | Valid Acc:0.3368 | Test Acc:0.3355
[Epoch:640] Train Loss:0.3950 | Valid Acc:0.3395 | Test Acc:0.3336
[Epoch:650] Train Loss:0.3949 | Valid Acc:0.3441 | Test Acc:0.3401
[Epoch:660] Train Loss:0.4003 | Valid Acc:0.3368 | Test Acc:0.3336
[Epoch:670] Train Loss:0.3919 | Valid Acc:0.3388 | Test Acc:0.3349
[Epoch:680] Train Loss:0.3907 | Valid Acc:0.3349 | Test Acc:0.3382
[Epoch:690] Train Loss:0.3988 | Valid Acc:0.3349 | Test Acc:0.3362
[Epoch:700] Train Loss:0.3960 | Valid Acc:0.3395 | Test Acc:0.3401
[Epoch:710] Train Loss:0.3927 | Valid Acc:0.3395 | Test Acc:0.3368
[Epoch:720] Train Loss:0.3904 | Valid Acc:0.3362 | Test Acc:0.3368
[Epoch:730] Train Loss:0.3902 | Valid Acc:0.3382 | Test Acc:0.3382
[Epoch:740] Train Loss:0.3935 | Valid Acc:0.3368 | Test Acc:0.3375
[Epoch:750] Train Loss:0.3964 | Valid Acc:0.3375 | Test Acc:0.3368
[Epoch:760] Train Loss:0.3966 | Valid Acc:0.3382 | Test Acc:0.3401
[Epoch:770] Train Loss:0.3983 | Valid Acc:0.3362 | Test Acc:0.3362
[Epoch:780] Train Loss:0.3943 | Valid Acc:0.3362 | Test Acc:0.3388
[Epoch:790] Train Loss:0.3929 | Valid Acc:0.3362 | Test Acc:0.3388
Best results: epoch=27, valid loss=1.3402, valid acc=0.3967, test loss:1.3509,test acc=0.4007
{'activation': 'elu', 'dropout': 2, 'hidden_size': 128, 'learning_rate': 0.0021636872064609448, 'model': 'f2gnn', 'optimizer': 'adagrad', 'weight_decay': 0.00022626293599891593}
Current Hyperparameter: {'activation': 'elu', 'dropout': 2, 'hidden_size': 128, 'learning_rate': 0.0021636872064609448, 'model': 'f2gnn', 'optimizer': 'adagrad', 'weight_decay': 0.00022626293599891593}
[Epoch:0] Train Loss:1.6438 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:10] Train Loss:1.5711 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:20] Train Loss:1.5717 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:30] Train Loss:1.5660 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:40] Train Loss:1.5594 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:50] Train Loss:1.5529 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:60] Train Loss:1.5717 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:70] Train Loss:1.5471 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:80] Train Loss:1.5143 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:90] Train Loss:1.4960 | Valid Acc:0.2691 | Test Acc:0.2632
[Epoch:100] Train Loss:1.4826 | Valid Acc:0.2757 | Test Acc:0.2783
[Epoch:110] Train Loss:1.4686 | Valid Acc:0.2947 | Test Acc:0.3020
[Epoch:120] Train Loss:1.4527 | Valid Acc:0.3013 | Test Acc:0.3039
[Epoch:130] Train Loss:1.4322 | Valid Acc:0.3230 | Test Acc:0.3158
[Epoch:140] Train Loss:1.4179 | Valid Acc:0.3289 | Test Acc:0.3289
[Epoch:150] Train Loss:1.4092 | Valid Acc:0.3316 | Test Acc:0.3421
[Epoch:160] Train Loss:1.3888 | Valid Acc:0.3382 | Test Acc:0.3526
[Epoch:170] Train Loss:1.3889 | Valid Acc:0.3382 | Test Acc:0.3507
[Epoch:180] Train Loss:1.3732 | Valid Acc:0.3493 | Test Acc:0.3605
[Epoch:190] Train Loss:1.3635 | Valid Acc:0.3487 | Test Acc:0.3599
[Epoch:200] Train Loss:1.3454 | Valid Acc:0.3625 | Test Acc:0.3789
[Epoch:210] Train Loss:1.3561 | Valid Acc:0.3447 | Test Acc:0.3599
[Epoch:220] Train Loss:1.3319 | Valid Acc:0.3664 | Test Acc:0.3829
[Epoch:230] Train Loss:1.3289 | Valid Acc:0.3704 | Test Acc:0.3882
[Epoch:240] Train Loss:1.3253 | Valid Acc:0.3579 | Test Acc:0.3816
[Epoch:250] Train Loss:1.3259 | Valid Acc:0.3697 | Test Acc:0.3842
[Epoch:260] Train Loss:1.3140 | Valid Acc:0.3770 | Test Acc:0.3882
[Epoch:270] Train Loss:1.3108 | Valid Acc:0.3763 | Test Acc:0.3914
[Epoch:280] Train Loss:1.3006 | Valid Acc:0.3789 | Test Acc:0.3947
[Epoch:290] Train Loss:1.3093 | Valid Acc:0.3796 | Test Acc:0.3901
[Epoch:300] Train Loss:1.2994 | Valid Acc:0.3783 | Test Acc:0.3934
[Epoch:310] Train Loss:1.2955 | Valid Acc:0.3836 | Test Acc:0.3954
[Epoch:320] Train Loss:1.2913 | Valid Acc:0.3888 | Test Acc:0.3967
[Epoch:330] Train Loss:1.2894 | Valid Acc:0.3934 | Test Acc:0.3974
[Epoch:340] Train Loss:1.2786 | Valid Acc:0.3908 | Test Acc:0.4000
[Epoch:350] Train Loss:1.2869 | Valid Acc:0.3934 | Test Acc:0.3961
[Epoch:360] Train Loss:1.2818 | Valid Acc:0.3928 | Test Acc:0.4013
[Epoch:370] Train Loss:1.2756 | Valid Acc:0.3921 | Test Acc:0.3980
[Epoch:380] Train Loss:1.2730 | Valid Acc:0.4026 | Test Acc:0.4086
[Epoch:390] Train Loss:1.2745 | Valid Acc:0.4013 | Test Acc:0.4046
[Epoch:400] Train Loss:1.2615 | Valid Acc:0.4020 | Test Acc:0.4086
[Epoch:410] Train Loss:1.2631 | Valid Acc:0.4020 | Test Acc:0.4066
[Epoch:420] Train Loss:1.2621 | Valid Acc:0.4046 | Test Acc:0.4086
[Epoch:430] Train Loss:1.2634 | Valid Acc:0.3980 | Test Acc:0.4066
[Epoch:440] Train Loss:1.2533 | Valid Acc:0.4053 | Test Acc:0.4086
[Epoch:450] Train Loss:1.2624 | Valid Acc:0.4020 | Test Acc:0.4046
[Epoch:460] Train Loss:1.2590 | Valid Acc:0.3967 | Test Acc:0.4039
[Epoch:470] Train Loss:1.2604 | Valid Acc:0.4033 | Test Acc:0.4059
[Epoch:480] Train Loss:1.2533 | Valid Acc:0.4039 | Test Acc:0.4059
[Epoch:490] Train Loss:1.2537 | Valid Acc:0.4013 | Test Acc:0.4059
[Epoch:500] Train Loss:1.2544 | Valid Acc:0.4026 | Test Acc:0.4033
[Epoch:510] Train Loss:1.2515 | Valid Acc:0.4007 | Test Acc:0.4007
[Epoch:520] Train Loss:1.2528 | Valid Acc:0.4039 | Test Acc:0.4053
[Epoch:530] Train Loss:1.2498 | Valid Acc:0.4039 | Test Acc:0.4046
[Epoch:540] Train Loss:1.2464 | Valid Acc:0.4033 | Test Acc:0.4033
[Epoch:550] Train Loss:1.2511 | Valid Acc:0.4039 | Test Acc:0.4039
[Epoch:560] Train Loss:1.2492 | Valid Acc:0.4033 | Test Acc:0.4046
[Epoch:570] Train Loss:1.2492 | Valid Acc:0.4013 | Test Acc:0.4020
[Epoch:580] Train Loss:1.2476 | Valid Acc:0.4000 | Test Acc:0.4039
[Epoch:590] Train Loss:1.2413 | Valid Acc:0.4013 | Test Acc:0.4026
[Epoch:600] Train Loss:1.2483 | Valid Acc:0.4007 | Test Acc:0.4020
[Epoch:610] Train Loss:1.2403 | Valid Acc:0.4007 | Test Acc:0.4039
[Epoch:620] Train Loss:1.2472 | Valid Acc:0.4007 | Test Acc:0.4039
[Epoch:630] Train Loss:1.2356 | Valid Acc:0.4020 | Test Acc:0.4039
[Epoch:640] Train Loss:1.2442 | Valid Acc:0.4020 | Test Acc:0.4046
[Epoch:650] Train Loss:1.2412 | Valid Acc:0.4026 | Test Acc:0.4033
[Epoch:660] Train Loss:1.2469 | Valid Acc:0.4013 | Test Acc:0.4039
[Epoch:670] Train Loss:1.2455 | Valid Acc:0.4026 | Test Acc:0.4039
[Epoch:680] Train Loss:1.2432 | Valid Acc:0.4000 | Test Acc:0.4046
[Epoch:690] Train Loss:1.2448 | Valid Acc:0.4026 | Test Acc:0.4026
[Epoch:700] Train Loss:1.2417 | Valid Acc:0.4020 | Test Acc:0.4026
[Epoch:710] Train Loss:1.2474 | Valid Acc:0.4026 | Test Acc:0.4026
[Epoch:720] Train Loss:1.2440 | Valid Acc:0.4020 | Test Acc:0.4039
[Epoch:730] Train Loss:1.2447 | Valid Acc:0.4026 | Test Acc:0.4020
[Epoch:740] Train Loss:1.2411 | Valid Acc:0.4020 | Test Acc:0.4039
[Epoch:750] Train Loss:1.2476 | Valid Acc:0.4020 | Test Acc:0.4039
[Epoch:760] Train Loss:1.2413 | Valid Acc:0.4026 | Test Acc:0.4039
[Epoch:770] Train Loss:1.2410 | Valid Acc:0.4026 | Test Acc:0.4033
[Epoch:780] Train Loss:1.2403 | Valid Acc:0.4020 | Test Acc:0.4039
[Epoch:790] Train Loss:1.2421 | Valid Acc:0.4026 | Test Acc:0.4039
Best results: epoch=291, valid loss=1.3407, valid acc=0.4132, test loss:1.3437,test acc=0.4033
{'activation': 'elu', 'dropout': 2, 'hidden_size': 128, 'learning_rate': 0.0010257899344066897, 'model': 'f2gnn', 'optimizer': 'adagrad', 'weight_decay': 0.0001303237435931783}
Current Hyperparameter: {'activation': 'elu', 'dropout': 2, 'hidden_size': 128, 'learning_rate': 0.0010257899344066897, 'model': 'f2gnn', 'optimizer': 'adagrad', 'weight_decay': 0.0001303237435931783}
[Epoch:0] Train Loss:1.6438 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:10] Train Loss:1.5735 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:20] Train Loss:1.5763 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:30] Train Loss:1.5722 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:40] Train Loss:1.5721 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:50] Train Loss:1.5675 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:60] Train Loss:1.5660 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:70] Train Loss:1.5565 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:80] Train Loss:1.5577 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:90] Train Loss:1.5480 | Valid Acc:0.2612 | Test Acc:0.2612
[Epoch:100] Train Loss:1.5390 | Valid Acc:0.3171 | Test Acc:0.3316
[Epoch:110] Train Loss:1.5344 | Valid Acc:0.2770 | Test Acc:0.2796
[Epoch:120] Train Loss:1.5259 | Valid Acc:0.2605 | Test Acc:0.2605
[Epoch:130] Train Loss:1.5211 | Valid Acc:0.2671 | Test Acc:0.2638
[Epoch:140] Train Loss:1.5140 | Valid Acc:0.2691 | Test Acc:0.2651
[Epoch:150] Train Loss:1.5072 | Valid Acc:0.2697 | Test Acc:0.2651
[Epoch:160] Train Loss:1.4893 | Valid Acc:0.2829 | Test Acc:0.2822
[Epoch:170] Train Loss:1.4885 | Valid Acc:0.2789 | Test Acc:0.2783
[Epoch:180] Train Loss:1.4780 | Valid Acc:0.2862 | Test Acc:0.2901
[Epoch:190] Train Loss:1.4690 | Valid Acc:0.2954 | Test Acc:0.3039
[Epoch:200] Train Loss:1.4557 | Valid Acc:0.3197 | Test Acc:0.3204
[Epoch:210] Train Loss:1.4600 | Valid Acc:0.2914 | Test Acc:0.2947
[Epoch:220] Train Loss:1.4465 | Valid Acc:0.3151 | Test Acc:0.3217
[Epoch:230] Train Loss:1.4421 | Valid Acc:0.3303 | Test Acc:0.3329
[Epoch:240] Train Loss:1.4295 | Valid Acc:0.3224 | Test Acc:0.3211
[Epoch:250] Train Loss:1.4314 | Valid Acc:0.3263 | Test Acc:0.3289
[Epoch:260] Train Loss:1.4222 | Valid Acc:0.3270 | Test Acc:0.3342
[Epoch:270] Train Loss:1.4134 | Valid Acc:0.3276 | Test Acc:0.3322
[Epoch:280] Train Loss:1.4081 | Valid Acc:0.3395 | Test Acc:0.3375
[Epoch:290] Train Loss:1.4094 | Valid Acc:0.3355 | Test Acc:0.3375
[Epoch:300] Train Loss:1.3988 | Valid Acc:0.3408 | Test Acc:0.3487
[Epoch:310] Train Loss:1.4028 | Valid Acc:0.3454 | Test Acc:0.3658
[Epoch:320] Train Loss:1.3955 | Valid Acc:0.3513 | Test Acc:0.3658
[Epoch:330] Train Loss:1.3904 | Valid Acc:0.3467 | Test Acc:0.3638
[Epoch:340] Train Loss:1.3852 | Valid Acc:0.3533 | Test Acc:0.3678
[Epoch:350] Train Loss:1.3849 | Valid Acc:0.3507 | Test Acc:0.3651
[Epoch:360] Train Loss:1.3802 | Valid Acc:0.3553 | Test Acc:0.3671
[Epoch:370] Train Loss:1.3769 | Valid Acc:0.3507 | Test Acc:0.3678
[Epoch:380] Train Loss:1.3710 | Valid Acc:0.3539 | Test Acc:0.3697
[Epoch:390] Train Loss:1.3776 | Valid Acc:0.3493 | Test Acc:0.3664
[Epoch:400] Train Loss:1.3633 | Valid Acc:0.3507 | Test Acc:0.3645
[Epoch:410] Train Loss:1.3654 | Valid Acc:0.3546 | Test Acc:0.3651
[Epoch:420] Train Loss:1.3652 | Valid Acc:0.3539 | Test Acc:0.3664
[Epoch:430] Train Loss:1.3613 | Valid Acc:0.3487 | Test Acc:0.3651
[Epoch:440] Train Loss:1.3529 | Valid Acc:0.3605 | Test Acc:0.3724
[Epoch:450] Train Loss:1.3596 | Valid Acc:0.3507 | Test Acc:0.3711
[Epoch:460] Train Loss:1.3608 | Valid Acc:0.3579 | Test Acc:0.3724
[Epoch:470] Train Loss:1.3586 | Valid Acc:0.3566 | Test Acc:0.3724
[Epoch:480] Train Loss:1.3553 | Valid Acc:0.3553 | Test Acc:0.3724
[Epoch:490] Train Loss:1.3553 | Valid Acc:0.3553 | Test Acc:0.3724
[Epoch:500] Train Loss:1.3521 | Valid Acc:0.3553 | Test Acc:0.3697
[Epoch:510] Train Loss:1.3532 | Valid Acc:0.3546 | Test Acc:0.3737
[Epoch:520] Train Loss:1.3479 | Valid Acc:0.3533 | Test Acc:0.3730
[Epoch:530] Train Loss:1.3510 | Valid Acc:0.3546 | Test Acc:0.3730
[Epoch:540] Train Loss:1.3421 | Valid Acc:0.3546 | Test Acc:0.3730
[Epoch:550] Train Loss:1.3520 | Valid Acc:0.3533 | Test Acc:0.3737
[Epoch:560] Train Loss:1.3470 | Valid Acc:0.3526 | Test Acc:0.3724
[Epoch:570] Train Loss:1.3455 | Valid Acc:0.3520 | Test Acc:0.3737
[Epoch:580] Train Loss:1.3444 | Valid Acc:0.3526 | Test Acc:0.3750
[Epoch:590] Train Loss:1.3384 | Valid Acc:0.3513 | Test Acc:0.3711
[Epoch:600] Train Loss:1.3458 | Valid Acc:0.3520 | Test Acc:0.3704
[Epoch:610] Train Loss:1.3419 | Valid Acc:0.3559 | Test Acc:0.3691
[Epoch:620] Train Loss:1.3462 | Valid Acc:0.3533 | Test Acc:0.3704
[Epoch:630] Train Loss:1.3373 | Valid Acc:0.3553 | Test Acc:0.3684
[Epoch:640] Train Loss:1.3481 | Valid Acc:0.3553 | Test Acc:0.3684
[Epoch:650] Train Loss:1.3406 | Valid Acc:0.3566 | Test Acc:0.3684
[Epoch:660] Train Loss:1.3496 | Valid Acc:0.3559 | Test Acc:0.3684
[Epoch:670] Train Loss:1.3414 | Valid Acc:0.3553 | Test Acc:0.3684
[Epoch:680] Train Loss:1.3503 | Valid Acc:0.3546 | Test Acc:0.3684
[Epoch:690] Train Loss:1.3430 | Valid Acc:0.3553 | Test Acc:0.3684
[Epoch:700] Train Loss:1.3427 | Valid Acc:0.3559 | Test Acc:0.3678
[Epoch:710] Train Loss:1.3424 | Valid Acc:0.3559 | Test Acc:0.3678
[Epoch:720] Train Loss:1.3441 | Valid Acc:0.3559 | Test Acc:0.3678
[Epoch:730] Train Loss:1.3424 | Valid Acc:0.3553 | Test Acc:0.3678
[Epoch:740] Train Loss:1.3412 | Valid Acc:0.3553 | Test Acc:0.3678
[Epoch:750] Train Loss:1.3432 | Valid Acc:0.3546 | Test Acc:0.3678
[Epoch:760] Train Loss:1.3385 | Valid Acc:0.3553 | Test Acc:0.3678
[Epoch:770] Train Loss:1.3404 | Valid Acc:0.3553 | Test Acc:0.3678
[Epoch:780] Train Loss:1.3388 | Valid Acc:0.3553 | Test Acc:0.3678
[Epoch:790] Train Loss:1.3436 | Valid Acc:0.3553 | Test Acc:0.3678
Best results: epoch=211, valid loss=1.4502, valid acc=0.3618, test loss:1.4499,test acc=0.3480
{'activation': 'elu', 'dropout': 0, 'hidden_size': 256, 'learning_rate': 0.002174578323991514, 'model': 'f2gnn', 'optimizer': 'adagrad', 'weight_decay': 0.0003450682351004687}
Current Hyperparameter: {'activation': 'elu', 'dropout': 0, 'hidden_size': 256, 'learning_rate': 0.002174578323991514, 'model': 'f2gnn', 'optimizer': 'adagrad', 'weight_decay': 0.0003450682351004687}
[Epoch:0] Train Loss:1.6264 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:10] Train Loss:1.5723 | Valid Acc:0.2388 | Test Acc:0.2388
[Epoch:20] Train Loss:1.5662 | Valid Acc:0.2513 | Test Acc:0.2513
[Epoch:30] Train Loss:1.5624 | Valid Acc:0.2454 | Test Acc:0.2480
[Epoch:40] Train Loss:1.5549 | Valid Acc:0.2961 | Test Acc:0.2908
[Epoch:50] Train Loss:1.5468 | Valid Acc:0.2592 | Test Acc:0.2599
[Epoch:60] Train Loss:1.5850 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:70] Train Loss:1.5300 | Valid Acc:0.2289 | Test Acc:0.2336
[Epoch:80] Train Loss:1.5153 | Valid Acc:0.3217 | Test Acc:0.3053
[Epoch:90] Train Loss:1.5036 | Valid Acc:0.2908 | Test Acc:0.2868
[Epoch:100] Train Loss:1.5050 | Valid Acc:0.2526 | Test Acc:0.2480
[Epoch:110] Train Loss:1.4621 | Valid Acc:0.3421 | Test Acc:0.3276
[Epoch:120] Train Loss:1.4428 | Valid Acc:0.3599 | Test Acc:0.3441
[Epoch:130] Train Loss:1.4354 | Valid Acc:0.3467 | Test Acc:0.3599
[Epoch:140] Train Loss:1.4040 | Valid Acc:0.3493 | Test Acc:0.3559
[Epoch:150] Train Loss:1.3835 | Valid Acc:0.3375 | Test Acc:0.3336
[Epoch:160] Train Loss:1.3717 | Valid Acc:0.3678 | Test Acc:0.3796
[Epoch:170] Train Loss:1.3409 | Valid Acc:0.3717 | Test Acc:0.3783
[Epoch:180] Train Loss:1.3322 | Valid Acc:0.3678 | Test Acc:0.3717
[Epoch:190] Train Loss:1.3106 | Valid Acc:0.3724 | Test Acc:0.3868
[Epoch:200] Train Loss:1.3112 | Valid Acc:0.3770 | Test Acc:0.3822
[Epoch:210] Train Loss:1.2953 | Valid Acc:0.3605 | Test Acc:0.3586
[Epoch:220] Train Loss:1.2931 | Valid Acc:0.3783 | Test Acc:0.3763
[Epoch:230] Train Loss:1.2690 | Valid Acc:0.3809 | Test Acc:0.3980
[Epoch:240] Train Loss:1.2811 | Valid Acc:0.3553 | Test Acc:0.3467
[Epoch:250] Train Loss:1.2514 | Valid Acc:0.3855 | Test Acc:0.3934
[Epoch:260] Train Loss:1.2576 | Valid Acc:0.3822 | Test Acc:0.3882
[Epoch:270] Train Loss:1.2336 | Valid Acc:0.3901 | Test Acc:0.4020
[Epoch:280] Train Loss:1.2493 | Valid Acc:0.3697 | Test Acc:0.3803
[Epoch:290] Train Loss:1.2326 | Valid Acc:0.3658 | Test Acc:0.3691
[Epoch:300] Train Loss:1.2208 | Valid Acc:0.3691 | Test Acc:0.3789
[Epoch:310] Train Loss:1.2115 | Valid Acc:0.3671 | Test Acc:0.3862
[Epoch:320] Train Loss:1.2139 | Valid Acc:0.3803 | Test Acc:0.3862
[Epoch:330] Train Loss:1.2018 | Valid Acc:0.3868 | Test Acc:0.3914
[Epoch:340] Train Loss:1.1999 | Valid Acc:0.3809 | Test Acc:0.3934
[Epoch:350] Train Loss:1.1895 | Valid Acc:0.3888 | Test Acc:0.3901
[Epoch:360] Train Loss:1.1820 | Valid Acc:0.3809 | Test Acc:0.3947
[Epoch:370] Train Loss:1.1778 | Valid Acc:0.3770 | Test Acc:0.3888
[Epoch:380] Train Loss:1.1768 | Valid Acc:0.3836 | Test Acc:0.3954
[Epoch:390] Train Loss:1.1751 | Valid Acc:0.3783 | Test Acc:0.3849
[Epoch:400] Train Loss:1.1692 | Valid Acc:0.3842 | Test Acc:0.3875
[Epoch:410] Train Loss:1.1630 | Valid Acc:0.3829 | Test Acc:0.3888
[Epoch:420] Train Loss:1.1579 | Valid Acc:0.3783 | Test Acc:0.3875
[Epoch:430] Train Loss:1.1541 | Valid Acc:0.3770 | Test Acc:0.3914
[Epoch:440] Train Loss:1.1508 | Valid Acc:0.3789 | Test Acc:0.3895
[Epoch:450] Train Loss:1.1479 | Valid Acc:0.3796 | Test Acc:0.3901
[Epoch:460] Train Loss:1.1452 | Valid Acc:0.3803 | Test Acc:0.3882
[Epoch:470] Train Loss:1.1425 | Valid Acc:0.3803 | Test Acc:0.3895
[Epoch:480] Train Loss:1.1399 | Valid Acc:0.3822 | Test Acc:0.3882
[Epoch:490] Train Loss:1.1376 | Valid Acc:0.3816 | Test Acc:0.3888
[Epoch:500] Train Loss:1.1354 | Valid Acc:0.3803 | Test Acc:0.3868
[Epoch:510] Train Loss:1.1334 | Valid Acc:0.3849 | Test Acc:0.3862
[Epoch:520] Train Loss:1.1315 | Valid Acc:0.3822 | Test Acc:0.3855
[Epoch:530] Train Loss:1.1298 | Valid Acc:0.3829 | Test Acc:0.3882
[Epoch:540] Train Loss:1.1281 | Valid Acc:0.3836 | Test Acc:0.3868
[Epoch:550] Train Loss:1.1266 | Valid Acc:0.3836 | Test Acc:0.3836
[Epoch:560] Train Loss:1.1252 | Valid Acc:0.3842 | Test Acc:0.3816
[Epoch:570] Train Loss:1.1239 | Valid Acc:0.3855 | Test Acc:0.3822
[Epoch:580] Train Loss:1.1227 | Valid Acc:0.3849 | Test Acc:0.3822
[Epoch:590] Train Loss:1.1216 | Valid Acc:0.3849 | Test Acc:0.3809
[Epoch:600] Train Loss:1.1206 | Valid Acc:0.3842 | Test Acc:0.3803
[Epoch:610] Train Loss:1.1197 | Valid Acc:0.3842 | Test Acc:0.3796
[Epoch:620] Train Loss:1.1190 | Valid Acc:0.3842 | Test Acc:0.3776
[Epoch:630] Train Loss:1.1182 | Valid Acc:0.3842 | Test Acc:0.3770
[Epoch:640] Train Loss:1.1176 | Valid Acc:0.3842 | Test Acc:0.3776
[Epoch:650] Train Loss:1.1171 | Valid Acc:0.3842 | Test Acc:0.3770
[Epoch:660] Train Loss:1.1166 | Valid Acc:0.3855 | Test Acc:0.3770
[Epoch:670] Train Loss:1.1162 | Valid Acc:0.3855 | Test Acc:0.3770
[Epoch:680] Train Loss:1.1158 | Valid Acc:0.3855 | Test Acc:0.3776
[Epoch:690] Train Loss:1.1155 | Valid Acc:0.3855 | Test Acc:0.3776
[Epoch:700] Train Loss:1.1153 | Valid Acc:0.3836 | Test Acc:0.3783
[Epoch:710] Train Loss:1.1151 | Valid Acc:0.3836 | Test Acc:0.3789
[Epoch:720] Train Loss:1.1149 | Valid Acc:0.3836 | Test Acc:0.3789
[Epoch:730] Train Loss:1.1148 | Valid Acc:0.3829 | Test Acc:0.3789
[Epoch:740] Train Loss:1.1147 | Valid Acc:0.3822 | Test Acc:0.3789
[Epoch:750] Train Loss:1.1146 | Valid Acc:0.3822 | Test Acc:0.3789
[Epoch:760] Train Loss:1.1146 | Valid Acc:0.3822 | Test Acc:0.3789
[Epoch:770] Train Loss:1.1146 | Valid Acc:0.3822 | Test Acc:0.3789
[Epoch:780] Train Loss:1.1145 | Valid Acc:0.3822 | Test Acc:0.3789
[Epoch:790] Train Loss:1.1145 | Valid Acc:0.3822 | Test Acc:0.3789
Best results: epoch=268, valid loss=1.3479, valid acc=0.3908, test loss:1.3458,test acc=0.4039
{'activation': 'relu', 'dropout': 3, 'hidden_size': 128, 'learning_rate': 0.002814211565940107, 'model': 'f2gnn', 'optimizer': 'adagrad', 'weight_decay': 0.00024424809660775014}
Current Hyperparameter: {'activation': 'relu', 'dropout': 3, 'hidden_size': 128, 'learning_rate': 0.002814211565940107, 'model': 'f2gnn', 'optimizer': 'adagrad', 'weight_decay': 0.00024424809660775014}
[Epoch:0] Train Loss:1.6701 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:10] Train Loss:1.5758 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:20] Train Loss:1.5746 | Valid Acc:0.2618 | Test Acc:0.2612
[Epoch:30] Train Loss:1.5717 | Valid Acc:0.3046 | Test Acc:0.3079
[Epoch:40] Train Loss:1.5668 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:50] Train Loss:1.5575 | Valid Acc:0.2711 | Test Acc:0.2737
[Epoch:60] Train Loss:1.5547 | Valid Acc:0.3020 | Test Acc:0.3033
[Epoch:70] Train Loss:1.5480 | Valid Acc:0.3007 | Test Acc:0.3026
[Epoch:80] Train Loss:1.5417 | Valid Acc:0.3020 | Test Acc:0.3059
[Epoch:90] Train Loss:1.5323 | Valid Acc:0.3164 | Test Acc:0.3276
[Epoch:100] Train Loss:1.5170 | Valid Acc:0.3204 | Test Acc:0.3395
[Epoch:110] Train Loss:1.4971 | Valid Acc:0.3513 | Test Acc:0.3513
[Epoch:120] Train Loss:1.4822 | Valid Acc:0.3520 | Test Acc:0.3546
[Epoch:130] Train Loss:1.4759 | Valid Acc:0.3546 | Test Acc:0.3605
[Epoch:140] Train Loss:1.4605 | Valid Acc:0.3572 | Test Acc:0.3737
[Epoch:150] Train Loss:1.4455 | Valid Acc:0.3599 | Test Acc:0.3632
[Epoch:160] Train Loss:1.4405 | Valid Acc:0.3632 | Test Acc:0.3750
[Epoch:170] Train Loss:1.4335 | Valid Acc:0.3651 | Test Acc:0.3862
[Epoch:180] Train Loss:1.4235 | Valid Acc:0.3704 | Test Acc:0.3862
[Epoch:190] Train Loss:1.4207 | Valid Acc:0.3796 | Test Acc:0.3901
[Epoch:200] Train Loss:1.4027 | Valid Acc:0.3816 | Test Acc:0.3836
[Epoch:210] Train Loss:1.4016 | Valid Acc:0.3829 | Test Acc:0.3928
[Epoch:220] Train Loss:1.3925 | Valid Acc:0.3822 | Test Acc:0.3868
[Epoch:230] Train Loss:1.3785 | Valid Acc:0.3796 | Test Acc:0.3816
[Epoch:240] Train Loss:1.3780 | Valid Acc:0.3803 | Test Acc:0.3928
[Epoch:250] Train Loss:1.3719 | Valid Acc:0.3882 | Test Acc:0.3921
[Epoch:260] Train Loss:1.3717 | Valid Acc:0.3928 | Test Acc:0.3862
[Epoch:270] Train Loss:1.3637 | Valid Acc:0.3921 | Test Acc:0.3816
[Epoch:280] Train Loss:1.3599 | Valid Acc:0.3855 | Test Acc:0.3783
[Epoch:290] Train Loss:1.3631 | Valid Acc:0.3888 | Test Acc:0.3796
[Epoch:300] Train Loss:1.3554 | Valid Acc:0.3888 | Test Acc:0.3829
[Epoch:310] Train Loss:1.3519 | Valid Acc:0.3822 | Test Acc:0.3730
[Epoch:320] Train Loss:1.3541 | Valid Acc:0.3829 | Test Acc:0.3691
[Epoch:330] Train Loss:1.3460 | Valid Acc:0.3862 | Test Acc:0.3632
[Epoch:340] Train Loss:1.3457 | Valid Acc:0.3829 | Test Acc:0.3684
[Epoch:350] Train Loss:1.3426 | Valid Acc:0.3849 | Test Acc:0.3671
[Epoch:360] Train Loss:1.3397 | Valid Acc:0.3822 | Test Acc:0.3678
[Epoch:370] Train Loss:1.3436 | Valid Acc:0.3816 | Test Acc:0.3651
[Epoch:380] Train Loss:1.3392 | Valid Acc:0.3849 | Test Acc:0.3658
[Epoch:390] Train Loss:1.3390 | Valid Acc:0.3862 | Test Acc:0.3717
[Epoch:400] Train Loss:1.3340 | Valid Acc:0.3855 | Test Acc:0.3717
[Epoch:410] Train Loss:1.3317 | Valid Acc:0.3664 | Test Acc:0.3513
[Epoch:420] Train Loss:1.3233 | Valid Acc:0.3803 | Test Acc:0.3618
[Epoch:430] Train Loss:1.3281 | Valid Acc:0.3717 | Test Acc:0.3566
[Epoch:440] Train Loss:1.3193 | Valid Acc:0.3783 | Test Acc:0.3625
[Epoch:450] Train Loss:1.3247 | Valid Acc:0.3757 | Test Acc:0.3638
[Epoch:460] Train Loss:1.3271 | Valid Acc:0.3763 | Test Acc:0.3645
[Epoch:470] Train Loss:1.3317 | Valid Acc:0.3691 | Test Acc:0.3586
[Epoch:480] Train Loss:1.3156 | Valid Acc:0.3783 | Test Acc:0.3632
[Epoch:490] Train Loss:1.3184 | Valid Acc:0.3776 | Test Acc:0.3572
[Epoch:500] Train Loss:1.3211 | Valid Acc:0.3763 | Test Acc:0.3678
[Epoch:510] Train Loss:1.3190 | Valid Acc:0.3796 | Test Acc:0.3691
[Epoch:520] Train Loss:1.3144 | Valid Acc:0.3776 | Test Acc:0.3684
[Epoch:530] Train Loss:1.3165 | Valid Acc:0.3789 | Test Acc:0.3684
[Epoch:540] Train Loss:1.3103 | Valid Acc:0.3789 | Test Acc:0.3691
[Epoch:550] Train Loss:1.3166 | Valid Acc:0.3796 | Test Acc:0.3691
[Epoch:560] Train Loss:1.3144 | Valid Acc:0.3783 | Test Acc:0.3711
[Epoch:570] Train Loss:1.3139 | Valid Acc:0.3796 | Test Acc:0.3704
[Epoch:580] Train Loss:1.3116 | Valid Acc:0.3770 | Test Acc:0.3691
[Epoch:590] Train Loss:1.3119 | Valid Acc:0.3789 | Test Acc:0.3697
[Epoch:600] Train Loss:1.3099 | Valid Acc:0.3776 | Test Acc:0.3691
[Epoch:610] Train Loss:1.3084 | Valid Acc:0.3770 | Test Acc:0.3697
[Epoch:620] Train Loss:1.3121 | Valid Acc:0.3789 | Test Acc:0.3684
[Epoch:630] Train Loss:1.3080 | Valid Acc:0.3776 | Test Acc:0.3697
[Epoch:640] Train Loss:1.3129 | Valid Acc:0.3783 | Test Acc:0.3711
[Epoch:650] Train Loss:1.3108 | Valid Acc:0.3796 | Test Acc:0.3711
[Epoch:660] Train Loss:1.3104 | Valid Acc:0.3783 | Test Acc:0.3697
[Epoch:670] Train Loss:1.3054 | Valid Acc:0.3770 | Test Acc:0.3697
[Epoch:680] Train Loss:1.3119 | Valid Acc:0.3763 | Test Acc:0.3697
[Epoch:690] Train Loss:1.3108 | Valid Acc:0.3770 | Test Acc:0.3697
[Epoch:700] Train Loss:1.3139 | Valid Acc:0.3770 | Test Acc:0.3697
[Epoch:710] Train Loss:1.3155 | Valid Acc:0.3770 | Test Acc:0.3697
[Epoch:720] Train Loss:1.3136 | Valid Acc:0.3763 | Test Acc:0.3684
[Epoch:730] Train Loss:1.3135 | Valid Acc:0.3757 | Test Acc:0.3691
[Epoch:740] Train Loss:1.3140 | Valid Acc:0.3757 | Test Acc:0.3697
[Epoch:750] Train Loss:1.3166 | Valid Acc:0.3763 | Test Acc:0.3697
[Epoch:760] Train Loss:1.3080 | Valid Acc:0.3770 | Test Acc:0.3697
[Epoch:770] Train Loss:1.3155 | Valid Acc:0.3770 | Test Acc:0.3697
[Epoch:780] Train Loss:1.3048 | Valid Acc:0.3770 | Test Acc:0.3697
[Epoch:790] Train Loss:1.3087 | Valid Acc:0.3770 | Test Acc:0.3697
Best results: epoch=266, valid loss=1.3749, valid acc=0.3941, test loss:1.3811,test acc=0.3875
{'activation': 'elu', 'dropout': 2, 'hidden_size': 128, 'learning_rate': 0.006895844094667194, 'model': 'f2gnn', 'optimizer': 'adagrad', 'weight_decay': 0.0005545861780755636}
Current Hyperparameter: {'activation': 'elu', 'dropout': 2, 'hidden_size': 128, 'learning_rate': 0.006895844094667194, 'model': 'f2gnn', 'optimizer': 'adagrad', 'weight_decay': 0.0005545861780755636}
[Epoch:0] Train Loss:1.6438 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:10] Train Loss:1.5702 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:20] Train Loss:1.5602 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:30] Train Loss:1.5973 | Valid Acc:0.2145 | Test Acc:0.2145
[Epoch:40] Train Loss:1.5336 | Valid Acc:0.2822 | Test Acc:0.2822
[Epoch:50] Train Loss:1.5142 | Valid Acc:0.2717 | Test Acc:0.2724
[Epoch:60] Train Loss:1.6022 | Valid Acc:0.2375 | Test Acc:0.2375
[Epoch:70] Train Loss:1.4515 | Valid Acc:0.3368 | Test Acc:0.3329
[Epoch:80] Train Loss:1.4467 | Valid Acc:0.3099 | Test Acc:0.3086
[Epoch:90] Train Loss:1.4567 | Valid Acc:0.3355 | Test Acc:0.3289
[Epoch:100] Train Loss:1.4305 | Valid Acc:0.3434 | Test Acc:0.3408
[Epoch:110] Train Loss:1.4189 | Valid Acc:0.3553 | Test Acc:0.3605
[Epoch:120] Train Loss:1.4045 | Valid Acc:0.3599 | Test Acc:0.3638
[Epoch:130] Train Loss:1.3690 | Valid Acc:0.3711 | Test Acc:0.3737
[Epoch:140] Train Loss:1.3567 | Valid Acc:0.3763 | Test Acc:0.3809
[Epoch:150] Train Loss:1.3355 | Valid Acc:0.3750 | Test Acc:0.3961
[Epoch:160] Train Loss:1.3161 | Valid Acc:0.3658 | Test Acc:0.3882
[Epoch:170] Train Loss:1.3211 | Valid Acc:0.3697 | Test Acc:0.3763
[Epoch:180] Train Loss:1.3126 | Valid Acc:0.3638 | Test Acc:0.3822
[Epoch:190] Train Loss:1.3109 | Valid Acc:0.3862 | Test Acc:0.3868
[Epoch:200] Train Loss:1.2638 | Valid Acc:0.3770 | Test Acc:0.3908
[Epoch:210] Train Loss:1.2549 | Valid Acc:0.3789 | Test Acc:0.3888
[Epoch:220] Train Loss:1.2990 | Valid Acc:0.3809 | Test Acc:0.3882
[Epoch:230] Train Loss:1.2403 | Valid Acc:0.3711 | Test Acc:0.3974
[Epoch:240] Train Loss:1.2550 | Valid Acc:0.3789 | Test Acc:0.3862
[Epoch:250] Train Loss:1.2364 | Valid Acc:0.3809 | Test Acc:0.3967
[Epoch:260] Train Loss:1.2296 | Valid Acc:0.3678 | Test Acc:0.3934
[Epoch:270] Train Loss:1.2346 | Valid Acc:0.3783 | Test Acc:0.3770
[Epoch:280] Train Loss:1.2120 | Valid Acc:0.3678 | Test Acc:0.3954
[Epoch:290] Train Loss:1.2218 | Valid Acc:0.3737 | Test Acc:0.4020
[Epoch:300] Train Loss:1.1983 | Valid Acc:0.3691 | Test Acc:0.3941
[Epoch:310] Train Loss:1.2114 | Valid Acc:0.3763 | Test Acc:0.3789
[Epoch:320] Train Loss:1.1963 | Valid Acc:0.3849 | Test Acc:0.3743
[Epoch:330] Train Loss:1.1976 | Valid Acc:0.3770 | Test Acc:0.3711
[Epoch:340] Train Loss:1.1821 | Valid Acc:0.3836 | Test Acc:0.3796
[Epoch:350] Train Loss:1.1837 | Valid Acc:0.3816 | Test Acc:0.3730
[Epoch:360] Train Loss:1.1769 | Valid Acc:0.3803 | Test Acc:0.3770
[Epoch:370] Train Loss:1.1705 | Valid Acc:0.3711 | Test Acc:0.3724
[Epoch:380] Train Loss:1.1762 | Valid Acc:0.3697 | Test Acc:0.3908
[Epoch:390] Train Loss:1.1740 | Valid Acc:0.3658 | Test Acc:0.3987
[Epoch:400] Train Loss:1.1641 | Valid Acc:0.3704 | Test Acc:0.3921
[Epoch:410] Train Loss:1.1564 | Valid Acc:0.3763 | Test Acc:0.3862
[Epoch:420] Train Loss:1.1487 | Valid Acc:0.3684 | Test Acc:0.3928
[Epoch:430] Train Loss:1.1580 | Valid Acc:0.3664 | Test Acc:0.3961
[Epoch:440] Train Loss:1.1569 | Valid Acc:0.3704 | Test Acc:0.3829
[Epoch:450] Train Loss:1.1474 | Valid Acc:0.3730 | Test Acc:0.3862
[Epoch:460] Train Loss:1.1452 | Valid Acc:0.3776 | Test Acc:0.3829
[Epoch:470] Train Loss:1.1429 | Valid Acc:0.3776 | Test Acc:0.3757
[Epoch:480] Train Loss:1.1336 | Valid Acc:0.3757 | Test Acc:0.3796
[Epoch:490] Train Loss:1.1342 | Valid Acc:0.3757 | Test Acc:0.3770
[Epoch:500] Train Loss:1.1374 | Valid Acc:0.3776 | Test Acc:0.3842
[Epoch:510] Train Loss:1.1321 | Valid Acc:0.3770 | Test Acc:0.3776
[Epoch:520] Train Loss:1.1270 | Valid Acc:0.3724 | Test Acc:0.3803
[Epoch:530] Train Loss:1.1310 | Valid Acc:0.3770 | Test Acc:0.3789
[Epoch:540] Train Loss:1.1257 | Valid Acc:0.3743 | Test Acc:0.3809
[Epoch:550] Train Loss:1.1308 | Valid Acc:0.3737 | Test Acc:0.3836
[Epoch:560] Train Loss:1.1241 | Valid Acc:0.3743 | Test Acc:0.3822
[Epoch:570] Train Loss:1.1276 | Valid Acc:0.3750 | Test Acc:0.3836
[Epoch:580] Train Loss:1.1217 | Valid Acc:0.3724 | Test Acc:0.3816
[Epoch:590] Train Loss:1.1233 | Valid Acc:0.3763 | Test Acc:0.3822
[Epoch:600] Train Loss:1.1204 | Valid Acc:0.3763 | Test Acc:0.3809
[Epoch:610] Train Loss:1.1197 | Valid Acc:0.3763 | Test Acc:0.3829
[Epoch:620] Train Loss:1.1215 | Valid Acc:0.3737 | Test Acc:0.3822
[Epoch:630] Train Loss:1.1101 | Valid Acc:0.3743 | Test Acc:0.3816
[Epoch:640] Train Loss:1.1222 | Valid Acc:0.3743 | Test Acc:0.3816
[Epoch:650] Train Loss:1.1229 | Valid Acc:0.3750 | Test Acc:0.3816
[Epoch:660] Train Loss:1.1143 | Valid Acc:0.3730 | Test Acc:0.3803
[Epoch:670] Train Loss:1.1133 | Valid Acc:0.3724 | Test Acc:0.3809
[Epoch:680] Train Loss:1.1178 | Valid Acc:0.3724 | Test Acc:0.3789
[Epoch:690] Train Loss:1.1138 | Valid Acc:0.3743 | Test Acc:0.3803
[Epoch:700] Train Loss:1.1182 | Valid Acc:0.3737 | Test Acc:0.3789
[Epoch:710] Train Loss:1.1169 | Valid Acc:0.3724 | Test Acc:0.3796
[Epoch:720] Train Loss:1.1164 | Valid Acc:0.3730 | Test Acc:0.3796
[Epoch:730] Train Loss:1.1134 | Valid Acc:0.3737 | Test Acc:0.3789
[Epoch:740] Train Loss:1.1162 | Valid Acc:0.3730 | Test Acc:0.3789
[Epoch:750] Train Loss:1.1203 | Valid Acc:0.3737 | Test Acc:0.3789
[Epoch:760] Train Loss:1.1164 | Valid Acc:0.3743 | Test Acc:0.3796
[Epoch:770] Train Loss:1.1148 | Valid Acc:0.3743 | Test Acc:0.3796
[Epoch:780] Train Loss:1.1138 | Valid Acc:0.3743 | Test Acc:0.3796
[Epoch:790] Train Loss:1.1183 | Valid Acc:0.3743 | Test Acc:0.3789
Best results: epoch=167, valid loss=1.3641, valid acc=0.3908, test loss:1.3715,test acc=0.3803
{'activation': 'relu', 'dropout': 2, 'hidden_size': 256, 'learning_rate': 0.0012350969663316282, 'model': 'f2gnn', 'optimizer': 'adagrad', 'weight_decay': 0.000267962362016806}
Current Hyperparameter: {'activation': 'relu', 'dropout': 2, 'hidden_size': 256, 'learning_rate': 0.0012350969663316282, 'model': 'f2gnn', 'optimizer': 'adagrad', 'weight_decay': 0.000267962362016806}
[Epoch:0] Train Loss:1.6379 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:10] Train Loss:1.5743 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:20] Train Loss:1.5751 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:30] Train Loss:1.5731 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:40] Train Loss:1.5689 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:50] Train Loss:1.5678 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:60] Train Loss:1.5654 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:70] Train Loss:1.5700 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:80] Train Loss:1.5552 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:90] Train Loss:1.5525 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:100] Train Loss:1.5553 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:110] Train Loss:1.5504 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:120] Train Loss:1.5423 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:130] Train Loss:1.5342 | Valid Acc:0.2592 | Test Acc:0.2586
[Epoch:140] Train Loss:1.5232 | Valid Acc:0.2684 | Test Acc:0.2625
[Epoch:150] Train Loss:1.5243 | Valid Acc:0.2691 | Test Acc:0.2645
[Epoch:160] Train Loss:1.5104 | Valid Acc:0.2743 | Test Acc:0.2750
[Epoch:170] Train Loss:1.5012 | Valid Acc:0.2803 | Test Acc:0.2862
[Epoch:180] Train Loss:1.4934 | Valid Acc:0.3000 | Test Acc:0.2987
[Epoch:190] Train Loss:1.4871 | Valid Acc:0.3033 | Test Acc:0.3000
[Epoch:200] Train Loss:1.4836 | Valid Acc:0.3125 | Test Acc:0.3072
[Epoch:210] Train Loss:1.4633 | Valid Acc:0.3270 | Test Acc:0.3296
[Epoch:220] Train Loss:1.4631 | Valid Acc:0.3322 | Test Acc:0.3329
[Epoch:230] Train Loss:1.4531 | Valid Acc:0.3434 | Test Acc:0.3513
[Epoch:240] Train Loss:1.4443 | Valid Acc:0.3401 | Test Acc:0.3487
[Epoch:250] Train Loss:1.4429 | Valid Acc:0.3428 | Test Acc:0.3526
[Epoch:260] Train Loss:1.4312 | Valid Acc:0.3533 | Test Acc:0.3553
[Epoch:270] Train Loss:1.4164 | Valid Acc:0.3572 | Test Acc:0.3776
[Epoch:280] Train Loss:1.4224 | Valid Acc:0.3625 | Test Acc:0.3789
[Epoch:290] Train Loss:1.4180 | Valid Acc:0.3632 | Test Acc:0.3737
[Epoch:300] Train Loss:1.4072 | Valid Acc:0.3625 | Test Acc:0.3809
[Epoch:310] Train Loss:1.4038 | Valid Acc:0.3618 | Test Acc:0.3855
[Epoch:320] Train Loss:1.4045 | Valid Acc:0.3651 | Test Acc:0.3882
[Epoch:330] Train Loss:1.3945 | Valid Acc:0.3711 | Test Acc:0.3987
[Epoch:340] Train Loss:1.3886 | Valid Acc:0.3717 | Test Acc:0.3980
[Epoch:350] Train Loss:1.3917 | Valid Acc:0.3750 | Test Acc:0.3987
[Epoch:360] Train Loss:1.3862 | Valid Acc:0.3750 | Test Acc:0.3987
[Epoch:370] Train Loss:1.3835 | Valid Acc:0.3697 | Test Acc:0.3974
[Epoch:380] Train Loss:1.3829 | Valid Acc:0.3730 | Test Acc:0.4000
[Epoch:390] Train Loss:1.3807 | Valid Acc:0.3809 | Test Acc:0.3980
[Epoch:400] Train Loss:1.3751 | Valid Acc:0.3816 | Test Acc:0.4046
[Epoch:410] Train Loss:1.3703 | Valid Acc:0.3829 | Test Acc:0.4013
[Epoch:420] Train Loss:1.3756 | Valid Acc:0.3868 | Test Acc:0.4066
[Epoch:430] Train Loss:1.3713 | Valid Acc:0.3849 | Test Acc:0.4026
[Epoch:440] Train Loss:1.3718 | Valid Acc:0.3901 | Test Acc:0.4092
[Epoch:450] Train Loss:1.3725 | Valid Acc:0.3875 | Test Acc:0.4125
[Epoch:460] Train Loss:1.3682 | Valid Acc:0.3842 | Test Acc:0.4132
[Epoch:470] Train Loss:1.3697 | Valid Acc:0.3882 | Test Acc:0.4072
[Epoch:480] Train Loss:1.3665 | Valid Acc:0.3849 | Test Acc:0.4105
[Epoch:490] Train Loss:1.3598 | Valid Acc:0.3875 | Test Acc:0.4072
[Epoch:500] Train Loss:1.3562 | Valid Acc:0.3882 | Test Acc:0.4079
[Epoch:510] Train Loss:1.3653 | Valid Acc:0.3875 | Test Acc:0.4086
[Epoch:520] Train Loss:1.3667 | Valid Acc:0.3842 | Test Acc:0.4046
[Epoch:530] Train Loss:1.3681 | Valid Acc:0.3842 | Test Acc:0.4059
[Epoch:540] Train Loss:1.3613 | Valid Acc:0.3842 | Test Acc:0.4079
[Epoch:550] Train Loss:1.3547 | Valid Acc:0.3842 | Test Acc:0.4072
[Epoch:560] Train Loss:1.3638 | Valid Acc:0.3855 | Test Acc:0.4079
[Epoch:570] Train Loss:1.3564 | Valid Acc:0.3849 | Test Acc:0.4072
[Epoch:580] Train Loss:1.3601 | Valid Acc:0.3849 | Test Acc:0.4086
[Epoch:590] Train Loss:1.3559 | Valid Acc:0.3868 | Test Acc:0.4086
[Epoch:600] Train Loss:1.3563 | Valid Acc:0.3862 | Test Acc:0.4079
[Epoch:610] Train Loss:1.3594 | Valid Acc:0.3849 | Test Acc:0.4079
[Epoch:620] Train Loss:1.3594 | Valid Acc:0.3862 | Test Acc:0.4072
[Epoch:630] Train Loss:1.3545 | Valid Acc:0.3862 | Test Acc:0.4079
[Epoch:640] Train Loss:1.3621 | Valid Acc:0.3868 | Test Acc:0.4112
[Epoch:650] Train Loss:1.3588 | Valid Acc:0.3882 | Test Acc:0.4079
[Epoch:660] Train Loss:1.3612 | Valid Acc:0.3882 | Test Acc:0.4072
[Epoch:670] Train Loss:1.3550 | Valid Acc:0.3875 | Test Acc:0.4079
[Epoch:680] Train Loss:1.3511 | Valid Acc:0.3868 | Test Acc:0.4086
[Epoch:690] Train Loss:1.3590 | Valid Acc:0.3868 | Test Acc:0.4086
[Epoch:700] Train Loss:1.3557 | Valid Acc:0.3875 | Test Acc:0.4086
[Epoch:710] Train Loss:1.3519 | Valid Acc:0.3875 | Test Acc:0.4079
[Epoch:720] Train Loss:1.3556 | Valid Acc:0.3862 | Test Acc:0.4092
[Epoch:730] Train Loss:1.3495 | Valid Acc:0.3862 | Test Acc:0.4092
[Epoch:740] Train Loss:1.3511 | Valid Acc:0.3862 | Test Acc:0.4092
[Epoch:750] Train Loss:1.3514 | Valid Acc:0.3862 | Test Acc:0.4092
[Epoch:760] Train Loss:1.3585 | Valid Acc:0.3862 | Test Acc:0.4092
[Epoch:770] Train Loss:1.3585 | Valid Acc:0.3862 | Test Acc:0.4092
[Epoch:780] Train Loss:1.3560 | Valid Acc:0.3862 | Test Acc:0.4092
[Epoch:790] Train Loss:1.3461 | Valid Acc:0.3862 | Test Acc:0.4092
Best results: epoch=415, valid loss=1.3675, valid acc=0.3908, test loss:1.3708,test acc=0.4079
{'activation': 'elu', 'dropout': 4, 'hidden_size': 32, 'learning_rate': 0.003943379368844057, 'model': 'f2gnn', 'optimizer': 'adagrad', 'weight_decay': 0.0009825933400192306}
Current Hyperparameter: {'activation': 'elu', 'dropout': 4, 'hidden_size': 32, 'learning_rate': 0.003943379368844057, 'model': 'f2gnn', 'optimizer': 'adagrad', 'weight_decay': 0.0009825933400192306}
[Epoch:0] Train Loss:1.6118 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:10] Train Loss:1.5747 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:20] Train Loss:1.5721 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:30] Train Loss:1.5747 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:40] Train Loss:1.5684 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:50] Train Loss:1.5642 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:60] Train Loss:1.5565 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:70] Train Loss:1.5510 | Valid Acc:0.3211 | Test Acc:0.3033
[Epoch:80] Train Loss:1.5385 | Valid Acc:0.3316 | Test Acc:0.3178
[Epoch:90] Train Loss:1.5193 | Valid Acc:0.3349 | Test Acc:0.3289
[Epoch:100] Train Loss:1.5021 | Valid Acc:0.3382 | Test Acc:0.3309
[Epoch:110] Train Loss:1.4849 | Valid Acc:0.3401 | Test Acc:0.3368
[Epoch:120] Train Loss:1.4632 | Valid Acc:0.3428 | Test Acc:0.3401
[Epoch:130] Train Loss:1.4549 | Valid Acc:0.3428 | Test Acc:0.3421
[Epoch:140] Train Loss:1.4436 | Valid Acc:0.3480 | Test Acc:0.3612
[Epoch:150] Train Loss:1.4216 | Valid Acc:0.3599 | Test Acc:0.3625
[Epoch:160] Train Loss:1.4112 | Valid Acc:0.3658 | Test Acc:0.3737
[Epoch:170] Train Loss:1.3978 | Valid Acc:0.3678 | Test Acc:0.3849
[Epoch:180] Train Loss:1.3947 | Valid Acc:0.3559 | Test Acc:0.3618
[Epoch:190] Train Loss:1.3867 | Valid Acc:0.3664 | Test Acc:0.3763
[Epoch:200] Train Loss:1.3773 | Valid Acc:0.3849 | Test Acc:0.3980
[Epoch:210] Train Loss:1.3745 | Valid Acc:0.3836 | Test Acc:0.4000
[Epoch:220] Train Loss:1.3668 | Valid Acc:0.3888 | Test Acc:0.4026
[Epoch:230] Train Loss:1.3581 | Valid Acc:0.3868 | Test Acc:0.4007
[Epoch:240] Train Loss:1.3486 | Valid Acc:0.3947 | Test Acc:0.4007
[Epoch:250] Train Loss:1.3557 | Valid Acc:0.3868 | Test Acc:0.4033
[Epoch:260] Train Loss:1.3386 | Valid Acc:0.3901 | Test Acc:0.4053
[Epoch:270] Train Loss:1.3287 | Valid Acc:0.3921 | Test Acc:0.4039
[Epoch:280] Train Loss:1.3310 | Valid Acc:0.3941 | Test Acc:0.4046
[Epoch:290] Train Loss:1.3235 | Valid Acc:0.3987 | Test Acc:0.4007
[Epoch:300] Train Loss:1.3215 | Valid Acc:0.3974 | Test Acc:0.4046
[Epoch:310] Train Loss:1.3160 | Valid Acc:0.3987 | Test Acc:0.4013
[Epoch:320] Train Loss:1.3268 | Valid Acc:0.3967 | Test Acc:0.3993
[Epoch:330] Train Loss:1.3169 | Valid Acc:0.3993 | Test Acc:0.4039
[Epoch:340] Train Loss:1.3216 | Valid Acc:0.3993 | Test Acc:0.3961
[Epoch:350] Train Loss:1.3121 | Valid Acc:0.4007 | Test Acc:0.4026
[Epoch:360] Train Loss:1.3158 | Valid Acc:0.3961 | Test Acc:0.4000
[Epoch:370] Train Loss:1.3026 | Valid Acc:0.4007 | Test Acc:0.4007
[Epoch:380] Train Loss:1.3025 | Valid Acc:0.3993 | Test Acc:0.3980
[Epoch:390] Train Loss:1.3093 | Valid Acc:0.3954 | Test Acc:0.4020
[Epoch:400] Train Loss:1.3068 | Valid Acc:0.3941 | Test Acc:0.4026
[Epoch:410] Train Loss:1.2973 | Valid Acc:0.3987 | Test Acc:0.3967
[Epoch:420] Train Loss:1.3009 | Valid Acc:0.3934 | Test Acc:0.4007
[Epoch:430] Train Loss:1.2932 | Valid Acc:0.3961 | Test Acc:0.4000
[Epoch:440] Train Loss:1.2914 | Valid Acc:0.3974 | Test Acc:0.4020
[Epoch:450] Train Loss:1.2940 | Valid Acc:0.3954 | Test Acc:0.4013
[Epoch:460] Train Loss:1.2898 | Valid Acc:0.3980 | Test Acc:0.4033
[Epoch:470] Train Loss:1.2930 | Valid Acc:0.3980 | Test Acc:0.4039
[Epoch:480] Train Loss:1.2896 | Valid Acc:0.3974 | Test Acc:0.4013
[Epoch:490] Train Loss:1.2960 | Valid Acc:0.3974 | Test Acc:0.4026
[Epoch:500] Train Loss:1.2897 | Valid Acc:0.3980 | Test Acc:0.4013
[Epoch:510] Train Loss:1.2960 | Valid Acc:0.3974 | Test Acc:0.4026
[Epoch:520] Train Loss:1.2849 | Valid Acc:0.3980 | Test Acc:0.4033
[Epoch:530] Train Loss:1.2904 | Valid Acc:0.3967 | Test Acc:0.4046
[Epoch:540] Train Loss:1.2824 | Valid Acc:0.3987 | Test Acc:0.4033
[Epoch:550] Train Loss:1.2841 | Valid Acc:0.3980 | Test Acc:0.4013
[Epoch:560] Train Loss:1.2843 | Valid Acc:0.3980 | Test Acc:0.4013
[Epoch:570] Train Loss:1.2855 | Valid Acc:0.3980 | Test Acc:0.4039
[Epoch:580] Train Loss:1.2899 | Valid Acc:0.3980 | Test Acc:0.4033
[Epoch:590] Train Loss:1.2844 | Valid Acc:0.4007 | Test Acc:0.4039
[Epoch:600] Train Loss:1.2785 | Valid Acc:0.4000 | Test Acc:0.4053
[Epoch:610] Train Loss:1.2846 | Valid Acc:0.3987 | Test Acc:0.4046
[Epoch:620] Train Loss:1.2764 | Valid Acc:0.4020 | Test Acc:0.4039
[Epoch:630] Train Loss:1.2842 | Valid Acc:0.3987 | Test Acc:0.4053
[Epoch:640] Train Loss:1.2860 | Valid Acc:0.3987 | Test Acc:0.4053
[Epoch:650] Train Loss:1.2837 | Valid Acc:0.4007 | Test Acc:0.4033
[Epoch:660] Train Loss:1.2765 | Valid Acc:0.3987 | Test Acc:0.4039
[Epoch:670] Train Loss:1.2860 | Valid Acc:0.3980 | Test Acc:0.4059
[Epoch:680] Train Loss:1.2806 | Valid Acc:0.3980 | Test Acc:0.4053
[Epoch:690] Train Loss:1.2807 | Valid Acc:0.3980 | Test Acc:0.4053
[Epoch:700] Train Loss:1.2843 | Valid Acc:0.3980 | Test Acc:0.4053
[Epoch:710] Train Loss:1.2821 | Valid Acc:0.3974 | Test Acc:0.4059
[Epoch:720] Train Loss:1.2784 | Valid Acc:0.3980 | Test Acc:0.4053
[Epoch:730] Train Loss:1.2761 | Valid Acc:0.3974 | Test Acc:0.4053
[Epoch:740] Train Loss:1.2778 | Valid Acc:0.3980 | Test Acc:0.4053
[Epoch:750] Train Loss:1.2807 | Valid Acc:0.3980 | Test Acc:0.4053
[Epoch:760] Train Loss:1.2786 | Valid Acc:0.3980 | Test Acc:0.4053
[Epoch:770] Train Loss:1.2771 | Valid Acc:0.3980 | Test Acc:0.4053
[Epoch:780] Train Loss:1.2798 | Valid Acc:0.3980 | Test Acc:0.4053
[Epoch:790] Train Loss:1.2723 | Valid Acc:0.3980 | Test Acc:0.4053
Best results: epoch=354, valid loss=1.3365, valid acc=0.4033, test loss:1.3431,test acc=0.4000
{'activation': 'elu', 'dropout': 3, 'hidden_size': 64, 'learning_rate': 0.0073910813635672465, 'model': 'f2gnn', 'optimizer': 'adagrad', 'weight_decay': 0.0004106376690085103}
Current Hyperparameter: {'activation': 'elu', 'dropout': 3, 'hidden_size': 64, 'learning_rate': 0.0073910813635672465, 'model': 'f2gnn', 'optimizer': 'adagrad', 'weight_decay': 0.0004106376690085103}
[Epoch:0] Train Loss:1.6074 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:10] Train Loss:1.5738 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:20] Train Loss:1.5665 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:30] Train Loss:1.5689 | Valid Acc:0.2586 | Test Acc:0.2592
[Epoch:40] Train Loss:1.5156 | Valid Acc:0.2849 | Test Acc:0.2895
[Epoch:50] Train Loss:1.4951 | Valid Acc:0.2730 | Test Acc:0.2717
[Epoch:60] Train Loss:1.4427 | Valid Acc:0.3441 | Test Acc:0.3467
[Epoch:70] Train Loss:1.4394 | Valid Acc:0.2961 | Test Acc:0.2947
[Epoch:80] Train Loss:1.3915 | Valid Acc:0.3776 | Test Acc:0.3645
[Epoch:90] Train Loss:1.3722 | Valid Acc:0.3803 | Test Acc:0.3678
[Epoch:100] Train Loss:1.3556 | Valid Acc:0.3862 | Test Acc:0.3757
[Epoch:110] Train Loss:1.3466 | Valid Acc:0.3816 | Test Acc:0.3711
[Epoch:120] Train Loss:1.3350 | Valid Acc:0.3855 | Test Acc:0.3737
[Epoch:130] Train Loss:1.3136 | Valid Acc:0.4039 | Test Acc:0.3967
[Epoch:140] Train Loss:1.3073 | Valid Acc:0.3724 | Test Acc:0.3737
[Epoch:150] Train Loss:1.2921 | Valid Acc:0.3849 | Test Acc:0.3816
[Epoch:160] Train Loss:1.2757 | Valid Acc:0.3816 | Test Acc:0.3941
[Epoch:170] Train Loss:1.2801 | Valid Acc:0.3842 | Test Acc:0.3822
[Epoch:180] Train Loss:1.2709 | Valid Acc:0.3888 | Test Acc:0.3888
[Epoch:190] Train Loss:1.2471 | Valid Acc:0.3822 | Test Acc:0.3868
[Epoch:200] Train Loss:1.2646 | Valid Acc:0.3796 | Test Acc:0.3796
[Epoch:210] Train Loss:1.2442 | Valid Acc:0.3855 | Test Acc:0.3928
[Epoch:220] Train Loss:1.2394 | Valid Acc:0.3783 | Test Acc:0.3717
[Epoch:230] Train Loss:1.2264 | Valid Acc:0.3809 | Test Acc:0.3868
[Epoch:240] Train Loss:1.2248 | Valid Acc:0.3724 | Test Acc:0.3770
[Epoch:250] Train Loss:1.2234 | Valid Acc:0.3730 | Test Acc:0.3776
[Epoch:260] Train Loss:1.2139 | Valid Acc:0.3770 | Test Acc:0.3836
[Epoch:270] Train Loss:1.2174 | Valid Acc:0.3724 | Test Acc:0.3849
[Epoch:280] Train Loss:1.1997 | Valid Acc:0.3704 | Test Acc:0.3770
[Epoch:290] Train Loss:1.1995 | Valid Acc:0.3658 | Test Acc:0.3822
[Epoch:300] Train Loss:1.1949 | Valid Acc:0.3645 | Test Acc:0.3829
[Epoch:310] Train Loss:1.1895 | Valid Acc:0.3671 | Test Acc:0.3783
[Epoch:320] Train Loss:1.1849 | Valid Acc:0.3691 | Test Acc:0.3862
[Epoch:330] Train Loss:1.1871 | Valid Acc:0.3803 | Test Acc:0.3803
[Epoch:340] Train Loss:1.1815 | Valid Acc:0.3691 | Test Acc:0.3849
[Epoch:350] Train Loss:1.1753 | Valid Acc:0.3678 | Test Acc:0.3809
[Epoch:360] Train Loss:1.1716 | Valid Acc:0.3704 | Test Acc:0.3796
[Epoch:370] Train Loss:1.1646 | Valid Acc:0.3697 | Test Acc:0.3789
[Epoch:380] Train Loss:1.1667 | Valid Acc:0.3711 | Test Acc:0.3789
[Epoch:390] Train Loss:1.1639 | Valid Acc:0.3743 | Test Acc:0.3796
[Epoch:400] Train Loss:1.1579 | Valid Acc:0.3730 | Test Acc:0.3829
[Epoch:410] Train Loss:1.1576 | Valid Acc:0.3743 | Test Acc:0.3855
[Epoch:420] Train Loss:1.1585 | Valid Acc:0.3724 | Test Acc:0.3770
[Epoch:430] Train Loss:1.1497 | Valid Acc:0.3704 | Test Acc:0.3796
[Epoch:440] Train Loss:1.1541 | Valid Acc:0.3684 | Test Acc:0.3757
[Epoch:450] Train Loss:1.1521 | Valid Acc:0.3717 | Test Acc:0.3789
[Epoch:460] Train Loss:1.1461 | Valid Acc:0.3730 | Test Acc:0.3783
[Epoch:470] Train Loss:1.1449 | Valid Acc:0.3724 | Test Acc:0.3770
[Epoch:480] Train Loss:1.1392 | Valid Acc:0.3730 | Test Acc:0.3783
[Epoch:490] Train Loss:1.1430 | Valid Acc:0.3737 | Test Acc:0.3763
[Epoch:500] Train Loss:1.1456 | Valid Acc:0.3757 | Test Acc:0.3776
[Epoch:510] Train Loss:1.1401 | Valid Acc:0.3757 | Test Acc:0.3743
[Epoch:520] Train Loss:1.1372 | Valid Acc:0.3737 | Test Acc:0.3770
[Epoch:530] Train Loss:1.1363 | Valid Acc:0.3724 | Test Acc:0.3743
[Epoch:540] Train Loss:1.1401 | Valid Acc:0.3763 | Test Acc:0.3763
[Epoch:550] Train Loss:1.1347 | Valid Acc:0.3770 | Test Acc:0.3770
[Epoch:560] Train Loss:1.1309 | Valid Acc:0.3757 | Test Acc:0.3776
[Epoch:570] Train Loss:1.1291 | Valid Acc:0.3757 | Test Acc:0.3776
[Epoch:580] Train Loss:1.1334 | Valid Acc:0.3750 | Test Acc:0.3776
[Epoch:590] Train Loss:1.1371 | Valid Acc:0.3763 | Test Acc:0.3789
[Epoch:600] Train Loss:1.1330 | Valid Acc:0.3763 | Test Acc:0.3776
[Epoch:610] Train Loss:1.1309 | Valid Acc:0.3763 | Test Acc:0.3783
[Epoch:620] Train Loss:1.1278 | Valid Acc:0.3757 | Test Acc:0.3770
[Epoch:630] Train Loss:1.1243 | Valid Acc:0.3763 | Test Acc:0.3770
[Epoch:640] Train Loss:1.1341 | Valid Acc:0.3763 | Test Acc:0.3776
[Epoch:650] Train Loss:1.1242 | Valid Acc:0.3763 | Test Acc:0.3776
[Epoch:660] Train Loss:1.1291 | Valid Acc:0.3770 | Test Acc:0.3783
[Epoch:670] Train Loss:1.1289 | Valid Acc:0.3763 | Test Acc:0.3776
[Epoch:680] Train Loss:1.1284 | Valid Acc:0.3763 | Test Acc:0.3770
[Epoch:690] Train Loss:1.1263 | Valid Acc:0.3763 | Test Acc:0.3770
[Epoch:700] Train Loss:1.1298 | Valid Acc:0.3757 | Test Acc:0.3770
[Epoch:710] Train Loss:1.1279 | Valid Acc:0.3770 | Test Acc:0.3770
[Epoch:720] Train Loss:1.1307 | Valid Acc:0.3770 | Test Acc:0.3776
[Epoch:730] Train Loss:1.1257 | Valid Acc:0.3757 | Test Acc:0.3776
[Epoch:740] Train Loss:1.1224 | Valid Acc:0.3757 | Test Acc:0.3770
[Epoch:750] Train Loss:1.1283 | Valid Acc:0.3763 | Test Acc:0.3770
[Epoch:760] Train Loss:1.1244 | Valid Acc:0.3763 | Test Acc:0.3770
[Epoch:770] Train Loss:1.1223 | Valid Acc:0.3763 | Test Acc:0.3770
[Epoch:780] Train Loss:1.1299 | Valid Acc:0.3763 | Test Acc:0.3770
[Epoch:790] Train Loss:1.1296 | Valid Acc:0.3763 | Test Acc:0.3770
Best results: epoch=130, valid loss=1.3537, valid acc=0.4039, test loss:1.3611,test acc=0.3967
{'activation': 'elu', 'dropout': 0, 'hidden_size': 16, 'learning_rate': 0.005319676827013, 'model': 'f2gnn', 'optimizer': 'adagrad', 'weight_decay': 0.000747563700291065}
Current Hyperparameter: {'activation': 'elu', 'dropout': 0, 'hidden_size': 16, 'learning_rate': 0.005319676827013, 'model': 'f2gnn', 'optimizer': 'adagrad', 'weight_decay': 0.000747563700291065}
[Epoch:0] Train Loss:1.6079 | Valid Acc:0.2388 | Test Acc:0.2388
[Epoch:10] Train Loss:1.5660 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:20] Train Loss:1.5548 | Valid Acc:0.2586 | Test Acc:0.2592
[Epoch:30] Train Loss:1.5379 | Valid Acc:0.2711 | Test Acc:0.2704
[Epoch:40] Train Loss:1.5179 | Valid Acc:0.3329 | Test Acc:0.3329
[Epoch:50] Train Loss:1.4896 | Valid Acc:0.3546 | Test Acc:0.3559
[Epoch:60] Train Loss:1.4590 | Valid Acc:0.3500 | Test Acc:0.3474
[Epoch:70] Train Loss:1.4364 | Valid Acc:0.3592 | Test Acc:0.3664
[Epoch:80] Train Loss:1.4073 | Valid Acc:0.3632 | Test Acc:0.3776
[Epoch:90] Train Loss:1.3872 | Valid Acc:0.3651 | Test Acc:0.3803
[Epoch:100] Train Loss:1.3667 | Valid Acc:0.3651 | Test Acc:0.3842
[Epoch:110] Train Loss:1.3562 | Valid Acc:0.3763 | Test Acc:0.3908
[Epoch:120] Train Loss:1.3370 | Valid Acc:0.3711 | Test Acc:0.3947
[Epoch:130] Train Loss:1.3268 | Valid Acc:0.3809 | Test Acc:0.4000
[Epoch:140] Train Loss:1.3186 | Valid Acc:0.3829 | Test Acc:0.4000
[Epoch:150] Train Loss:1.3061 | Valid Acc:0.3836 | Test Acc:0.4020
[Epoch:160] Train Loss:1.2987 | Valid Acc:0.3882 | Test Acc:0.4059
[Epoch:170] Train Loss:1.2937 | Valid Acc:0.3888 | Test Acc:0.3954
[Epoch:180] Train Loss:1.2845 | Valid Acc:0.3849 | Test Acc:0.4000
[Epoch:190] Train Loss:1.2778 | Valid Acc:0.3868 | Test Acc:0.3987
[Epoch:200] Train Loss:1.2727 | Valid Acc:0.3842 | Test Acc:0.3980
[Epoch:210] Train Loss:1.2696 | Valid Acc:0.3914 | Test Acc:0.4033
[Epoch:220] Train Loss:1.2637 | Valid Acc:0.3901 | Test Acc:0.4046
[Epoch:230] Train Loss:1.2586 | Valid Acc:0.3868 | Test Acc:0.4059
[Epoch:240] Train Loss:1.2546 | Valid Acc:0.3895 | Test Acc:0.4046
[Epoch:250] Train Loss:1.2511 | Valid Acc:0.3862 | Test Acc:0.4033
[Epoch:260] Train Loss:1.2478 | Valid Acc:0.3849 | Test Acc:0.4053
[Epoch:270] Train Loss:1.2445 | Valid Acc:0.3868 | Test Acc:0.4013
[Epoch:280] Train Loss:1.2414 | Valid Acc:0.3868 | Test Acc:0.4013
[Epoch:290] Train Loss:1.2384 | Valid Acc:0.3882 | Test Acc:0.4007
[Epoch:300] Train Loss:1.2356 | Valid Acc:0.3868 | Test Acc:0.3961
[Epoch:310] Train Loss:1.2331 | Valid Acc:0.3855 | Test Acc:0.3934
[Epoch:320] Train Loss:1.2306 | Valid Acc:0.3862 | Test Acc:0.3961
[Epoch:330] Train Loss:1.2282 | Valid Acc:0.3809 | Test Acc:0.3987
[Epoch:340] Train Loss:1.2261 | Valid Acc:0.3803 | Test Acc:0.3987
[Epoch:350] Train Loss:1.2241 | Valid Acc:0.3796 | Test Acc:0.3974
[Epoch:360] Train Loss:1.2223 | Valid Acc:0.3783 | Test Acc:0.3967
[Epoch:370] Train Loss:1.2205 | Valid Acc:0.3803 | Test Acc:0.3934
[Epoch:380] Train Loss:1.2189 | Valid Acc:0.3796 | Test Acc:0.3934
[Epoch:390] Train Loss:1.2173 | Valid Acc:0.3789 | Test Acc:0.3941
[Epoch:400] Train Loss:1.2158 | Valid Acc:0.3783 | Test Acc:0.3934
[Epoch:410] Train Loss:1.2144 | Valid Acc:0.3789 | Test Acc:0.3921
[Epoch:420] Train Loss:1.2131 | Valid Acc:0.3776 | Test Acc:0.3921
[Epoch:430] Train Loss:1.2119 | Valid Acc:0.3776 | Test Acc:0.3901
[Epoch:440] Train Loss:1.2107 | Valid Acc:0.3783 | Test Acc:0.3908
[Epoch:450] Train Loss:1.2097 | Valid Acc:0.3783 | Test Acc:0.3901
[Epoch:460] Train Loss:1.2086 | Valid Acc:0.3789 | Test Acc:0.3901
[Epoch:470] Train Loss:1.2077 | Valid Acc:0.3796 | Test Acc:0.3901
[Epoch:480] Train Loss:1.2068 | Valid Acc:0.3789 | Test Acc:0.3901
[Epoch:490] Train Loss:1.2059 | Valid Acc:0.3796 | Test Acc:0.3901
[Epoch:500] Train Loss:1.2051 | Valid Acc:0.3796 | Test Acc:0.3895
[Epoch:510] Train Loss:1.2044 | Valid Acc:0.3796 | Test Acc:0.3895
[Epoch:520] Train Loss:1.2037 | Valid Acc:0.3796 | Test Acc:0.3888
[Epoch:530] Train Loss:1.2031 | Valid Acc:0.3796 | Test Acc:0.3895
[Epoch:540] Train Loss:1.2025 | Valid Acc:0.3789 | Test Acc:0.3901
[Epoch:550] Train Loss:1.2020 | Valid Acc:0.3783 | Test Acc:0.3901
[Epoch:560] Train Loss:1.2015 | Valid Acc:0.3783 | Test Acc:0.3908
[Epoch:570] Train Loss:1.2010 | Valid Acc:0.3776 | Test Acc:0.3908
[Epoch:580] Train Loss:1.2006 | Valid Acc:0.3776 | Test Acc:0.3908
[Epoch:590] Train Loss:1.2002 | Valid Acc:0.3776 | Test Acc:0.3901
[Epoch:600] Train Loss:1.1999 | Valid Acc:0.3789 | Test Acc:0.3895
[Epoch:610] Train Loss:1.1996 | Valid Acc:0.3803 | Test Acc:0.3895
[Epoch:620] Train Loss:1.1993 | Valid Acc:0.3809 | Test Acc:0.3908
[Epoch:630] Train Loss:1.1991 | Valid Acc:0.3809 | Test Acc:0.3908
[Epoch:640] Train Loss:1.1989 | Valid Acc:0.3809 | Test Acc:0.3914
[Epoch:650] Train Loss:1.1987 | Valid Acc:0.3809 | Test Acc:0.3921
[Epoch:660] Train Loss:1.1985 | Valid Acc:0.3803 | Test Acc:0.3921
[Epoch:670] Train Loss:1.1983 | Valid Acc:0.3803 | Test Acc:0.3921
[Epoch:680] Train Loss:1.1982 | Valid Acc:0.3803 | Test Acc:0.3921
[Epoch:690] Train Loss:1.1981 | Valid Acc:0.3809 | Test Acc:0.3921
[Epoch:700] Train Loss:1.1980 | Valid Acc:0.3809 | Test Acc:0.3921
[Epoch:710] Train Loss:1.1980 | Valid Acc:0.3809 | Test Acc:0.3921
[Epoch:720] Train Loss:1.1979 | Valid Acc:0.3803 | Test Acc:0.3921
[Epoch:730] Train Loss:1.1979 | Valid Acc:0.3803 | Test Acc:0.3921
[Epoch:740] Train Loss:1.1978 | Valid Acc:0.3803 | Test Acc:0.3921
[Epoch:750] Train Loss:1.1978 | Valid Acc:0.3803 | Test Acc:0.3921
[Epoch:760] Train Loss:1.1978 | Valid Acc:0.3803 | Test Acc:0.3921
[Epoch:770] Train Loss:1.1978 | Valid Acc:0.3803 | Test Acc:0.3921
[Epoch:780] Train Loss:1.1978 | Valid Acc:0.3803 | Test Acc:0.3921
[Epoch:790] Train Loss:1.1978 | Valid Acc:0.3803 | Test Acc:0.3921
Best results: epoch=204, valid loss=1.3475, valid acc=0.3914, test loss:1.3457,test acc=0.3993
{'activation': 'relu', 'dropout': 2, 'hidden_size': 128, 'learning_rate': 0.002116974073439665, 'model': 'f2gnn', 'optimizer': 'adam', 'weight_decay': 0.0001037664689191369}
Current Hyperparameter: {'activation': 'relu', 'dropout': 2, 'hidden_size': 128, 'learning_rate': 0.002116974073439665, 'model': 'f2gnn', 'optimizer': 'adam', 'weight_decay': 0.0001037664689191369}
[Epoch:0] Train Loss:1.6525 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:10] Train Loss:1.5717 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:20] Train Loss:1.5601 | Valid Acc:0.2691 | Test Acc:0.2625
[Epoch:30] Train Loss:1.4914 | Valid Acc:0.3257 | Test Acc:0.3145
[Epoch:40] Train Loss:1.4123 | Valid Acc:0.3651 | Test Acc:0.3618
[Epoch:50] Train Loss:1.3456 | Valid Acc:0.3875 | Test Acc:0.4072
[Epoch:60] Train Loss:1.3097 | Valid Acc:0.4046 | Test Acc:0.4039
[Epoch:70] Train Loss:1.2746 | Valid Acc:0.3934 | Test Acc:0.4099
[Epoch:80] Train Loss:1.2459 | Valid Acc:0.3868 | Test Acc:0.3987
[Epoch:90] Train Loss:1.2142 | Valid Acc:0.3783 | Test Acc:0.3849
[Epoch:100] Train Loss:1.1931 | Valid Acc:0.3724 | Test Acc:0.3750
[Epoch:110] Train Loss:1.1513 | Valid Acc:0.3658 | Test Acc:0.3678
[Epoch:120] Train Loss:1.1341 | Valid Acc:0.3704 | Test Acc:0.3638
[Epoch:130] Train Loss:1.0961 | Valid Acc:0.3730 | Test Acc:0.3730
[Epoch:140] Train Loss:1.0598 | Valid Acc:0.3776 | Test Acc:0.3684
[Epoch:150] Train Loss:1.0280 | Valid Acc:0.3743 | Test Acc:0.3664
[Epoch:160] Train Loss:1.0013 | Valid Acc:0.3704 | Test Acc:0.3572
[Epoch:170] Train Loss:0.9506 | Valid Acc:0.3763 | Test Acc:0.3553
[Epoch:180] Train Loss:0.9037 | Valid Acc:0.3664 | Test Acc:0.3599
[Epoch:190] Train Loss:0.8725 | Valid Acc:0.3625 | Test Acc:0.3618
[Epoch:200] Train Loss:0.8276 | Valid Acc:0.3658 | Test Acc:0.3474
[Epoch:210] Train Loss:0.7889 | Valid Acc:0.3605 | Test Acc:0.3599
[Epoch:220] Train Loss:0.7268 | Valid Acc:0.3566 | Test Acc:0.3513
[Epoch:230] Train Loss:0.6880 | Valid Acc:0.3553 | Test Acc:0.3421
[Epoch:240] Train Loss:0.6460 | Valid Acc:0.3579 | Test Acc:0.3493
[Epoch:250] Train Loss:0.6090 | Valid Acc:0.3572 | Test Acc:0.3441
[Epoch:260] Train Loss:0.5742 | Valid Acc:0.3572 | Test Acc:0.3474
[Epoch:270] Train Loss:0.5460 | Valid Acc:0.3579 | Test Acc:0.3467
[Epoch:280] Train Loss:0.5144 | Valid Acc:0.3526 | Test Acc:0.3395
[Epoch:290] Train Loss:0.4864 | Valid Acc:0.3513 | Test Acc:0.3467
[Epoch:300] Train Loss:0.4797 | Valid Acc:0.3500 | Test Acc:0.3441
[Epoch:310] Train Loss:0.4453 | Valid Acc:0.3487 | Test Acc:0.3447
[Epoch:320] Train Loss:0.4324 | Valid Acc:0.3500 | Test Acc:0.3368
[Epoch:330] Train Loss:0.4245 | Valid Acc:0.3520 | Test Acc:0.3487
[Epoch:340] Train Loss:0.4086 | Valid Acc:0.3586 | Test Acc:0.3513
[Epoch:350] Train Loss:0.3974 | Valid Acc:0.3507 | Test Acc:0.3467
[Epoch:360] Train Loss:0.3949 | Valid Acc:0.3493 | Test Acc:0.3539
[Epoch:370] Train Loss:0.3819 | Valid Acc:0.3434 | Test Acc:0.3447
[Epoch:380] Train Loss:0.3749 | Valid Acc:0.3480 | Test Acc:0.3493
[Epoch:390] Train Loss:0.3721 | Valid Acc:0.3447 | Test Acc:0.3474
[Epoch:400] Train Loss:0.3711 | Valid Acc:0.3493 | Test Acc:0.3500
[Epoch:410] Train Loss:0.3608 | Valid Acc:0.3513 | Test Acc:0.3586
[Epoch:420] Train Loss:0.3518 | Valid Acc:0.3447 | Test Acc:0.3454
[Epoch:430] Train Loss:0.3598 | Valid Acc:0.3500 | Test Acc:0.3474
[Epoch:440] Train Loss:0.3550 | Valid Acc:0.3487 | Test Acc:0.3480
[Epoch:450] Train Loss:0.3398 | Valid Acc:0.3441 | Test Acc:0.3474
[Epoch:460] Train Loss:0.3438 | Valid Acc:0.3487 | Test Acc:0.3487
[Epoch:470] Train Loss:0.3460 | Valid Acc:0.3474 | Test Acc:0.3500
[Epoch:480] Train Loss:0.3388 | Valid Acc:0.3461 | Test Acc:0.3480
[Epoch:490] Train Loss:0.3346 | Valid Acc:0.3434 | Test Acc:0.3507
[Epoch:500] Train Loss:0.3250 | Valid Acc:0.3474 | Test Acc:0.3507
[Epoch:510] Train Loss:0.3376 | Valid Acc:0.3480 | Test Acc:0.3441
[Epoch:520] Train Loss:0.3268 | Valid Acc:0.3520 | Test Acc:0.3507
[Epoch:530] Train Loss:0.3260 | Valid Acc:0.3467 | Test Acc:0.3454
[Epoch:540] Train Loss:0.3210 | Valid Acc:0.3467 | Test Acc:0.3441
[Epoch:550] Train Loss:0.3184 | Valid Acc:0.3533 | Test Acc:0.3382
[Epoch:560] Train Loss:0.3280 | Valid Acc:0.3493 | Test Acc:0.3428
[Epoch:570] Train Loss:0.3194 | Valid Acc:0.3441 | Test Acc:0.3474
[Epoch:580] Train Loss:0.3203 | Valid Acc:0.3474 | Test Acc:0.3414
[Epoch:590] Train Loss:0.3227 | Valid Acc:0.3461 | Test Acc:0.3414
[Epoch:600] Train Loss:0.3181 | Valid Acc:0.3487 | Test Acc:0.3395
[Epoch:610] Train Loss:0.3162 | Valid Acc:0.3526 | Test Acc:0.3428
[Epoch:620] Train Loss:0.3156 | Valid Acc:0.3461 | Test Acc:0.3395
[Epoch:630] Train Loss:0.3111 | Valid Acc:0.3513 | Test Acc:0.3408
[Epoch:640] Train Loss:0.3180 | Valid Acc:0.3434 | Test Acc:0.3447
[Epoch:650] Train Loss:0.3136 | Valid Acc:0.3461 | Test Acc:0.3441
[Epoch:660] Train Loss:0.3090 | Valid Acc:0.3500 | Test Acc:0.3408
[Epoch:670] Train Loss:0.3115 | Valid Acc:0.3474 | Test Acc:0.3382
[Epoch:680] Train Loss:0.3064 | Valid Acc:0.3474 | Test Acc:0.3388
[Epoch:690] Train Loss:0.3094 | Valid Acc:0.3441 | Test Acc:0.3401
[Epoch:700] Train Loss:0.3104 | Valid Acc:0.3467 | Test Acc:0.3395
[Epoch:710] Train Loss:0.3097 | Valid Acc:0.3461 | Test Acc:0.3421
[Epoch:720] Train Loss:0.3107 | Valid Acc:0.3474 | Test Acc:0.3395
[Epoch:730] Train Loss:0.3148 | Valid Acc:0.3500 | Test Acc:0.3395
[Epoch:740] Train Loss:0.3122 | Valid Acc:0.3480 | Test Acc:0.3388
[Epoch:750] Train Loss:0.3086 | Valid Acc:0.3493 | Test Acc:0.3395
[Epoch:760] Train Loss:0.3184 | Valid Acc:0.3493 | Test Acc:0.3388
[Epoch:770] Train Loss:0.3058 | Valid Acc:0.3480 | Test Acc:0.3395
[Epoch:780] Train Loss:0.3120 | Valid Acc:0.3461 | Test Acc:0.3401
[Epoch:790] Train Loss:0.3092 | Valid Acc:0.3467 | Test Acc:0.3408
Best results: epoch=54, valid loss=1.3343, valid acc=0.4138, test loss:1.3401,test acc=0.4105
{'activation': 'relu', 'dropout': 1, 'hidden_size': 128, 'learning_rate': 0.003896890397413533, 'model': 'f2gnn', 'optimizer': 'adam', 'weight_decay': 0.00010597633212260216}
Current Hyperparameter: {'activation': 'relu', 'dropout': 1, 'hidden_size': 128, 'learning_rate': 0.003896890397413533, 'model': 'f2gnn', 'optimizer': 'adam', 'weight_decay': 0.00010597633212260216}
[Epoch:0] Train Loss:1.6417 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:10] Train Loss:1.5691 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:20] Train Loss:1.5375 | Valid Acc:0.2783 | Test Acc:0.2809
[Epoch:30] Train Loss:1.3999 | Valid Acc:0.3605 | Test Acc:0.3737
[Epoch:40] Train Loss:1.2832 | Valid Acc:0.3803 | Test Acc:0.3888
[Epoch:50] Train Loss:1.2262 | Valid Acc:0.3849 | Test Acc:0.3882
[Epoch:60] Train Loss:1.1738 | Valid Acc:0.3750 | Test Acc:0.3770
[Epoch:70] Train Loss:1.1239 | Valid Acc:0.3901 | Test Acc:0.3849
[Epoch:80] Train Loss:1.0968 | Valid Acc:0.3783 | Test Acc:0.3678
[Epoch:90] Train Loss:1.0510 | Valid Acc:0.3737 | Test Acc:0.3737
[Epoch:100] Train Loss:1.0196 | Valid Acc:0.3737 | Test Acc:0.3704
[Epoch:110] Train Loss:0.9873 | Valid Acc:0.3691 | Test Acc:0.3618
[Epoch:120] Train Loss:0.9851 | Valid Acc:0.3671 | Test Acc:0.3625
[Epoch:130] Train Loss:0.9280 | Valid Acc:0.3704 | Test Acc:0.3526
[Epoch:140] Train Loss:0.8957 | Valid Acc:0.3750 | Test Acc:0.3539
[Epoch:150] Train Loss:0.8582 | Valid Acc:0.3704 | Test Acc:0.3586
[Epoch:160] Train Loss:0.8177 | Valid Acc:0.3697 | Test Acc:0.3507
[Epoch:170] Train Loss:0.7949 | Valid Acc:0.3618 | Test Acc:0.3579
[Epoch:180] Train Loss:0.7785 | Valid Acc:0.3678 | Test Acc:0.3553
[Epoch:190] Train Loss:0.7300 | Valid Acc:0.3717 | Test Acc:0.3500
[Epoch:200] Train Loss:0.6862 | Valid Acc:0.3645 | Test Acc:0.3605
[Epoch:210] Train Loss:0.6503 | Valid Acc:0.3638 | Test Acc:0.3533
[Epoch:220] Train Loss:0.6123 | Valid Acc:0.3697 | Test Acc:0.3447
[Epoch:230] Train Loss:0.6252 | Valid Acc:0.3612 | Test Acc:0.3507
[Epoch:240] Train Loss:0.5795 | Valid Acc:0.3632 | Test Acc:0.3566
[Epoch:250] Train Loss:0.5335 | Valid Acc:0.3579 | Test Acc:0.3467
[Epoch:260] Train Loss:0.5146 | Valid Acc:0.3605 | Test Acc:0.3579
[Epoch:270] Train Loss:0.4853 | Valid Acc:0.3684 | Test Acc:0.3539
[Epoch:280] Train Loss:0.4546 | Valid Acc:0.3645 | Test Acc:0.3493
[Epoch:290] Train Loss:0.4517 | Valid Acc:0.3592 | Test Acc:0.3500
[Epoch:300] Train Loss:0.4327 | Valid Acc:0.3605 | Test Acc:0.3539
[Epoch:310] Train Loss:0.4247 | Valid Acc:0.3553 | Test Acc:0.3507
[Epoch:320] Train Loss:0.4099 | Valid Acc:0.3632 | Test Acc:0.3520
[Epoch:330] Train Loss:0.3993 | Valid Acc:0.3599 | Test Acc:0.3586
[Epoch:340] Train Loss:0.3944 | Valid Acc:0.3632 | Test Acc:0.3592
[Epoch:350] Train Loss:0.3790 | Valid Acc:0.3697 | Test Acc:0.3546
[Epoch:360] Train Loss:0.3612 | Valid Acc:0.3651 | Test Acc:0.3520
[Epoch:370] Train Loss:0.3519 | Valid Acc:0.3645 | Test Acc:0.3493
[Epoch:380] Train Loss:0.3485 | Valid Acc:0.3632 | Test Acc:0.3526
[Epoch:390] Train Loss:0.3424 | Valid Acc:0.3559 | Test Acc:0.3592
[Epoch:400] Train Loss:0.3358 | Valid Acc:0.3605 | Test Acc:0.3592
[Epoch:410] Train Loss:0.3366 | Valid Acc:0.3658 | Test Acc:0.3566
[Epoch:420] Train Loss:0.3282 | Valid Acc:0.3599 | Test Acc:0.3579
[Epoch:430] Train Loss:0.3306 | Valid Acc:0.3533 | Test Acc:0.3559
[Epoch:440] Train Loss:0.3249 | Valid Acc:0.3526 | Test Acc:0.3507
[Epoch:450] Train Loss:0.3228 | Valid Acc:0.3605 | Test Acc:0.3507
[Epoch:460] Train Loss:0.3162 | Valid Acc:0.3599 | Test Acc:0.3566
[Epoch:470] Train Loss:0.3141 | Valid Acc:0.3566 | Test Acc:0.3539
[Epoch:480] Train Loss:0.3073 | Valid Acc:0.3586 | Test Acc:0.3539
[Epoch:490] Train Loss:0.3120 | Valid Acc:0.3579 | Test Acc:0.3546
[Epoch:500] Train Loss:0.3076 | Valid Acc:0.3546 | Test Acc:0.3559
[Epoch:510] Train Loss:0.3144 | Valid Acc:0.3546 | Test Acc:0.3546
[Epoch:520] Train Loss:0.3000 | Valid Acc:0.3566 | Test Acc:0.3500
[Epoch:530] Train Loss:0.3064 | Valid Acc:0.3566 | Test Acc:0.3520
[Epoch:540] Train Loss:0.3038 | Valid Acc:0.3566 | Test Acc:0.3507
[Epoch:550] Train Loss:0.3029 | Valid Acc:0.3645 | Test Acc:0.3500
[Epoch:560] Train Loss:0.2992 | Valid Acc:0.3618 | Test Acc:0.3487
[Epoch:570] Train Loss:0.3014 | Valid Acc:0.3592 | Test Acc:0.3487
[Epoch:580] Train Loss:0.3005 | Valid Acc:0.3605 | Test Acc:0.3520
[Epoch:590] Train Loss:0.2939 | Valid Acc:0.3599 | Test Acc:0.3553
[Epoch:600] Train Loss:0.2923 | Valid Acc:0.3566 | Test Acc:0.3572
[Epoch:610] Train Loss:0.2912 | Valid Acc:0.3605 | Test Acc:0.3513
[Epoch:620] Train Loss:0.2940 | Valid Acc:0.3612 | Test Acc:0.3493
[Epoch:630] Train Loss:0.2955 | Valid Acc:0.3546 | Test Acc:0.3533
[Epoch:640] Train Loss:0.2904 | Valid Acc:0.3559 | Test Acc:0.3559
[Epoch:650] Train Loss:0.2906 | Valid Acc:0.3586 | Test Acc:0.3480
[Epoch:660] Train Loss:0.2868 | Valid Acc:0.3579 | Test Acc:0.3500
[Epoch:670] Train Loss:0.2970 | Valid Acc:0.3572 | Test Acc:0.3520
[Epoch:680] Train Loss:0.2880 | Valid Acc:0.3592 | Test Acc:0.3493
[Epoch:690] Train Loss:0.2960 | Valid Acc:0.3599 | Test Acc:0.3507
[Epoch:700] Train Loss:0.2899 | Valid Acc:0.3592 | Test Acc:0.3513
[Epoch:710] Train Loss:0.2908 | Valid Acc:0.3572 | Test Acc:0.3513
[Epoch:720] Train Loss:0.2928 | Valid Acc:0.3592 | Test Acc:0.3500
[Epoch:730] Train Loss:0.2911 | Valid Acc:0.3605 | Test Acc:0.3507
[Epoch:740] Train Loss:0.2900 | Valid Acc:0.3586 | Test Acc:0.3507
[Epoch:750] Train Loss:0.2951 | Valid Acc:0.3586 | Test Acc:0.3513
[Epoch:760] Train Loss:0.2968 | Valid Acc:0.3592 | Test Acc:0.3513
[Epoch:770] Train Loss:0.2901 | Valid Acc:0.3572 | Test Acc:0.3526
[Epoch:780] Train Loss:0.2935 | Valid Acc:0.3566 | Test Acc:0.3533
[Epoch:790] Train Loss:0.2934 | Valid Acc:0.3559 | Test Acc:0.3533
Best results: epoch=77, valid loss=1.4791, valid acc=0.3934, test loss:1.4566,test acc=0.3789
{'activation': 'relu', 'dropout': 2, 'hidden_size': 128, 'learning_rate': 0.0025958163238602645, 'model': 'f2gnn', 'optimizer': 'adam', 'weight_decay': 0.00020148846958869854}
Current Hyperparameter: {'activation': 'relu', 'dropout': 2, 'hidden_size': 128, 'learning_rate': 0.0025958163238602645, 'model': 'f2gnn', 'optimizer': 'adam', 'weight_decay': 0.00020148846958869854}
[Epoch:0] Train Loss:1.6525 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:10] Train Loss:1.5717 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:20] Train Loss:1.5615 | Valid Acc:0.2711 | Test Acc:0.2717
[Epoch:30] Train Loss:1.4928 | Valid Acc:0.3145 | Test Acc:0.3303
[Epoch:40] Train Loss:1.4327 | Valid Acc:0.3711 | Test Acc:0.3664
[Epoch:50] Train Loss:1.3622 | Valid Acc:0.3967 | Test Acc:0.4007
[Epoch:60] Train Loss:1.3103 | Valid Acc:0.3987 | Test Acc:0.4039
[Epoch:70] Train Loss:1.2795 | Valid Acc:0.3974 | Test Acc:0.3993
[Epoch:80] Train Loss:1.2554 | Valid Acc:0.3895 | Test Acc:0.3717
[Epoch:90] Train Loss:1.2194 | Valid Acc:0.3691 | Test Acc:0.3625
[Epoch:100] Train Loss:1.2026 | Valid Acc:0.3678 | Test Acc:0.3658
[Epoch:110] Train Loss:1.1675 | Valid Acc:0.3599 | Test Acc:0.3533
[Epoch:120] Train Loss:1.1601 | Valid Acc:0.3809 | Test Acc:0.3941
[Epoch:130] Train Loss:1.1291 | Valid Acc:0.3796 | Test Acc:0.3776
[Epoch:140] Train Loss:1.0929 | Valid Acc:0.3743 | Test Acc:0.3678
[Epoch:150] Train Loss:1.0688 | Valid Acc:0.3757 | Test Acc:0.3612
[Epoch:160] Train Loss:1.0212 | Valid Acc:0.3678 | Test Acc:0.3526
[Epoch:170] Train Loss:0.9843 | Valid Acc:0.3684 | Test Acc:0.3533
[Epoch:180] Train Loss:0.9275 | Valid Acc:0.3586 | Test Acc:0.3507
[Epoch:190] Train Loss:0.8971 | Valid Acc:0.3737 | Test Acc:0.3592
[Epoch:200] Train Loss:0.8412 | Valid Acc:0.3651 | Test Acc:0.3579
[Epoch:210] Train Loss:0.7992 | Valid Acc:0.3691 | Test Acc:0.3493
[Epoch:220] Train Loss:0.7297 | Valid Acc:0.3618 | Test Acc:0.3428
[Epoch:230] Train Loss:0.6902 | Valid Acc:0.3638 | Test Acc:0.3428
[Epoch:240] Train Loss:0.6746 | Valid Acc:0.3632 | Test Acc:0.3493
[Epoch:250] Train Loss:0.5745 | Valid Acc:0.3579 | Test Acc:0.3533
[Epoch:260] Train Loss:0.5571 | Valid Acc:0.3566 | Test Acc:0.3395
[Epoch:270] Train Loss:0.5396 | Valid Acc:0.3526 | Test Acc:0.3493
[Epoch:280] Train Loss:0.5118 | Valid Acc:0.3500 | Test Acc:0.3553
[Epoch:290] Train Loss:0.4802 | Valid Acc:0.3520 | Test Acc:0.3546
[Epoch:300] Train Loss:0.4551 | Valid Acc:0.3434 | Test Acc:0.3572
[Epoch:310] Train Loss:0.4344 | Valid Acc:0.3441 | Test Acc:0.3500
[Epoch:320] Train Loss:0.4217 | Valid Acc:0.3441 | Test Acc:0.3520
[Epoch:330] Train Loss:0.4065 | Valid Acc:0.3447 | Test Acc:0.3533
[Epoch:340] Train Loss:0.4021 | Valid Acc:0.3414 | Test Acc:0.3566
[Epoch:350] Train Loss:0.3971 | Valid Acc:0.3421 | Test Acc:0.3513
[Epoch:360] Train Loss:0.3864 | Valid Acc:0.3441 | Test Acc:0.3546
[Epoch:370] Train Loss:0.3739 | Valid Acc:0.3382 | Test Acc:0.3559
[Epoch:380] Train Loss:0.3713 | Valid Acc:0.3454 | Test Acc:0.3539
[Epoch:390] Train Loss:0.3705 | Valid Acc:0.3401 | Test Acc:0.3566
[Epoch:400] Train Loss:0.3611 | Valid Acc:0.3480 | Test Acc:0.3586
[Epoch:410] Train Loss:0.3536 | Valid Acc:0.3539 | Test Acc:0.3586
[Epoch:420] Train Loss:0.3512 | Valid Acc:0.3520 | Test Acc:0.3586
[Epoch:430] Train Loss:0.3570 | Valid Acc:0.3421 | Test Acc:0.3605
[Epoch:440] Train Loss:0.3588 | Valid Acc:0.3480 | Test Acc:0.3539
[Epoch:450] Train Loss:0.3458 | Valid Acc:0.3493 | Test Acc:0.3487
[Epoch:460] Train Loss:0.3518 | Valid Acc:0.3467 | Test Acc:0.3586
[Epoch:470] Train Loss:0.3386 | Valid Acc:0.3546 | Test Acc:0.3480
[Epoch:480] Train Loss:0.3297 | Valid Acc:0.3546 | Test Acc:0.3546
[Epoch:490] Train Loss:0.3331 | Valid Acc:0.3533 | Test Acc:0.3566
[Epoch:500] Train Loss:0.3303 | Valid Acc:0.3500 | Test Acc:0.3572
[Epoch:510] Train Loss:0.3383 | Valid Acc:0.3434 | Test Acc:0.3645
[Epoch:520] Train Loss:0.3346 | Valid Acc:0.3454 | Test Acc:0.3599
[Epoch:530] Train Loss:0.3264 | Valid Acc:0.3500 | Test Acc:0.3539
[Epoch:540] Train Loss:0.3252 | Valid Acc:0.3434 | Test Acc:0.3645
[Epoch:550] Train Loss:0.3258 | Valid Acc:0.3467 | Test Acc:0.3553
[Epoch:560] Train Loss:0.3258 | Valid Acc:0.3461 | Test Acc:0.3566
[Epoch:570] Train Loss:0.3266 | Valid Acc:0.3461 | Test Acc:0.3572
[Epoch:580] Train Loss:0.3200 | Valid Acc:0.3513 | Test Acc:0.3579
[Epoch:590] Train Loss:0.3205 | Valid Acc:0.3487 | Test Acc:0.3566
[Epoch:600] Train Loss:0.3234 | Valid Acc:0.3546 | Test Acc:0.3520
[Epoch:610] Train Loss:0.3197 | Valid Acc:0.3520 | Test Acc:0.3592
[Epoch:620] Train Loss:0.3198 | Valid Acc:0.3539 | Test Acc:0.3553
[Epoch:630] Train Loss:0.3182 | Valid Acc:0.3500 | Test Acc:0.3539
[Epoch:640] Train Loss:0.3128 | Valid Acc:0.3520 | Test Acc:0.3526
[Epoch:650] Train Loss:0.3153 | Valid Acc:0.3539 | Test Acc:0.3579
[Epoch:660] Train Loss:0.3127 | Valid Acc:0.3507 | Test Acc:0.3546
[Epoch:670] Train Loss:0.3088 | Valid Acc:0.3500 | Test Acc:0.3559
[Epoch:680] Train Loss:0.3163 | Valid Acc:0.3507 | Test Acc:0.3520
[Epoch:690] Train Loss:0.3164 | Valid Acc:0.3493 | Test Acc:0.3539
[Epoch:700] Train Loss:0.3142 | Valid Acc:0.3513 | Test Acc:0.3500
[Epoch:710] Train Loss:0.3124 | Valid Acc:0.3493 | Test Acc:0.3533
[Epoch:720] Train Loss:0.3107 | Valid Acc:0.3526 | Test Acc:0.3539
[Epoch:730] Train Loss:0.3096 | Valid Acc:0.3487 | Test Acc:0.3546
[Epoch:740] Train Loss:0.3158 | Valid Acc:0.3467 | Test Acc:0.3546
[Epoch:750] Train Loss:0.3130 | Valid Acc:0.3493 | Test Acc:0.3546
[Epoch:760] Train Loss:0.3148 | Valid Acc:0.3500 | Test Acc:0.3546
[Epoch:770] Train Loss:0.3215 | Valid Acc:0.3526 | Test Acc:0.3546
[Epoch:780] Train Loss:0.3122 | Valid Acc:0.3526 | Test Acc:0.3553
[Epoch:790] Train Loss:0.3139 | Valid Acc:0.3513 | Test Acc:0.3553
Best results: epoch=56, valid loss=1.3329, valid acc=0.4053, test loss:1.3393,test acc=0.4079
{'activation': 'relu', 'dropout': 2, 'hidden_size': 128, 'learning_rate': 0.001751471492739323, 'model': 'f2gnn', 'optimizer': 'adam', 'weight_decay': 0.0003501642422925773}
Current Hyperparameter: {'activation': 'relu', 'dropout': 2, 'hidden_size': 128, 'learning_rate': 0.001751471492739323, 'model': 'f2gnn', 'optimizer': 'adam', 'weight_decay': 0.0003501642422925773}
[Epoch:0] Train Loss:1.6525 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:10] Train Loss:1.5729 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:20] Train Loss:1.5673 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:30] Train Loss:1.5343 | Valid Acc:0.3132 | Test Acc:0.3276
[Epoch:40] Train Loss:1.4509 | Valid Acc:0.3178 | Test Acc:0.3283
[Epoch:50] Train Loss:1.3963 | Valid Acc:0.3724 | Test Acc:0.3941
[Epoch:60] Train Loss:1.3404 | Valid Acc:0.3934 | Test Acc:0.4125
[Epoch:70] Train Loss:1.3061 | Valid Acc:0.3928 | Test Acc:0.4000
[Epoch:80] Train Loss:1.2791 | Valid Acc:0.3961 | Test Acc:0.3776
[Epoch:90] Train Loss:1.2542 | Valid Acc:0.3895 | Test Acc:0.3730
[Epoch:100] Train Loss:1.2352 | Valid Acc:0.3842 | Test Acc:0.3829
[Epoch:110] Train Loss:1.2146 | Valid Acc:0.3612 | Test Acc:0.3539
[Epoch:120] Train Loss:1.1919 | Valid Acc:0.3776 | Test Acc:0.3697
[Epoch:130] Train Loss:1.1642 | Valid Acc:0.3737 | Test Acc:0.3757
[Epoch:140] Train Loss:1.1492 | Valid Acc:0.3737 | Test Acc:0.3651
[Epoch:150] Train Loss:1.1240 | Valid Acc:0.3678 | Test Acc:0.3645
[Epoch:160] Train Loss:1.1238 | Valid Acc:0.3618 | Test Acc:0.3664
[Epoch:170] Train Loss:1.0825 | Valid Acc:0.3691 | Test Acc:0.3612
[Epoch:180] Train Loss:1.0505 | Valid Acc:0.3724 | Test Acc:0.3730
[Epoch:190] Train Loss:1.0211 | Valid Acc:0.3684 | Test Acc:0.3618
[Epoch:200] Train Loss:0.9881 | Valid Acc:0.3704 | Test Acc:0.3645
[Epoch:210] Train Loss:1.0074 | Valid Acc:0.3770 | Test Acc:0.3559
[Epoch:220] Train Loss:0.9252 | Valid Acc:0.3658 | Test Acc:0.3401
[Epoch:230] Train Loss:0.8833 | Valid Acc:0.3730 | Test Acc:0.3461
[Epoch:240] Train Loss:0.8366 | Valid Acc:0.3717 | Test Acc:0.3474
[Epoch:250] Train Loss:0.7938 | Valid Acc:0.3625 | Test Acc:0.3441
[Epoch:260] Train Loss:0.7570 | Valid Acc:0.3632 | Test Acc:0.3474
[Epoch:270] Train Loss:0.7127 | Valid Acc:0.3618 | Test Acc:0.3500
[Epoch:280] Train Loss:0.6758 | Valid Acc:0.3638 | Test Acc:0.3467
[Epoch:290] Train Loss:0.6429 | Valid Acc:0.3599 | Test Acc:0.3355
[Epoch:300] Train Loss:0.6085 | Valid Acc:0.3592 | Test Acc:0.3461
[Epoch:310] Train Loss:0.5892 | Valid Acc:0.3507 | Test Acc:0.3533
[Epoch:320] Train Loss:0.5631 | Valid Acc:0.3539 | Test Acc:0.3480
[Epoch:330] Train Loss:0.5376 | Valid Acc:0.3559 | Test Acc:0.3461
[Epoch:340] Train Loss:0.5097 | Valid Acc:0.3592 | Test Acc:0.3467
[Epoch:350] Train Loss:0.5014 | Valid Acc:0.3539 | Test Acc:0.3572
[Epoch:360] Train Loss:0.4868 | Valid Acc:0.3507 | Test Acc:0.3572
[Epoch:370] Train Loss:0.4664 | Valid Acc:0.3546 | Test Acc:0.3539
[Epoch:380] Train Loss:0.4660 | Valid Acc:0.3539 | Test Acc:0.3566
[Epoch:390] Train Loss:0.4481 | Valid Acc:0.3507 | Test Acc:0.3625
[Epoch:400] Train Loss:0.4402 | Valid Acc:0.3428 | Test Acc:0.3605
[Epoch:410] Train Loss:0.4389 | Valid Acc:0.3461 | Test Acc:0.3632
[Epoch:420] Train Loss:0.4217 | Valid Acc:0.3434 | Test Acc:0.3605
[Epoch:430] Train Loss:0.4280 | Valid Acc:0.3421 | Test Acc:0.3632
[Epoch:440] Train Loss:0.4174 | Valid Acc:0.3474 | Test Acc:0.3632
[Epoch:450] Train Loss:0.4012 | Valid Acc:0.3474 | Test Acc:0.3612
[Epoch:460] Train Loss:0.3973 | Valid Acc:0.3513 | Test Acc:0.3664
[Epoch:470] Train Loss:0.4036 | Valid Acc:0.3487 | Test Acc:0.3605
[Epoch:480] Train Loss:0.3951 | Valid Acc:0.3467 | Test Acc:0.3586
[Epoch:490] Train Loss:0.3865 | Valid Acc:0.3520 | Test Acc:0.3546
[Epoch:500] Train Loss:0.3855 | Valid Acc:0.3447 | Test Acc:0.3605
[Epoch:510] Train Loss:0.3814 | Valid Acc:0.3500 | Test Acc:0.3572
[Epoch:520] Train Loss:0.3910 | Valid Acc:0.3493 | Test Acc:0.3579
[Epoch:530] Train Loss:0.3763 | Valid Acc:0.3454 | Test Acc:0.3559
[Epoch:540] Train Loss:0.3742 | Valid Acc:0.3474 | Test Acc:0.3553
[Epoch:550] Train Loss:0.3739 | Valid Acc:0.3461 | Test Acc:0.3566
[Epoch:560] Train Loss:0.3799 | Valid Acc:0.3461 | Test Acc:0.3599
[Epoch:570] Train Loss:0.3644 | Valid Acc:0.3461 | Test Acc:0.3566
[Epoch:580] Train Loss:0.3682 | Valid Acc:0.3513 | Test Acc:0.3579
[Epoch:590] Train Loss:0.3594 | Valid Acc:0.3480 | Test Acc:0.3572
[Epoch:600] Train Loss:0.3654 | Valid Acc:0.3467 | Test Acc:0.3579
[Epoch:610] Train Loss:0.3638 | Valid Acc:0.3487 | Test Acc:0.3546
[Epoch:620] Train Loss:0.3646 | Valid Acc:0.3454 | Test Acc:0.3579
[Epoch:630] Train Loss:0.3560 | Valid Acc:0.3493 | Test Acc:0.3559
[Epoch:640] Train Loss:0.3581 | Valid Acc:0.3487 | Test Acc:0.3572
[Epoch:650] Train Loss:0.3659 | Valid Acc:0.3493 | Test Acc:0.3520
[Epoch:660] Train Loss:0.3573 | Valid Acc:0.3480 | Test Acc:0.3546
[Epoch:670] Train Loss:0.3516 | Valid Acc:0.3480 | Test Acc:0.3559
[Epoch:680] Train Loss:0.3567 | Valid Acc:0.3539 | Test Acc:0.3579
[Epoch:690] Train Loss:0.3532 | Valid Acc:0.3487 | Test Acc:0.3559
[Epoch:700] Train Loss:0.3574 | Valid Acc:0.3493 | Test Acc:0.3559
[Epoch:710] Train Loss:0.3557 | Valid Acc:0.3513 | Test Acc:0.3553
[Epoch:720] Train Loss:0.3477 | Valid Acc:0.3480 | Test Acc:0.3559
[Epoch:730] Train Loss:0.3569 | Valid Acc:0.3487 | Test Acc:0.3546
[Epoch:740] Train Loss:0.3597 | Valid Acc:0.3507 | Test Acc:0.3572
[Epoch:750] Train Loss:0.3589 | Valid Acc:0.3500 | Test Acc:0.3572
[Epoch:760] Train Loss:0.3519 | Valid Acc:0.3493 | Test Acc:0.3566
[Epoch:770] Train Loss:0.3493 | Valid Acc:0.3493 | Test Acc:0.3559
[Epoch:780] Train Loss:0.3542 | Valid Acc:0.3500 | Test Acc:0.3566
[Epoch:790] Train Loss:0.3602 | Valid Acc:0.3500 | Test Acc:0.3566
Best results: epoch=59, valid loss=1.3526, valid acc=0.4007, test loss:1.3588,test acc=0.3993
{'activation': 'relu', 'dropout': 2, 'hidden_size': 128, 'learning_rate': 0.0032238964229651373, 'model': 'f2gnn', 'optimizer': 'adam', 'weight_decay': 0.00020436212483148185}
Current Hyperparameter: {'activation': 'relu', 'dropout': 2, 'hidden_size': 128, 'learning_rate': 0.0032238964229651373, 'model': 'f2gnn', 'optimizer': 'adam', 'weight_decay': 0.00020436212483148185}
[Epoch:0] Train Loss:1.6525 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:10] Train Loss:1.5705 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:20] Train Loss:1.5595 | Valid Acc:0.2697 | Test Acc:0.2651
[Epoch:30] Train Loss:1.4775 | Valid Acc:0.2928 | Test Acc:0.2914
[Epoch:40] Train Loss:1.3771 | Valid Acc:0.3743 | Test Acc:0.3901
[Epoch:50] Train Loss:1.3101 | Valid Acc:0.3974 | Test Acc:0.4125
[Epoch:60] Train Loss:1.2809 | Valid Acc:0.3914 | Test Acc:0.4072
[Epoch:70] Train Loss:1.2481 | Valid Acc:0.3849 | Test Acc:0.4039
[Epoch:80] Train Loss:1.2270 | Valid Acc:0.3697 | Test Acc:0.3638
[Epoch:90] Train Loss:1.1842 | Valid Acc:0.3717 | Test Acc:0.3842
[Epoch:100] Train Loss:1.1789 | Valid Acc:0.3553 | Test Acc:0.3539
[Epoch:110] Train Loss:1.1610 | Valid Acc:0.3572 | Test Acc:0.3401
[Epoch:120] Train Loss:1.1341 | Valid Acc:0.3849 | Test Acc:0.3783
[Epoch:130] Train Loss:1.1106 | Valid Acc:0.3809 | Test Acc:0.3678
[Epoch:140] Train Loss:1.0626 | Valid Acc:0.3684 | Test Acc:0.3586
[Epoch:150] Train Loss:1.0225 | Valid Acc:0.3776 | Test Acc:0.3599
[Epoch:160] Train Loss:0.9752 | Valid Acc:0.3678 | Test Acc:0.3461
[Epoch:170] Train Loss:0.9496 | Valid Acc:0.3605 | Test Acc:0.3441
[Epoch:180] Train Loss:0.9013 | Valid Acc:0.3507 | Test Acc:0.3421
[Epoch:190] Train Loss:0.8433 | Valid Acc:0.3638 | Test Acc:0.3500
[Epoch:200] Train Loss:0.7779 | Valid Acc:0.3632 | Test Acc:0.3349
[Epoch:210] Train Loss:0.7267 | Valid Acc:0.3724 | Test Acc:0.3474
[Epoch:220] Train Loss:0.6688 | Valid Acc:0.3711 | Test Acc:0.3434
[Epoch:230] Train Loss:0.6420 | Valid Acc:0.3546 | Test Acc:0.3421
[Epoch:240] Train Loss:0.5818 | Valid Acc:0.3507 | Test Acc:0.3375
[Epoch:250] Train Loss:0.5446 | Valid Acc:0.3474 | Test Acc:0.3447
[Epoch:260] Train Loss:0.5094 | Valid Acc:0.3559 | Test Acc:0.3474
[Epoch:270] Train Loss:0.4826 | Valid Acc:0.3487 | Test Acc:0.3520
[Epoch:280] Train Loss:0.4678 | Valid Acc:0.3507 | Test Acc:0.3605
[Epoch:290] Train Loss:0.4432 | Valid Acc:0.3441 | Test Acc:0.3421
[Epoch:300] Train Loss:0.4205 | Valid Acc:0.3368 | Test Acc:0.3520
[Epoch:310] Train Loss:0.4001 | Valid Acc:0.3428 | Test Acc:0.3533
[Epoch:320] Train Loss:0.3938 | Valid Acc:0.3401 | Test Acc:0.3539
[Epoch:330] Train Loss:0.3836 | Valid Acc:0.3375 | Test Acc:0.3474
[Epoch:340] Train Loss:0.3768 | Valid Acc:0.3414 | Test Acc:0.3493
[Epoch:350] Train Loss:0.3636 | Valid Acc:0.3395 | Test Acc:0.3520
[Epoch:360] Train Loss:0.3609 | Valid Acc:0.3434 | Test Acc:0.3572
[Epoch:370] Train Loss:0.3578 | Valid Acc:0.3447 | Test Acc:0.3461
[Epoch:380] Train Loss:0.3570 | Valid Acc:0.3408 | Test Acc:0.3513
[Epoch:390] Train Loss:0.3484 | Valid Acc:0.3375 | Test Acc:0.3454
[Epoch:400] Train Loss:0.3453 | Valid Acc:0.3382 | Test Acc:0.3395
[Epoch:410] Train Loss:0.3400 | Valid Acc:0.3329 | Test Acc:0.3474
[Epoch:420] Train Loss:0.3365 | Valid Acc:0.3362 | Test Acc:0.3493
[Epoch:430] Train Loss:0.3348 | Valid Acc:0.3401 | Test Acc:0.3507
[Epoch:440] Train Loss:0.3348 | Valid Acc:0.3441 | Test Acc:0.3586
[Epoch:450] Train Loss:0.3345 | Valid Acc:0.3493 | Test Acc:0.3553
[Epoch:460] Train Loss:0.3326 | Valid Acc:0.3395 | Test Acc:0.3572
[Epoch:470] Train Loss:0.3244 | Valid Acc:0.3355 | Test Acc:0.3546
[Epoch:480] Train Loss:0.3194 | Valid Acc:0.3428 | Test Acc:0.3592
[Epoch:490] Train Loss:0.3270 | Valid Acc:0.3349 | Test Acc:0.3539
[Epoch:500] Train Loss:0.3231 | Valid Acc:0.3408 | Test Acc:0.3520
[Epoch:510] Train Loss:0.3121 | Valid Acc:0.3434 | Test Acc:0.3520
[Epoch:520] Train Loss:0.3203 | Valid Acc:0.3421 | Test Acc:0.3526
[Epoch:530] Train Loss:0.3135 | Valid Acc:0.3395 | Test Acc:0.3513
[Epoch:540] Train Loss:0.3108 | Valid Acc:0.3414 | Test Acc:0.3533
[Epoch:550] Train Loss:0.3089 | Valid Acc:0.3388 | Test Acc:0.3493
[Epoch:560] Train Loss:0.3111 | Valid Acc:0.3447 | Test Acc:0.3520
[Epoch:570] Train Loss:0.3052 | Valid Acc:0.3434 | Test Acc:0.3526
[Epoch:580] Train Loss:0.2986 | Valid Acc:0.3375 | Test Acc:0.3533
[Epoch:590] Train Loss:0.3069 | Valid Acc:0.3395 | Test Acc:0.3520
[Epoch:600] Train Loss:0.3121 | Valid Acc:0.3342 | Test Acc:0.3533
[Epoch:610] Train Loss:0.3078 | Valid Acc:0.3349 | Test Acc:0.3513
[Epoch:620] Train Loss:0.3040 | Valid Acc:0.3395 | Test Acc:0.3526
[Epoch:630] Train Loss:0.3026 | Valid Acc:0.3401 | Test Acc:0.3520
[Epoch:640] Train Loss:0.3019 | Valid Acc:0.3382 | Test Acc:0.3539
[Epoch:650] Train Loss:0.3084 | Valid Acc:0.3362 | Test Acc:0.3526
[Epoch:660] Train Loss:0.2913 | Valid Acc:0.3447 | Test Acc:0.3493
[Epoch:670] Train Loss:0.3028 | Valid Acc:0.3414 | Test Acc:0.3507
[Epoch:680] Train Loss:0.2986 | Valid Acc:0.3349 | Test Acc:0.3474
[Epoch:690] Train Loss:0.3029 | Valid Acc:0.3362 | Test Acc:0.3513
[Epoch:700] Train Loss:0.3012 | Valid Acc:0.3375 | Test Acc:0.3520
[Epoch:710] Train Loss:0.2987 | Valid Acc:0.3355 | Test Acc:0.3520
[Epoch:720] Train Loss:0.2999 | Valid Acc:0.3362 | Test Acc:0.3520
[Epoch:730] Train Loss:0.2933 | Valid Acc:0.3375 | Test Acc:0.3533
[Epoch:740] Train Loss:0.3015 | Valid Acc:0.3362 | Test Acc:0.3520
[Epoch:750] Train Loss:0.2963 | Valid Acc:0.3355 | Test Acc:0.3526
[Epoch:760] Train Loss:0.2995 | Valid Acc:0.3355 | Test Acc:0.3487
[Epoch:770] Train Loss:0.3012 | Valid Acc:0.3362 | Test Acc:0.3500
[Epoch:780] Train Loss:0.3012 | Valid Acc:0.3355 | Test Acc:0.3507
[Epoch:790] Train Loss:0.3031 | Valid Acc:0.3349 | Test Acc:0.3513
Best results: epoch=57, valid loss=1.3409, valid acc=0.4033, test loss:1.3470,test acc=0.4020
{'activation': 'relu', 'dropout': 2, 'hidden_size': 128, 'learning_rate': 0.0010482024054932146, 'model': 'f2gnn', 'optimizer': 'adam', 'weight_decay': 0.00045554024437451226}
Current Hyperparameter: {'activation': 'relu', 'dropout': 2, 'hidden_size': 128, 'learning_rate': 0.0010482024054932146, 'model': 'f2gnn', 'optimizer': 'adam', 'weight_decay': 0.00045554024437451226}
[Epoch:0] Train Loss:1.6525 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:10] Train Loss:1.5757 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:20] Train Loss:1.5738 | Valid Acc:0.2592 | Test Acc:0.2592
[Epoch:30] Train Loss:1.5631 | Valid Acc:0.3151 | Test Acc:0.3296
[Epoch:40] Train Loss:1.5262 | Valid Acc:0.2954 | Test Acc:0.2954
[Epoch:50] Train Loss:1.4679 | Valid Acc:0.3441 | Test Acc:0.3546
[Epoch:60] Train Loss:1.4153 | Valid Acc:0.3678 | Test Acc:0.3612
[Epoch:70] Train Loss:1.3733 | Valid Acc:0.3868 | Test Acc:0.4000
[Epoch:80] Train Loss:1.3425 | Valid Acc:0.3934 | Test Acc:0.3954
[Epoch:90] Train Loss:1.3194 | Valid Acc:0.3849 | Test Acc:0.3888
[Epoch:100] Train Loss:1.2932 | Valid Acc:0.4007 | Test Acc:0.3934
[Epoch:110] Train Loss:1.2781 | Valid Acc:0.3980 | Test Acc:0.3862
[Epoch:120] Train Loss:1.2765 | Valid Acc:0.3947 | Test Acc:0.3954
[Epoch:130] Train Loss:1.2664 | Valid Acc:0.3816 | Test Acc:0.3632
[Epoch:140] Train Loss:1.2483 | Valid Acc:0.3882 | Test Acc:0.3868
[Epoch:150] Train Loss:1.2222 | Valid Acc:0.3783 | Test Acc:0.3895
[Epoch:160] Train Loss:1.2142 | Valid Acc:0.3783 | Test Acc:0.3849
[Epoch:170] Train Loss:1.1991 | Valid Acc:0.3757 | Test Acc:0.3743
[Epoch:180] Train Loss:1.1867 | Valid Acc:0.3730 | Test Acc:0.3789
[Epoch:190] Train Loss:1.1723 | Valid Acc:0.3757 | Test Acc:0.3697
[Epoch:200] Train Loss:1.1570 | Valid Acc:0.3711 | Test Acc:0.3750
[Epoch:210] Train Loss:1.1522 | Valid Acc:0.3822 | Test Acc:0.3678
[Epoch:220] Train Loss:1.1446 | Valid Acc:0.3737 | Test Acc:0.3763
[Epoch:230] Train Loss:1.1261 | Valid Acc:0.3750 | Test Acc:0.3632
[Epoch:240] Train Loss:1.1137 | Valid Acc:0.3796 | Test Acc:0.3618
[Epoch:250] Train Loss:1.1073 | Valid Acc:0.3757 | Test Acc:0.3632
[Epoch:260] Train Loss:1.0973 | Valid Acc:0.3809 | Test Acc:0.3684
[Epoch:270] Train Loss:1.0822 | Valid Acc:0.3763 | Test Acc:0.3750
[Epoch:280] Train Loss:1.0574 | Valid Acc:0.3743 | Test Acc:0.3743
[Epoch:290] Train Loss:1.0461 | Valid Acc:0.3757 | Test Acc:0.3750
[Epoch:300] Train Loss:1.0258 | Valid Acc:0.3770 | Test Acc:0.3717
[Epoch:310] Train Loss:1.0096 | Valid Acc:0.3711 | Test Acc:0.3697
[Epoch:320] Train Loss:1.0062 | Valid Acc:0.3704 | Test Acc:0.3671
[Epoch:330] Train Loss:0.9812 | Valid Acc:0.3704 | Test Acc:0.3671
[Epoch:340] Train Loss:0.9537 | Valid Acc:0.3664 | Test Acc:0.3678
[Epoch:350] Train Loss:0.9482 | Valid Acc:0.3618 | Test Acc:0.3645
[Epoch:360] Train Loss:0.9416 | Valid Acc:0.3592 | Test Acc:0.3618
[Epoch:370] Train Loss:0.9133 | Valid Acc:0.3572 | Test Acc:0.3664
[Epoch:380] Train Loss:0.8917 | Valid Acc:0.3586 | Test Acc:0.3599
[Epoch:390] Train Loss:0.8837 | Valid Acc:0.3579 | Test Acc:0.3586
[Epoch:400] Train Loss:0.8525 | Valid Acc:0.3572 | Test Acc:0.3539
[Epoch:410] Train Loss:0.8521 | Valid Acc:0.3572 | Test Acc:0.3526
[Epoch:420] Train Loss:0.8195 | Valid Acc:0.3599 | Test Acc:0.3507
[Epoch:430] Train Loss:0.8108 | Valid Acc:0.3572 | Test Acc:0.3539
[Epoch:440] Train Loss:0.7961 | Valid Acc:0.3678 | Test Acc:0.3572
[Epoch:450] Train Loss:0.7820 | Valid Acc:0.3658 | Test Acc:0.3592
[Epoch:460] Train Loss:0.7673 | Valid Acc:0.3664 | Test Acc:0.3539
[Epoch:470] Train Loss:0.7596 | Valid Acc:0.3658 | Test Acc:0.3579
[Epoch:480] Train Loss:0.7451 | Valid Acc:0.3691 | Test Acc:0.3572
[Epoch:490] Train Loss:0.7306 | Valid Acc:0.3651 | Test Acc:0.3520
[Epoch:500] Train Loss:0.7192 | Valid Acc:0.3632 | Test Acc:0.3507
[Epoch:510] Train Loss:0.7142 | Valid Acc:0.3599 | Test Acc:0.3526
[Epoch:520] Train Loss:0.6994 | Valid Acc:0.3618 | Test Acc:0.3553
[Epoch:530] Train Loss:0.6889 | Valid Acc:0.3599 | Test Acc:0.3526
[Epoch:540] Train Loss:0.6889 | Valid Acc:0.3553 | Test Acc:0.3493
[Epoch:550] Train Loss:0.6789 | Valid Acc:0.3579 | Test Acc:0.3533
[Epoch:560] Train Loss:0.6695 | Valid Acc:0.3592 | Test Acc:0.3520
[Epoch:570] Train Loss:0.6613 | Valid Acc:0.3632 | Test Acc:0.3500
[Epoch:580] Train Loss:0.6629 | Valid Acc:0.3605 | Test Acc:0.3520
[Epoch:590] Train Loss:0.6555 | Valid Acc:0.3592 | Test Acc:0.3513
[Epoch:600] Train Loss:0.6479 | Valid Acc:0.3546 | Test Acc:0.3474
[Epoch:610] Train Loss:0.6530 | Valid Acc:0.3553 | Test Acc:0.3553
[Epoch:620] Train Loss:0.6489 | Valid Acc:0.3566 | Test Acc:0.3533
[Epoch:630] Train Loss:0.6379 | Valid Acc:0.3539 | Test Acc:0.3493
[Epoch:640] Train Loss:0.6339 | Valid Acc:0.3572 | Test Acc:0.3507
[Epoch:650] Train Loss:0.6287 | Valid Acc:0.3546 | Test Acc:0.3513
[Epoch:660] Train Loss:0.6302 | Valid Acc:0.3553 | Test Acc:0.3520
[Epoch:670] Train Loss:0.6323 | Valid Acc:0.3559 | Test Acc:0.3526
[Epoch:680] Train Loss:0.6316 | Valid Acc:0.3566 | Test Acc:0.3533
[Epoch:690] Train Loss:0.6135 | Valid Acc:0.3526 | Test Acc:0.3520
[Epoch:700] Train Loss:0.6234 | Valid Acc:0.3553 | Test Acc:0.3500
[Epoch:710] Train Loss:0.6175 | Valid Acc:0.3553 | Test Acc:0.3526
[Epoch:720] Train Loss:0.6110 | Valid Acc:0.3546 | Test Acc:0.3474
[Epoch:730] Train Loss:0.6198 | Valid Acc:0.3553 | Test Acc:0.3500
[Epoch:740] Train Loss:0.6181 | Valid Acc:0.3559 | Test Acc:0.3487
[Epoch:750] Train Loss:0.6198 | Valid Acc:0.3566 | Test Acc:0.3487
[Epoch:760] Train Loss:0.6198 | Valid Acc:0.3559 | Test Acc:0.3507
[Epoch:770] Train Loss:0.6141 | Valid Acc:0.3546 | Test Acc:0.3500
[Epoch:780] Train Loss:0.6196 | Valid Acc:0.3559 | Test Acc:0.3493
[Epoch:790] Train Loss:0.6119 | Valid Acc:0.3559 | Test Acc:0.3487
Best results: epoch=92, valid loss=1.3397, valid acc=0.4072, test loss:1.3474,test acc=0.4020
{'activation': 'relu', 'dropout': 2, 'hidden_size': 32, 'learning_rate': 0.004733693197251865, 'model': 'f2gnn', 'optimizer': 'adam', 'weight_decay': 0.00011136439958241829}
Current Hyperparameter: {'activation': 'relu', 'dropout': 2, 'hidden_size': 32, 'learning_rate': 0.004733693197251865, 'model': 'f2gnn', 'optimizer': 'adam', 'weight_decay': 0.00011136439958241829}
[Epoch:0] Train Loss:1.6025 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:10] Train Loss:1.5652 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:20] Train Loss:1.5004 | Valid Acc:0.3033 | Test Acc:0.3112
[Epoch:30] Train Loss:1.3890 | Valid Acc:0.3539 | Test Acc:0.3526
[Epoch:40] Train Loss:1.3159 | Valid Acc:0.4020 | Test Acc:0.3895
[Epoch:50] Train Loss:1.2757 | Valid Acc:0.4105 | Test Acc:0.3974
[Epoch:60] Train Loss:1.2412 | Valid Acc:0.3921 | Test Acc:0.4020
[Epoch:70] Train Loss:1.1983 | Valid Acc:0.3855 | Test Acc:0.3901
[Epoch:80] Train Loss:1.1647 | Valid Acc:0.3816 | Test Acc:0.3862
[Epoch:90] Train Loss:1.1314 | Valid Acc:0.3789 | Test Acc:0.3868
[Epoch:100] Train Loss:1.0985 | Valid Acc:0.3664 | Test Acc:0.3783
[Epoch:110] Train Loss:1.0466 | Valid Acc:0.3770 | Test Acc:0.3711
[Epoch:120] Train Loss:1.0020 | Valid Acc:0.3651 | Test Acc:0.3618
[Epoch:130] Train Loss:0.9483 | Valid Acc:0.3717 | Test Acc:0.3566
[Epoch:140] Train Loss:0.9324 | Valid Acc:0.3724 | Test Acc:0.3691
[Epoch:150] Train Loss:0.8566 | Valid Acc:0.3711 | Test Acc:0.3618
[Epoch:160] Train Loss:0.8145 | Valid Acc:0.3632 | Test Acc:0.3579
[Epoch:170] Train Loss:0.7801 | Valid Acc:0.3546 | Test Acc:0.3533
[Epoch:180] Train Loss:0.7463 | Valid Acc:0.3612 | Test Acc:0.3520
[Epoch:190] Train Loss:0.7141 | Valid Acc:0.3493 | Test Acc:0.3493
[Epoch:200] Train Loss:0.6945 | Valid Acc:0.3526 | Test Acc:0.3487
[Epoch:210] Train Loss:0.6711 | Valid Acc:0.3408 | Test Acc:0.3507
[Epoch:220] Train Loss:0.6275 | Valid Acc:0.3408 | Test Acc:0.3612
[Epoch:230] Train Loss:0.6169 | Valid Acc:0.3408 | Test Acc:0.3507
[Epoch:240] Train Loss:0.5945 | Valid Acc:0.3289 | Test Acc:0.3474
[Epoch:250] Train Loss:0.5926 | Valid Acc:0.3336 | Test Acc:0.3507
[Epoch:260] Train Loss:0.5595 | Valid Acc:0.3322 | Test Acc:0.3461
[Epoch:270] Train Loss:0.5479 | Valid Acc:0.3362 | Test Acc:0.3500
[Epoch:280] Train Loss:0.5294 | Valid Acc:0.3401 | Test Acc:0.3454
[Epoch:290] Train Loss:0.5365 | Valid Acc:0.3316 | Test Acc:0.3539
[Epoch:300] Train Loss:0.5310 | Valid Acc:0.3342 | Test Acc:0.3480
[Epoch:310] Train Loss:0.5163 | Valid Acc:0.3289 | Test Acc:0.3487
[Epoch:320] Train Loss:0.5108 | Valid Acc:0.3296 | Test Acc:0.3500
[Epoch:330] Train Loss:0.5087 | Valid Acc:0.3349 | Test Acc:0.3539
[Epoch:340] Train Loss:0.4990 | Valid Acc:0.3316 | Test Acc:0.3461
[Epoch:350] Train Loss:0.4867 | Valid Acc:0.3309 | Test Acc:0.3480
[Epoch:360] Train Loss:0.4923 | Valid Acc:0.3230 | Test Acc:0.3520
[Epoch:370] Train Loss:0.4740 | Valid Acc:0.3309 | Test Acc:0.3539
[Epoch:380] Train Loss:0.4917 | Valid Acc:0.3375 | Test Acc:0.3539
[Epoch:390] Train Loss:0.4691 | Valid Acc:0.3336 | Test Acc:0.3520
[Epoch:400] Train Loss:0.4800 | Valid Acc:0.3283 | Test Acc:0.3507
[Epoch:410] Train Loss:0.4657 | Valid Acc:0.3309 | Test Acc:0.3520
[Epoch:420] Train Loss:0.4590 | Valid Acc:0.3303 | Test Acc:0.3513
[Epoch:430] Train Loss:0.4588 | Valid Acc:0.3329 | Test Acc:0.3421
[Epoch:440] Train Loss:0.4615 | Valid Acc:0.3336 | Test Acc:0.3493
[Epoch:450] Train Loss:0.4518 | Valid Acc:0.3316 | Test Acc:0.3513
[Epoch:460] Train Loss:0.4550 | Valid Acc:0.3283 | Test Acc:0.3526
[Epoch:470] Train Loss:0.4555 | Valid Acc:0.3283 | Test Acc:0.3467
[Epoch:480] Train Loss:0.4545 | Valid Acc:0.3283 | Test Acc:0.3507
[Epoch:490] Train Loss:0.4461 | Valid Acc:0.3316 | Test Acc:0.3526
[Epoch:500] Train Loss:0.4426 | Valid Acc:0.3382 | Test Acc:0.3493
[Epoch:510] Train Loss:0.4444 | Valid Acc:0.3349 | Test Acc:0.3520
[Epoch:520] Train Loss:0.4289 | Valid Acc:0.3355 | Test Acc:0.3592
[Epoch:530] Train Loss:0.4396 | Valid Acc:0.3375 | Test Acc:0.3592
[Epoch:540] Train Loss:0.4527 | Valid Acc:0.3336 | Test Acc:0.3546
[Epoch:550] Train Loss:0.4316 | Valid Acc:0.3342 | Test Acc:0.3513
[Epoch:560] Train Loss:0.4422 | Valid Acc:0.3388 | Test Acc:0.3526
[Epoch:570] Train Loss:0.4337 | Valid Acc:0.3408 | Test Acc:0.3526
[Epoch:580] Train Loss:0.4482 | Valid Acc:0.3362 | Test Acc:0.3533
[Epoch:590] Train Loss:0.4318 | Valid Acc:0.3362 | Test Acc:0.3566
[Epoch:600] Train Loss:0.4274 | Valid Acc:0.3375 | Test Acc:0.3533
[Epoch:610] Train Loss:0.4392 | Valid Acc:0.3382 | Test Acc:0.3546
[Epoch:620] Train Loss:0.4309 | Valid Acc:0.3362 | Test Acc:0.3553
[Epoch:630] Train Loss:0.4359 | Valid Acc:0.3382 | Test Acc:0.3520
[Epoch:640] Train Loss:0.4387 | Valid Acc:0.3368 | Test Acc:0.3513
[Epoch:650] Train Loss:0.4295 | Valid Acc:0.3362 | Test Acc:0.3467
[Epoch:660] Train Loss:0.4270 | Valid Acc:0.3349 | Test Acc:0.3546
[Epoch:670] Train Loss:0.4337 | Valid Acc:0.3349 | Test Acc:0.3507
[Epoch:680] Train Loss:0.4258 | Valid Acc:0.3362 | Test Acc:0.3526
[Epoch:690] Train Loss:0.4198 | Valid Acc:0.3395 | Test Acc:0.3507
[Epoch:700] Train Loss:0.4177 | Valid Acc:0.3355 | Test Acc:0.3546
[Epoch:710] Train Loss:0.4189 | Valid Acc:0.3375 | Test Acc:0.3507
[Epoch:720] Train Loss:0.4249 | Valid Acc:0.3382 | Test Acc:0.3507
[Epoch:730] Train Loss:0.4238 | Valid Acc:0.3375 | Test Acc:0.3533
[Epoch:740] Train Loss:0.4270 | Valid Acc:0.3342 | Test Acc:0.3533
[Epoch:750] Train Loss:0.4232 | Valid Acc:0.3362 | Test Acc:0.3500
[Epoch:760] Train Loss:0.4248 | Valid Acc:0.3362 | Test Acc:0.3513
[Epoch:770] Train Loss:0.4147 | Valid Acc:0.3362 | Test Acc:0.3533
[Epoch:780] Train Loss:0.4237 | Valid Acc:0.3375 | Test Acc:0.3539
[Epoch:790] Train Loss:0.4250 | Valid Acc:0.3375 | Test Acc:0.3539
Best results: epoch=50, valid loss=1.3430, valid acc=0.4105, test loss:1.3515,test acc=0.3974
{'activation': 'relu', 'dropout': 1, 'hidden_size': 256, 'learning_rate': 0.0017614318135007589, 'model': 'f2gnn', 'optimizer': 'adam', 'weight_decay': 0.0003240475309621991}
Current Hyperparameter: {'activation': 'relu', 'dropout': 1, 'hidden_size': 256, 'learning_rate': 0.0017614318135007589, 'model': 'f2gnn', 'optimizer': 'adam', 'weight_decay': 0.0003240475309621991}
[Epoch:0] Train Loss:1.6319 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:10] Train Loss:1.5723 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:20] Train Loss:1.5579 | Valid Acc:0.2625 | Test Acc:0.2599
[Epoch:30] Train Loss:1.4765 | Valid Acc:0.2638 | Test Acc:0.2605
[Epoch:40] Train Loss:1.3799 | Valid Acc:0.3191 | Test Acc:0.3289
[Epoch:50] Train Loss:1.3254 | Valid Acc:0.3539 | Test Acc:0.3743
[Epoch:60] Train Loss:1.2885 | Valid Acc:0.3836 | Test Acc:0.3967
[Epoch:70] Train Loss:1.2736 | Valid Acc:0.3980 | Test Acc:0.3757
[Epoch:80] Train Loss:1.2430 | Valid Acc:0.3816 | Test Acc:0.3822
[Epoch:90] Train Loss:1.2186 | Valid Acc:0.3849 | Test Acc:0.3862
[Epoch:100] Train Loss:1.1986 | Valid Acc:0.3763 | Test Acc:0.3796
[Epoch:110] Train Loss:1.1650 | Valid Acc:0.3717 | Test Acc:0.3796
[Epoch:120] Train Loss:1.1477 | Valid Acc:0.3658 | Test Acc:0.3618
[Epoch:130] Train Loss:1.1502 | Valid Acc:0.3645 | Test Acc:0.3796
[Epoch:140] Train Loss:1.1780 | Valid Acc:0.3559 | Test Acc:0.3737
[Epoch:150] Train Loss:1.1181 | Valid Acc:0.3730 | Test Acc:0.3750
[Epoch:160] Train Loss:1.0900 | Valid Acc:0.3750 | Test Acc:0.3763
[Epoch:170] Train Loss:1.0521 | Valid Acc:0.3691 | Test Acc:0.3757
[Epoch:180] Train Loss:1.0318 | Valid Acc:0.3829 | Test Acc:0.3816
[Epoch:190] Train Loss:1.0229 | Valid Acc:0.3717 | Test Acc:0.3671
[Epoch:200] Train Loss:1.0276 | Valid Acc:0.3697 | Test Acc:0.3789
[Epoch:210] Train Loss:0.9907 | Valid Acc:0.3789 | Test Acc:0.3822
[Epoch:220] Train Loss:0.9729 | Valid Acc:0.3704 | Test Acc:0.3717
[Epoch:230] Train Loss:0.9566 | Valid Acc:0.3664 | Test Acc:0.3684
[Epoch:240] Train Loss:0.9301 | Valid Acc:0.3671 | Test Acc:0.3586
[Epoch:250] Train Loss:0.9181 | Valid Acc:0.3691 | Test Acc:0.3612
[Epoch:260] Train Loss:0.8994 | Valid Acc:0.3671 | Test Acc:0.3546
[Epoch:270] Train Loss:0.8736 | Valid Acc:0.3664 | Test Acc:0.3625
[Epoch:280] Train Loss:0.8600 | Valid Acc:0.3592 | Test Acc:0.3632
[Epoch:290] Train Loss:0.8593 | Valid Acc:0.3586 | Test Acc:0.3599
[Epoch:300] Train Loss:0.8131 | Valid Acc:0.3579 | Test Acc:0.3625
[Epoch:310] Train Loss:0.7957 | Valid Acc:0.3533 | Test Acc:0.3605
[Epoch:320] Train Loss:0.7607 | Valid Acc:0.3599 | Test Acc:0.3664
[Epoch:330] Train Loss:0.7531 | Valid Acc:0.3553 | Test Acc:0.3730
[Epoch:340] Train Loss:0.7233 | Valid Acc:0.3533 | Test Acc:0.3526
[Epoch:350] Train Loss:0.7623 | Valid Acc:0.3566 | Test Acc:0.3625
[Epoch:360] Train Loss:0.6900 | Valid Acc:0.3592 | Test Acc:0.3691
[Epoch:370] Train Loss:0.6458 | Valid Acc:0.3559 | Test Acc:0.3645
[Epoch:380] Train Loss:0.6313 | Valid Acc:0.3546 | Test Acc:0.3566
[Epoch:390] Train Loss:0.6034 | Valid Acc:0.3539 | Test Acc:0.3579
[Epoch:400] Train Loss:0.5893 | Valid Acc:0.3513 | Test Acc:0.3638
[Epoch:410] Train Loss:0.5612 | Valid Acc:0.3599 | Test Acc:0.3566
[Epoch:420] Train Loss:0.5471 | Valid Acc:0.3605 | Test Acc:0.3553
[Epoch:430] Train Loss:0.5217 | Valid Acc:0.3612 | Test Acc:0.3572
[Epoch:440] Train Loss:0.5143 | Valid Acc:0.3579 | Test Acc:0.3572
[Epoch:450] Train Loss:0.4935 | Valid Acc:0.3546 | Test Acc:0.3592
[Epoch:460] Train Loss:0.4927 | Valid Acc:0.3586 | Test Acc:0.3526
[Epoch:470] Train Loss:0.4687 | Valid Acc:0.3559 | Test Acc:0.3539
[Epoch:480] Train Loss:0.4543 | Valid Acc:0.3533 | Test Acc:0.3559
[Epoch:490] Train Loss:0.4494 | Valid Acc:0.3612 | Test Acc:0.3526
[Epoch:500] Train Loss:0.4400 | Valid Acc:0.3559 | Test Acc:0.3559
[Epoch:510] Train Loss:0.4381 | Valid Acc:0.3572 | Test Acc:0.3454
[Epoch:520] Train Loss:0.4226 | Valid Acc:0.3586 | Test Acc:0.3487
[Epoch:530] Train Loss:0.4168 | Valid Acc:0.3612 | Test Acc:0.3539
[Epoch:540] Train Loss:0.4132 | Valid Acc:0.3638 | Test Acc:0.3500
[Epoch:550] Train Loss:0.4025 | Valid Acc:0.3625 | Test Acc:0.3474
[Epoch:560] Train Loss:0.4005 | Valid Acc:0.3618 | Test Acc:0.3441
[Epoch:570] Train Loss:0.4006 | Valid Acc:0.3553 | Test Acc:0.3474
[Epoch:580] Train Loss:0.3964 | Valid Acc:0.3579 | Test Acc:0.3493
[Epoch:590] Train Loss:0.3911 | Valid Acc:0.3572 | Test Acc:0.3480
[Epoch:600] Train Loss:0.3811 | Valid Acc:0.3586 | Test Acc:0.3493
[Epoch:610] Train Loss:0.3827 | Valid Acc:0.3605 | Test Acc:0.3474
[Epoch:620] Train Loss:0.3823 | Valid Acc:0.3592 | Test Acc:0.3454
[Epoch:630] Train Loss:0.3794 | Valid Acc:0.3566 | Test Acc:0.3487
[Epoch:640] Train Loss:0.3764 | Valid Acc:0.3572 | Test Acc:0.3500
[Epoch:650] Train Loss:0.3741 | Valid Acc:0.3546 | Test Acc:0.3467
[Epoch:660] Train Loss:0.3801 | Valid Acc:0.3599 | Test Acc:0.3447
[Epoch:670] Train Loss:0.3773 | Valid Acc:0.3572 | Test Acc:0.3447
[Epoch:680] Train Loss:0.3686 | Valid Acc:0.3566 | Test Acc:0.3487
[Epoch:690] Train Loss:0.3713 | Valid Acc:0.3566 | Test Acc:0.3467
[Epoch:700] Train Loss:0.3692 | Valid Acc:0.3586 | Test Acc:0.3467
[Epoch:710] Train Loss:0.3668 | Valid Acc:0.3592 | Test Acc:0.3467
[Epoch:720] Train Loss:0.3686 | Valid Acc:0.3566 | Test Acc:0.3461
[Epoch:730] Train Loss:0.3650 | Valid Acc:0.3592 | Test Acc:0.3474
[Epoch:740] Train Loss:0.3598 | Valid Acc:0.3572 | Test Acc:0.3454
[Epoch:750] Train Loss:0.3708 | Valid Acc:0.3579 | Test Acc:0.3467
[Epoch:760] Train Loss:0.3655 | Valid Acc:0.3579 | Test Acc:0.3467
[Epoch:770] Train Loss:0.3677 | Valid Acc:0.3592 | Test Acc:0.3461
[Epoch:780] Train Loss:0.3647 | Valid Acc:0.3592 | Test Acc:0.3467
[Epoch:790] Train Loss:0.3682 | Valid Acc:0.3592 | Test Acc:0.3474
Best results: epoch=59, valid loss=1.3332, valid acc=0.4086, test loss:1.3368,test acc=0.4039
{'activation': 'elu', 'dropout': 2, 'hidden_size': 64, 'learning_rate': 0.003236291311531272, 'model': 'f2gnn', 'optimizer': 'adagrad', 'weight_decay': 0.00018368081378528845}
Current Hyperparameter: {'activation': 'elu', 'dropout': 2, 'hidden_size': 64, 'learning_rate': 0.003236291311531272, 'model': 'f2gnn', 'optimizer': 'adagrad', 'weight_decay': 0.00018368081378528845}
[Epoch:0] Train Loss:1.6028 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:10] Train Loss:1.5725 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:20] Train Loss:1.5691 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:30] Train Loss:1.5583 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:40] Train Loss:1.5376 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:50] Train Loss:1.5228 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:60] Train Loss:1.5000 | Valid Acc:0.2651 | Test Acc:0.2618
[Epoch:70] Train Loss:1.4723 | Valid Acc:0.2770 | Test Acc:0.2829
[Epoch:80] Train Loss:1.4457 | Valid Acc:0.3000 | Test Acc:0.3020
[Epoch:90] Train Loss:1.4247 | Valid Acc:0.3151 | Test Acc:0.3184
[Epoch:100] Train Loss:1.4010 | Valid Acc:0.3270 | Test Acc:0.3408
[Epoch:110] Train Loss:1.3912 | Valid Acc:0.3382 | Test Acc:0.3572
[Epoch:120] Train Loss:1.3700 | Valid Acc:0.3480 | Test Acc:0.3645
[Epoch:130] Train Loss:1.3629 | Valid Acc:0.3467 | Test Acc:0.3651
[Epoch:140] Train Loss:1.3430 | Valid Acc:0.3572 | Test Acc:0.3796
[Epoch:150] Train Loss:1.3353 | Valid Acc:0.3691 | Test Acc:0.3855
[Epoch:160] Train Loss:1.3281 | Valid Acc:0.3664 | Test Acc:0.3882
[Epoch:170] Train Loss:1.3168 | Valid Acc:0.3717 | Test Acc:0.3974
[Epoch:180] Train Loss:1.3069 | Valid Acc:0.3803 | Test Acc:0.4007
[Epoch:190] Train Loss:1.3010 | Valid Acc:0.3776 | Test Acc:0.3934
[Epoch:200] Train Loss:1.2941 | Valid Acc:0.3868 | Test Acc:0.3967
[Epoch:210] Train Loss:1.2905 | Valid Acc:0.3842 | Test Acc:0.3967
[Epoch:220] Train Loss:1.2815 | Valid Acc:0.3901 | Test Acc:0.3980
[Epoch:230] Train Loss:1.2731 | Valid Acc:0.3954 | Test Acc:0.3987
[Epoch:240] Train Loss:1.2699 | Valid Acc:0.3987 | Test Acc:0.3993
[Epoch:250] Train Loss:1.2666 | Valid Acc:0.3941 | Test Acc:0.3908
[Epoch:260] Train Loss:1.2719 | Valid Acc:0.3901 | Test Acc:0.3875
[Epoch:270] Train Loss:1.2591 | Valid Acc:0.4020 | Test Acc:0.4000
[Epoch:280] Train Loss:1.2500 | Valid Acc:0.4020 | Test Acc:0.3980
[Epoch:290] Train Loss:1.2543 | Valid Acc:0.3987 | Test Acc:0.4007
[Epoch:300] Train Loss:1.2403 | Valid Acc:0.4007 | Test Acc:0.4007
[Epoch:310] Train Loss:1.2386 | Valid Acc:0.4000 | Test Acc:0.4013
[Epoch:320] Train Loss:1.2307 | Valid Acc:0.3961 | Test Acc:0.3974
[Epoch:330] Train Loss:1.2324 | Valid Acc:0.3961 | Test Acc:0.4039
[Epoch:340] Train Loss:1.2324 | Valid Acc:0.3974 | Test Acc:0.3987
[Epoch:350] Train Loss:1.2313 | Valid Acc:0.3921 | Test Acc:0.4000
[Epoch:360] Train Loss:1.2322 | Valid Acc:0.3954 | Test Acc:0.4000
[Epoch:370] Train Loss:1.2219 | Valid Acc:0.3954 | Test Acc:0.3993
[Epoch:380] Train Loss:1.2276 | Valid Acc:0.3941 | Test Acc:0.4033
[Epoch:390] Train Loss:1.2171 | Valid Acc:0.3934 | Test Acc:0.4000
[Epoch:400] Train Loss:1.2187 | Valid Acc:0.3908 | Test Acc:0.4033
[Epoch:410] Train Loss:1.2183 | Valid Acc:0.3888 | Test Acc:0.4072
[Epoch:420] Train Loss:1.2160 | Valid Acc:0.3888 | Test Acc:0.4020
[Epoch:430] Train Loss:1.2090 | Valid Acc:0.3855 | Test Acc:0.4059
[Epoch:440] Train Loss:1.2156 | Valid Acc:0.3862 | Test Acc:0.4000
[Epoch:450] Train Loss:1.2131 | Valid Acc:0.3875 | Test Acc:0.4013
[Epoch:460] Train Loss:1.2057 | Valid Acc:0.3862 | Test Acc:0.4039
[Epoch:470] Train Loss:1.2049 | Valid Acc:0.3862 | Test Acc:0.4020
[Epoch:480] Train Loss:1.2025 | Valid Acc:0.3868 | Test Acc:0.4007
[Epoch:490] Train Loss:1.2049 | Valid Acc:0.3849 | Test Acc:0.3974
[Epoch:500] Train Loss:1.2067 | Valid Acc:0.3882 | Test Acc:0.3974
[Epoch:510] Train Loss:1.2031 | Valid Acc:0.3855 | Test Acc:0.3987
[Epoch:520] Train Loss:1.2001 | Valid Acc:0.3882 | Test Acc:0.3961
[Epoch:530] Train Loss:1.2059 | Valid Acc:0.3895 | Test Acc:0.3974
[Epoch:540] Train Loss:1.2036 | Valid Acc:0.3901 | Test Acc:0.3947
[Epoch:550] Train Loss:1.2010 | Valid Acc:0.3895 | Test Acc:0.3954
[Epoch:560] Train Loss:1.1989 | Valid Acc:0.3901 | Test Acc:0.3928
[Epoch:570] Train Loss:1.2034 | Valid Acc:0.3908 | Test Acc:0.3961
[Epoch:580] Train Loss:1.2001 | Valid Acc:0.3908 | Test Acc:0.3947
[Epoch:590] Train Loss:1.1951 | Valid Acc:0.3901 | Test Acc:0.3961
[Epoch:600] Train Loss:1.1947 | Valid Acc:0.3908 | Test Acc:0.3941
[Epoch:610] Train Loss:1.1969 | Valid Acc:0.3914 | Test Acc:0.3954
[Epoch:620] Train Loss:1.1978 | Valid Acc:0.3901 | Test Acc:0.3954
[Epoch:630] Train Loss:1.1926 | Valid Acc:0.3908 | Test Acc:0.3961
[Epoch:640] Train Loss:1.2027 | Valid Acc:0.3921 | Test Acc:0.3961
[Epoch:650] Train Loss:1.1937 | Valid Acc:0.3921 | Test Acc:0.3961
[Epoch:660] Train Loss:1.1923 | Valid Acc:0.3921 | Test Acc:0.3954
[Epoch:670] Train Loss:1.1956 | Valid Acc:0.3914 | Test Acc:0.3954
[Epoch:680] Train Loss:1.1950 | Valid Acc:0.3908 | Test Acc:0.3954
[Epoch:690] Train Loss:1.1935 | Valid Acc:0.3921 | Test Acc:0.3954
[Epoch:700] Train Loss:1.1953 | Valid Acc:0.3921 | Test Acc:0.3954
[Epoch:710] Train Loss:1.1931 | Valid Acc:0.3908 | Test Acc:0.3947
[Epoch:720] Train Loss:1.1951 | Valid Acc:0.3908 | Test Acc:0.3947
[Epoch:730] Train Loss:1.1969 | Valid Acc:0.3908 | Test Acc:0.3954
[Epoch:740] Train Loss:1.1954 | Valid Acc:0.3901 | Test Acc:0.3954
[Epoch:750] Train Loss:1.1931 | Valid Acc:0.3908 | Test Acc:0.3947
[Epoch:760] Train Loss:1.1928 | Valid Acc:0.3901 | Test Acc:0.3947
[Epoch:770] Train Loss:1.1962 | Valid Acc:0.3908 | Test Acc:0.3947
[Epoch:780] Train Loss:1.1965 | Valid Acc:0.3914 | Test Acc:0.3947
[Epoch:790] Train Loss:1.1973 | Valid Acc:0.3908 | Test Acc:0.3947
Best results: epoch=203, valid loss=1.3352, valid acc=0.4099, test loss:1.3375,test acc=0.4020
{'activation': 'elu', 'dropout': 5, 'hidden_size': 512, 'learning_rate': 0.004338607259464629, 'model': 'f2gnn', 'optimizer': 'adam', 'weight_decay': 0.0005101218724526492}
Current Hyperparameter: {'activation': 'elu', 'dropout': 5, 'hidden_size': 512, 'learning_rate': 0.004338607259464629, 'model': 'f2gnn', 'optimizer': 'adam', 'weight_decay': 0.0005101218724526492}
[Epoch:0] Train Loss:1.6784 | Valid Acc:0.2388 | Test Acc:0.2388
[Epoch:10] Train Loss:1.8460 | Valid Acc:0.2388 | Test Acc:0.2388
[Epoch:20] Train Loss:1.5839 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:30] Train Loss:1.5781 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:40] Train Loss:1.5773 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:50] Train Loss:1.5734 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:60] Train Loss:1.5726 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:70] Train Loss:1.5711 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:80] Train Loss:1.5724 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:90] Train Loss:1.5705 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:100] Train Loss:1.5683 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:110] Train Loss:1.5639 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:120] Train Loss:1.5436 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:130] Train Loss:1.5002 | Valid Acc:0.2618 | Test Acc:0.2625
[Epoch:140] Train Loss:1.4304 | Valid Acc:0.3362 | Test Acc:0.3388
[Epoch:150] Train Loss:1.3820 | Valid Acc:0.3737 | Test Acc:0.3967
[Epoch:160] Train Loss:1.3603 | Valid Acc:0.3822 | Test Acc:0.3882
[Epoch:170] Train Loss:1.3827 | Valid Acc:0.3026 | Test Acc:0.3033
[Epoch:180] Train Loss:1.3210 | Valid Acc:0.3803 | Test Acc:0.3743
[Epoch:190] Train Loss:1.3222 | Valid Acc:0.3796 | Test Acc:0.4020
[Epoch:200] Train Loss:1.2925 | Valid Acc:0.3776 | Test Acc:0.4020
[Epoch:210] Train Loss:1.2687 | Valid Acc:0.3579 | Test Acc:0.3684
[Epoch:220] Train Loss:1.3525 | Valid Acc:0.3671 | Test Acc:0.3770
[Epoch:230] Train Loss:1.2922 | Valid Acc:0.3618 | Test Acc:0.3684
[Epoch:240] Train Loss:1.2799 | Valid Acc:0.3941 | Test Acc:0.4026
[Epoch:250] Train Loss:1.2548 | Valid Acc:0.3763 | Test Acc:0.3967
[Epoch:260] Train Loss:1.2385 | Valid Acc:0.3566 | Test Acc:0.3697
[Epoch:270] Train Loss:1.2266 | Valid Acc:0.3618 | Test Acc:0.3803
[Epoch:280] Train Loss:1.2147 | Valid Acc:0.3691 | Test Acc:0.3908
[Epoch:290] Train Loss:1.2045 | Valid Acc:0.3553 | Test Acc:0.3724
[Epoch:300] Train Loss:1.1975 | Valid Acc:0.3599 | Test Acc:0.3796
[Epoch:310] Train Loss:1.1987 | Valid Acc:0.3888 | Test Acc:0.3842
[Epoch:320] Train Loss:1.1958 | Valid Acc:0.3849 | Test Acc:0.3842
[Epoch:330] Train Loss:1.1811 | Valid Acc:0.3651 | Test Acc:0.3842
[Epoch:340] Train Loss:1.1686 | Valid Acc:0.3868 | Test Acc:0.3862
[Epoch:350] Train Loss:1.1662 | Valid Acc:0.3618 | Test Acc:0.3605
[Epoch:360] Train Loss:1.1687 | Valid Acc:0.3513 | Test Acc:0.3553
[Epoch:370] Train Loss:1.1575 | Valid Acc:0.3664 | Test Acc:0.3829
[Epoch:380] Train Loss:1.1402 | Valid Acc:0.3566 | Test Acc:0.3750
[Epoch:390] Train Loss:1.1353 | Valid Acc:0.3750 | Test Acc:0.3934
[Epoch:400] Train Loss:1.1404 | Valid Acc:0.3776 | Test Acc:0.3901
[Epoch:410] Train Loss:1.1384 | Valid Acc:0.3724 | Test Acc:0.3895
[Epoch:420] Train Loss:1.1167 | Valid Acc:0.3763 | Test Acc:0.3829
[Epoch:430] Train Loss:1.0950 | Valid Acc:0.3664 | Test Acc:0.3855
[Epoch:440] Train Loss:1.0900 | Valid Acc:0.3691 | Test Acc:0.3862
[Epoch:450] Train Loss:1.0840 | Valid Acc:0.3697 | Test Acc:0.3770
[Epoch:460] Train Loss:1.0857 | Valid Acc:0.3658 | Test Acc:0.3855
[Epoch:470] Train Loss:1.0798 | Valid Acc:0.3691 | Test Acc:0.3770
[Epoch:480] Train Loss:1.0589 | Valid Acc:0.3697 | Test Acc:0.3783
[Epoch:490] Train Loss:1.0472 | Valid Acc:0.3724 | Test Acc:0.3895
[Epoch:500] Train Loss:1.0455 | Valid Acc:0.3704 | Test Acc:0.3776
[Epoch:510] Train Loss:1.0289 | Valid Acc:0.3684 | Test Acc:0.3829
[Epoch:520] Train Loss:1.0231 | Valid Acc:0.3645 | Test Acc:0.3763
[Epoch:530] Train Loss:1.0062 | Valid Acc:0.3678 | Test Acc:0.3691
[Epoch:540] Train Loss:1.0172 | Valid Acc:0.3671 | Test Acc:0.3875
[Epoch:550] Train Loss:1.0024 | Valid Acc:0.3645 | Test Acc:0.3809
[Epoch:560] Train Loss:1.0146 | Valid Acc:0.3724 | Test Acc:0.3914
[Epoch:570] Train Loss:0.9947 | Valid Acc:0.3691 | Test Acc:0.3743
[Epoch:580] Train Loss:0.9840 | Valid Acc:0.3717 | Test Acc:0.3789
[Epoch:590] Train Loss:0.9736 | Valid Acc:0.3776 | Test Acc:0.3757
[Epoch:600] Train Loss:0.9669 | Valid Acc:0.3757 | Test Acc:0.3750
[Epoch:610] Train Loss:0.9632 | Valid Acc:0.3796 | Test Acc:0.3816
[Epoch:620] Train Loss:0.9586 | Valid Acc:0.3743 | Test Acc:0.3783
[Epoch:630] Train Loss:0.9557 | Valid Acc:0.3737 | Test Acc:0.3730
[Epoch:640] Train Loss:0.9534 | Valid Acc:0.3697 | Test Acc:0.3803
[Epoch:650] Train Loss:0.9476 | Valid Acc:0.3757 | Test Acc:0.3770
[Epoch:660] Train Loss:0.9447 | Valid Acc:0.3763 | Test Acc:0.3697
[Epoch:670] Train Loss:0.9384 | Valid Acc:0.3711 | Test Acc:0.3737
[Epoch:680] Train Loss:0.9291 | Valid Acc:0.3750 | Test Acc:0.3730
[Epoch:690] Train Loss:0.9315 | Valid Acc:0.3724 | Test Acc:0.3770
[Epoch:700] Train Loss:0.9265 | Valid Acc:0.3730 | Test Acc:0.3763
[Epoch:710] Train Loss:0.9294 | Valid Acc:0.3750 | Test Acc:0.3770
[Epoch:720] Train Loss:0.9292 | Valid Acc:0.3724 | Test Acc:0.3803
[Epoch:730] Train Loss:0.9333 | Valid Acc:0.3697 | Test Acc:0.3789
[Epoch:740] Train Loss:0.9253 | Valid Acc:0.3697 | Test Acc:0.3803
[Epoch:750] Train Loss:0.9220 | Valid Acc:0.3711 | Test Acc:0.3829
[Epoch:760] Train Loss:0.9292 | Valid Acc:0.3724 | Test Acc:0.3836
[Epoch:770] Train Loss:0.9175 | Valid Acc:0.3704 | Test Acc:0.3822
[Epoch:780] Train Loss:0.9286 | Valid Acc:0.3691 | Test Acc:0.3816
[Epoch:790] Train Loss:0.9222 | Valid Acc:0.3691 | Test Acc:0.3836
Best results: epoch=189, valid loss=1.3664, valid acc=0.3941, test loss:1.3715,test acc=0.3822
{'activation': 'relu', 'dropout': 3, 'hidden_size': 16, 'learning_rate': 0.005658710083959977, 'model': 'f2gnn', 'optimizer': 'adagrad', 'weight_decay': 0.00028044874649437194}
Current Hyperparameter: {'activation': 'relu', 'dropout': 3, 'hidden_size': 16, 'learning_rate': 0.005658710083959977, 'model': 'f2gnn', 'optimizer': 'adagrad', 'weight_decay': 0.00028044874649437194}
[Epoch:0] Train Loss:1.6102 | Valid Acc:0.2599 | Test Acc:0.2579
[Epoch:10] Train Loss:1.5776 | Valid Acc:0.2625 | Test Acc:0.2592
[Epoch:20] Train Loss:1.5710 | Valid Acc:0.2638 | Test Acc:0.2605
[Epoch:30] Train Loss:1.5632 | Valid Acc:0.2862 | Test Acc:0.2842
[Epoch:40] Train Loss:1.5540 | Valid Acc:0.2618 | Test Acc:0.2579
[Epoch:50] Train Loss:1.5447 | Valid Acc:0.2632 | Test Acc:0.2586
[Epoch:60] Train Loss:1.5236 | Valid Acc:0.2862 | Test Acc:0.2862
[Epoch:70] Train Loss:1.5054 | Valid Acc:0.3053 | Test Acc:0.3079
[Epoch:80] Train Loss:1.4832 | Valid Acc:0.3046 | Test Acc:0.3086
[Epoch:90] Train Loss:1.4700 | Valid Acc:0.3382 | Test Acc:0.3414
[Epoch:100] Train Loss:1.4486 | Valid Acc:0.3428 | Test Acc:0.3579
[Epoch:110] Train Loss:1.4484 | Valid Acc:0.3342 | Test Acc:0.3408
[Epoch:120] Train Loss:1.4267 | Valid Acc:0.3559 | Test Acc:0.3737
[Epoch:130] Train Loss:1.4184 | Valid Acc:0.3737 | Test Acc:0.3961
[Epoch:140] Train Loss:1.4023 | Valid Acc:0.3678 | Test Acc:0.3796
[Epoch:150] Train Loss:1.3956 | Valid Acc:0.3770 | Test Acc:0.3921
[Epoch:160] Train Loss:1.3931 | Valid Acc:0.3822 | Test Acc:0.4000
[Epoch:170] Train Loss:1.3796 | Valid Acc:0.3816 | Test Acc:0.3961
[Epoch:180] Train Loss:1.3766 | Valid Acc:0.3882 | Test Acc:0.3993
[Epoch:190] Train Loss:1.3633 | Valid Acc:0.3954 | Test Acc:0.4020
[Epoch:200] Train Loss:1.3607 | Valid Acc:0.3947 | Test Acc:0.4125
[Epoch:210] Train Loss:1.3579 | Valid Acc:0.3954 | Test Acc:0.4112
[Epoch:220] Train Loss:1.3480 | Valid Acc:0.3980 | Test Acc:0.4099
[Epoch:230] Train Loss:1.3401 | Valid Acc:0.3954 | Test Acc:0.4072
[Epoch:240] Train Loss:1.3400 | Valid Acc:0.3967 | Test Acc:0.4105
[Epoch:250] Train Loss:1.3414 | Valid Acc:0.4007 | Test Acc:0.4066
[Epoch:260] Train Loss:1.3220 | Valid Acc:0.3987 | Test Acc:0.4053
[Epoch:270] Train Loss:1.3262 | Valid Acc:0.4000 | Test Acc:0.4033
[Epoch:280] Train Loss:1.3258 | Valid Acc:0.3967 | Test Acc:0.4020
[Epoch:290] Train Loss:1.3179 | Valid Acc:0.4013 | Test Acc:0.4053
[Epoch:300] Train Loss:1.3139 | Valid Acc:0.3980 | Test Acc:0.4033
[Epoch:310] Train Loss:1.3125 | Valid Acc:0.3961 | Test Acc:0.4026
[Epoch:320] Train Loss:1.3084 | Valid Acc:0.3980 | Test Acc:0.4007
[Epoch:330] Train Loss:1.3139 | Valid Acc:0.3987 | Test Acc:0.4013
[Epoch:340] Train Loss:1.3035 | Valid Acc:0.3967 | Test Acc:0.4013
[Epoch:350] Train Loss:1.3048 | Valid Acc:0.3987 | Test Acc:0.3974
[Epoch:360] Train Loss:1.3025 | Valid Acc:0.4000 | Test Acc:0.3974
[Epoch:370] Train Loss:1.2917 | Valid Acc:0.3967 | Test Acc:0.4007
[Epoch:380] Train Loss:1.2993 | Valid Acc:0.3980 | Test Acc:0.4000
[Epoch:390] Train Loss:1.2961 | Valid Acc:0.3967 | Test Acc:0.3961
[Epoch:400] Train Loss:1.2958 | Valid Acc:0.3961 | Test Acc:0.3961
[Epoch:410] Train Loss:1.2963 | Valid Acc:0.3947 | Test Acc:0.3961
[Epoch:420] Train Loss:1.2837 | Valid Acc:0.3974 | Test Acc:0.3974
[Epoch:430] Train Loss:1.2860 | Valid Acc:0.3967 | Test Acc:0.3967
[Epoch:440] Train Loss:1.2913 | Valid Acc:0.3961 | Test Acc:0.4000
[Epoch:450] Train Loss:1.2828 | Valid Acc:0.3954 | Test Acc:0.3974
[Epoch:460] Train Loss:1.2842 | Valid Acc:0.3954 | Test Acc:0.3954
[Epoch:470] Train Loss:1.2826 | Valid Acc:0.3980 | Test Acc:0.3961
[Epoch:480] Train Loss:1.2825 | Valid Acc:0.3961 | Test Acc:0.4000
[Epoch:490] Train Loss:1.2916 | Valid Acc:0.3967 | Test Acc:0.3961
[Epoch:500] Train Loss:1.2788 | Valid Acc:0.3980 | Test Acc:0.3947
[Epoch:510] Train Loss:1.2797 | Valid Acc:0.3974 | Test Acc:0.3928
[Epoch:520] Train Loss:1.2743 | Valid Acc:0.3967 | Test Acc:0.3954
[Epoch:530] Train Loss:1.2815 | Valid Acc:0.3974 | Test Acc:0.3961
[Epoch:540] Train Loss:1.2820 | Valid Acc:0.3961 | Test Acc:0.3961
[Epoch:550] Train Loss:1.2742 | Valid Acc:0.3947 | Test Acc:0.3947
[Epoch:560] Train Loss:1.2726 | Valid Acc:0.3954 | Test Acc:0.3954
[Epoch:570] Train Loss:1.2823 | Valid Acc:0.3954 | Test Acc:0.3967
[Epoch:580] Train Loss:1.2723 | Valid Acc:0.3961 | Test Acc:0.3947
[Epoch:590] Train Loss:1.2725 | Valid Acc:0.3961 | Test Acc:0.3947
[Epoch:600] Train Loss:1.2735 | Valid Acc:0.3961 | Test Acc:0.3941
[Epoch:610] Train Loss:1.2727 | Valid Acc:0.3954 | Test Acc:0.3961
[Epoch:620] Train Loss:1.2772 | Valid Acc:0.3967 | Test Acc:0.3961
[Epoch:630] Train Loss:1.2795 | Valid Acc:0.3961 | Test Acc:0.3961
[Epoch:640] Train Loss:1.2757 | Valid Acc:0.3954 | Test Acc:0.3961
[Epoch:650] Train Loss:1.2752 | Valid Acc:0.3961 | Test Acc:0.3954
[Epoch:660] Train Loss:1.2714 | Valid Acc:0.3967 | Test Acc:0.3961
[Epoch:670] Train Loss:1.2785 | Valid Acc:0.3961 | Test Acc:0.3947
[Epoch:680] Train Loss:1.2718 | Valid Acc:0.3928 | Test Acc:0.3980
[Epoch:690] Train Loss:1.2673 | Valid Acc:0.3961 | Test Acc:0.3954
[Epoch:700] Train Loss:1.2722 | Valid Acc:0.3961 | Test Acc:0.3954
[Epoch:710] Train Loss:1.2735 | Valid Acc:0.3961 | Test Acc:0.3961
[Epoch:720] Train Loss:1.2694 | Valid Acc:0.3954 | Test Acc:0.3967
[Epoch:730] Train Loss:1.2682 | Valid Acc:0.3954 | Test Acc:0.3961
[Epoch:740] Train Loss:1.2710 | Valid Acc:0.3954 | Test Acc:0.3961
[Epoch:750] Train Loss:1.2691 | Valid Acc:0.3954 | Test Acc:0.3967
[Epoch:760] Train Loss:1.2678 | Valid Acc:0.3954 | Test Acc:0.3967
[Epoch:770] Train Loss:1.2746 | Valid Acc:0.3954 | Test Acc:0.3967
[Epoch:780] Train Loss:1.2662 | Valid Acc:0.3954 | Test Acc:0.3967
[Epoch:790] Train Loss:1.2773 | Valid Acc:0.3954 | Test Acc:0.3967
Best results: epoch=242, valid loss=1.3431, valid acc=0.4020, test loss:1.3469,test acc=0.4099
{'activation': 'relu', 'dropout': 0, 'hidden_size': 128, 'learning_rate': 0.0019119802423693286, 'model': 'f2gnn', 'optimizer': 'adam', 'weight_decay': 0.0006752104997269008}
Current Hyperparameter: {'activation': 'relu', 'dropout': 0, 'hidden_size': 128, 'learning_rate': 0.0019119802423693286, 'model': 'f2gnn', 'optimizer': 'adam', 'weight_decay': 0.0006752104997269008}
[Epoch:0] Train Loss:1.6361 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:10] Train Loss:1.5695 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:20] Train Loss:1.5486 | Valid Acc:0.2842 | Test Acc:0.2842
[Epoch:30] Train Loss:1.4747 | Valid Acc:0.2730 | Test Acc:0.2750
[Epoch:40] Train Loss:1.3734 | Valid Acc:0.3375 | Test Acc:0.3441
[Epoch:50] Train Loss:1.2766 | Valid Acc:0.3868 | Test Acc:0.4026
[Epoch:60] Train Loss:1.3063 | Valid Acc:0.4039 | Test Acc:0.3974
[Epoch:70] Train Loss:1.2234 | Valid Acc:0.3947 | Test Acc:0.3954
[Epoch:80] Train Loss:1.1673 | Valid Acc:0.3809 | Test Acc:0.3809
[Epoch:90] Train Loss:1.1328 | Valid Acc:0.3711 | Test Acc:0.3599
[Epoch:100] Train Loss:1.3140 | Valid Acc:0.3553 | Test Acc:0.3507
[Epoch:110] Train Loss:1.1620 | Valid Acc:0.3822 | Test Acc:0.3776
[Epoch:120] Train Loss:1.1431 | Valid Acc:0.3816 | Test Acc:0.3816
[Epoch:130] Train Loss:1.1117 | Valid Acc:0.3855 | Test Acc:0.3730
[Epoch:140] Train Loss:1.0923 | Valid Acc:0.3776 | Test Acc:0.3743
[Epoch:150] Train Loss:1.0912 | Valid Acc:0.3809 | Test Acc:0.3704
[Epoch:160] Train Loss:1.0880 | Valid Acc:0.3704 | Test Acc:0.3789
[Epoch:170] Train Loss:1.0395 | Valid Acc:0.3737 | Test Acc:0.3533
[Epoch:180] Train Loss:1.1071 | Valid Acc:0.3513 | Test Acc:0.3263
[Epoch:190] Train Loss:1.0077 | Valid Acc:0.3645 | Test Acc:0.3553
[Epoch:200] Train Loss:0.9746 | Valid Acc:0.3678 | Test Acc:0.3579
[Epoch:210] Train Loss:0.9323 | Valid Acc:0.3757 | Test Acc:0.3658
[Epoch:220] Train Loss:0.8958 | Valid Acc:0.3763 | Test Acc:0.3638
[Epoch:230] Train Loss:1.0064 | Valid Acc:0.3697 | Test Acc:0.3697
[Epoch:240] Train Loss:0.9753 | Valid Acc:0.3336 | Test Acc:0.3375
[Epoch:250] Train Loss:1.0271 | Valid Acc:0.3678 | Test Acc:0.3697
[Epoch:260] Train Loss:0.8577 | Valid Acc:0.3645 | Test Acc:0.3664
[Epoch:270] Train Loss:0.8129 | Valid Acc:0.3691 | Test Acc:0.3638
[Epoch:280] Train Loss:0.8242 | Valid Acc:0.3526 | Test Acc:0.3618
[Epoch:290] Train Loss:0.7595 | Valid Acc:0.3664 | Test Acc:0.3618
[Epoch:300] Train Loss:0.7654 | Valid Acc:0.3605 | Test Acc:0.3526
[Epoch:310] Train Loss:0.7270 | Valid Acc:0.3599 | Test Acc:0.3671
[Epoch:320] Train Loss:0.6956 | Valid Acc:0.3651 | Test Acc:0.3612
[Epoch:330] Train Loss:0.6630 | Valid Acc:0.3592 | Test Acc:0.3645
[Epoch:340] Train Loss:0.6436 | Valid Acc:0.3592 | Test Acc:0.3638
[Epoch:350] Train Loss:0.6239 | Valid Acc:0.3553 | Test Acc:0.3612
[Epoch:360] Train Loss:0.6257 | Valid Acc:0.3579 | Test Acc:0.3632
[Epoch:370] Train Loss:0.6041 | Valid Acc:0.3520 | Test Acc:0.3632
[Epoch:380] Train Loss:0.5949 | Valid Acc:0.3539 | Test Acc:0.3566
[Epoch:390] Train Loss:0.5804 | Valid Acc:0.3434 | Test Acc:0.3605
[Epoch:400] Train Loss:0.5666 | Valid Acc:0.3421 | Test Acc:0.3539
[Epoch:410] Train Loss:0.5454 | Valid Acc:0.3546 | Test Acc:0.3559
[Epoch:420] Train Loss:0.5313 | Valid Acc:0.3507 | Test Acc:0.3533
[Epoch:430] Train Loss:0.5223 | Valid Acc:0.3441 | Test Acc:0.3526
[Epoch:440] Train Loss:0.5135 | Valid Acc:0.3388 | Test Acc:0.3539
[Epoch:450] Train Loss:0.5075 | Valid Acc:0.3428 | Test Acc:0.3507
[Epoch:460] Train Loss:0.4975 | Valid Acc:0.3395 | Test Acc:0.3461
[Epoch:470] Train Loss:0.4900 | Valid Acc:0.3401 | Test Acc:0.3447
[Epoch:480] Train Loss:0.4855 | Valid Acc:0.3441 | Test Acc:0.3507
[Epoch:490] Train Loss:0.4768 | Valid Acc:0.3388 | Test Acc:0.3428
[Epoch:500] Train Loss:0.4709 | Valid Acc:0.3388 | Test Acc:0.3414
[Epoch:510] Train Loss:0.4676 | Valid Acc:0.3401 | Test Acc:0.3414
[Epoch:520] Train Loss:0.4618 | Valid Acc:0.3355 | Test Acc:0.3434
[Epoch:530] Train Loss:0.4570 | Valid Acc:0.3395 | Test Acc:0.3408
[Epoch:540] Train Loss:0.4531 | Valid Acc:0.3368 | Test Acc:0.3414
[Epoch:550] Train Loss:0.4485 | Valid Acc:0.3362 | Test Acc:0.3362
[Epoch:560] Train Loss:0.4454 | Valid Acc:0.3355 | Test Acc:0.3382
[Epoch:570] Train Loss:0.4421 | Valid Acc:0.3349 | Test Acc:0.3388
[Epoch:580] Train Loss:0.4393 | Valid Acc:0.3355 | Test Acc:0.3388
[Epoch:590] Train Loss:0.4367 | Valid Acc:0.3355 | Test Acc:0.3388
[Epoch:600] Train Loss:0.4345 | Valid Acc:0.3362 | Test Acc:0.3362
[Epoch:610] Train Loss:0.4324 | Valid Acc:0.3355 | Test Acc:0.3355
[Epoch:620] Train Loss:0.4306 | Valid Acc:0.3355 | Test Acc:0.3355
[Epoch:630] Train Loss:0.4288 | Valid Acc:0.3368 | Test Acc:0.3349
[Epoch:640] Train Loss:0.4274 | Valid Acc:0.3362 | Test Acc:0.3336
[Epoch:650] Train Loss:0.4261 | Valid Acc:0.3355 | Test Acc:0.3342
[Epoch:660] Train Loss:0.4249 | Valid Acc:0.3362 | Test Acc:0.3349
[Epoch:670] Train Loss:0.4239 | Valid Acc:0.3355 | Test Acc:0.3329
[Epoch:680] Train Loss:0.4231 | Valid Acc:0.3368 | Test Acc:0.3322
[Epoch:690] Train Loss:0.4223 | Valid Acc:0.3362 | Test Acc:0.3336
[Epoch:700] Train Loss:0.4217 | Valid Acc:0.3368 | Test Acc:0.3349
[Epoch:710] Train Loss:0.4212 | Valid Acc:0.3375 | Test Acc:0.3342
[Epoch:720] Train Loss:0.4208 | Valid Acc:0.3388 | Test Acc:0.3349
[Epoch:730] Train Loss:0.4205 | Valid Acc:0.3388 | Test Acc:0.3362
[Epoch:740] Train Loss:0.4203 | Valid Acc:0.3395 | Test Acc:0.3355
[Epoch:750] Train Loss:0.4201 | Valid Acc:0.3388 | Test Acc:0.3349
[Epoch:760] Train Loss:0.4200 | Valid Acc:0.3382 | Test Acc:0.3349
[Epoch:770] Train Loss:0.4199 | Valid Acc:0.3388 | Test Acc:0.3355
[Epoch:780] Train Loss:0.4199 | Valid Acc:0.3395 | Test Acc:0.3362
[Epoch:790] Train Loss:0.4198 | Valid Acc:0.3395 | Test Acc:0.3349
Best results: epoch=60, valid loss=1.3451, valid acc=0.4039, test loss:1.3481,test acc=0.3974
{'activation': 'elu', 'dropout': 6, 'hidden_size': 128, 'learning_rate': 0.0032200980313186944, 'model': 'f2gnn', 'optimizer': 'adagrad', 'weight_decay': 0.0008910039906481695}
Current Hyperparameter: {'activation': 'elu', 'dropout': 6, 'hidden_size': 128, 'learning_rate': 0.0032200980313186944, 'model': 'f2gnn', 'optimizer': 'adagrad', 'weight_decay': 0.0008910039906481695}
[Epoch:0] Train Loss:1.7480 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:10] Train Loss:1.5910 | Valid Acc:0.2388 | Test Acc:0.2388
[Epoch:20] Train Loss:1.5826 | Valid Acc:0.2388 | Test Acc:0.2388
[Epoch:30] Train Loss:1.5778 | Valid Acc:0.2164 | Test Acc:0.2171
[Epoch:40] Train Loss:1.5782 | Valid Acc:0.2388 | Test Acc:0.2388
[Epoch:50] Train Loss:1.5770 | Valid Acc:0.3020 | Test Acc:0.3171
[Epoch:60] Train Loss:1.5719 | Valid Acc:0.2697 | Test Acc:0.2704
[Epoch:70] Train Loss:1.5681 | Valid Acc:0.2803 | Test Acc:0.2836
[Epoch:80] Train Loss:1.5664 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:90] Train Loss:1.5603 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:100] Train Loss:1.5536 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:110] Train Loss:1.5429 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:120] Train Loss:1.5354 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:130] Train Loss:1.5257 | Valid Acc:0.2592 | Test Acc:0.2586
[Epoch:140] Train Loss:1.5121 | Valid Acc:0.2638 | Test Acc:0.2618
[Epoch:150] Train Loss:1.4972 | Valid Acc:0.2776 | Test Acc:0.2822
[Epoch:160] Train Loss:1.4807 | Valid Acc:0.3007 | Test Acc:0.2882
[Epoch:170] Train Loss:1.4789 | Valid Acc:0.3270 | Test Acc:0.3039
[Epoch:180] Train Loss:1.4614 | Valid Acc:0.3329 | Test Acc:0.3263
[Epoch:190] Train Loss:1.4561 | Valid Acc:0.3349 | Test Acc:0.3289
[Epoch:200] Train Loss:1.4433 | Valid Acc:0.3408 | Test Acc:0.3375
[Epoch:210] Train Loss:1.4305 | Valid Acc:0.3586 | Test Acc:0.3757
[Epoch:220] Train Loss:1.4212 | Valid Acc:0.3546 | Test Acc:0.3684
[Epoch:230] Train Loss:1.4157 | Valid Acc:0.3533 | Test Acc:0.3664
[Epoch:240] Train Loss:1.4021 | Valid Acc:0.3664 | Test Acc:0.3901
[Epoch:250] Train Loss:1.3932 | Valid Acc:0.3697 | Test Acc:0.3967
[Epoch:260] Train Loss:1.3948 | Valid Acc:0.3658 | Test Acc:0.4007
[Epoch:270] Train Loss:1.3871 | Valid Acc:0.3704 | Test Acc:0.3980
[Epoch:280] Train Loss:1.3819 | Valid Acc:0.3783 | Test Acc:0.3993
[Epoch:290] Train Loss:1.3732 | Valid Acc:0.3750 | Test Acc:0.3980
[Epoch:300] Train Loss:1.3745 | Valid Acc:0.3737 | Test Acc:0.4020
[Epoch:310] Train Loss:1.3624 | Valid Acc:0.3763 | Test Acc:0.4033
[Epoch:320] Train Loss:1.3554 | Valid Acc:0.3803 | Test Acc:0.3934
[Epoch:330] Train Loss:1.3625 | Valid Acc:0.3796 | Test Acc:0.4072
[Epoch:340] Train Loss:1.3562 | Valid Acc:0.3822 | Test Acc:0.4072
[Epoch:350] Train Loss:1.3561 | Valid Acc:0.3789 | Test Acc:0.4039
[Epoch:360] Train Loss:1.3428 | Valid Acc:0.3842 | Test Acc:0.4013
[Epoch:370] Train Loss:1.3447 | Valid Acc:0.3842 | Test Acc:0.3914
[Epoch:380] Train Loss:1.3466 | Valid Acc:0.3829 | Test Acc:0.3921
[Epoch:390] Train Loss:1.3474 | Valid Acc:0.3849 | Test Acc:0.3882
[Epoch:400] Train Loss:1.3341 | Valid Acc:0.3849 | Test Acc:0.3921
[Epoch:410] Train Loss:1.3384 | Valid Acc:0.3868 | Test Acc:0.3809
[Epoch:420] Train Loss:1.3353 | Valid Acc:0.3875 | Test Acc:0.3941
[Epoch:430] Train Loss:1.3324 | Valid Acc:0.3888 | Test Acc:0.4007
[Epoch:440] Train Loss:1.3318 | Valid Acc:0.3882 | Test Acc:0.3934
[Epoch:450] Train Loss:1.3261 | Valid Acc:0.3914 | Test Acc:0.4007
[Epoch:460] Train Loss:1.3228 | Valid Acc:0.3901 | Test Acc:0.4039
[Epoch:470] Train Loss:1.3287 | Valid Acc:0.3862 | Test Acc:0.3961
[Epoch:480] Train Loss:1.3281 | Valid Acc:0.3888 | Test Acc:0.4013
[Epoch:490] Train Loss:1.3259 | Valid Acc:0.3888 | Test Acc:0.3993
[Epoch:500] Train Loss:1.3305 | Valid Acc:0.3914 | Test Acc:0.3993
[Epoch:510] Train Loss:1.3204 | Valid Acc:0.3928 | Test Acc:0.3980
[Epoch:520] Train Loss:1.3222 | Valid Acc:0.3914 | Test Acc:0.4013
[Epoch:530] Train Loss:1.3177 | Valid Acc:0.3928 | Test Acc:0.4026
[Epoch:540] Train Loss:1.3293 | Valid Acc:0.3908 | Test Acc:0.4007
[Epoch:550] Train Loss:1.3230 | Valid Acc:0.3921 | Test Acc:0.4033
[Epoch:560] Train Loss:1.3171 | Valid Acc:0.3914 | Test Acc:0.4039
[Epoch:570] Train Loss:1.3176 | Valid Acc:0.3914 | Test Acc:0.3980
[Epoch:580] Train Loss:1.3189 | Valid Acc:0.3908 | Test Acc:0.4046
[Epoch:590] Train Loss:1.3199 | Valid Acc:0.3941 | Test Acc:0.4039
[Epoch:600] Train Loss:1.3151 | Valid Acc:0.3947 | Test Acc:0.4059
[Epoch:610] Train Loss:1.3150 | Valid Acc:0.3934 | Test Acc:0.4020
[Epoch:620] Train Loss:1.3169 | Valid Acc:0.3947 | Test Acc:0.4020
[Epoch:630] Train Loss:1.3189 | Valid Acc:0.3947 | Test Acc:0.4046
[Epoch:640] Train Loss:1.3139 | Valid Acc:0.3941 | Test Acc:0.4033
[Epoch:650] Train Loss:1.3110 | Valid Acc:0.3967 | Test Acc:0.3980
[Epoch:660] Train Loss:1.3108 | Valid Acc:0.3934 | Test Acc:0.4039
[Epoch:670] Train Loss:1.3061 | Valid Acc:0.3980 | Test Acc:0.4013
[Epoch:680] Train Loss:1.3139 | Valid Acc:0.3961 | Test Acc:0.4026
[Epoch:690] Train Loss:1.3098 | Valid Acc:0.3928 | Test Acc:0.4033
[Epoch:700] Train Loss:1.3197 | Valid Acc:0.3928 | Test Acc:0.4046
[Epoch:710] Train Loss:1.3154 | Valid Acc:0.3941 | Test Acc:0.4033
[Epoch:720] Train Loss:1.3182 | Valid Acc:0.3954 | Test Acc:0.4007
[Epoch:730] Train Loss:1.3071 | Valid Acc:0.3954 | Test Acc:0.4007
[Epoch:740] Train Loss:1.3116 | Valid Acc:0.3954 | Test Acc:0.4039
[Epoch:750] Train Loss:1.3184 | Valid Acc:0.3928 | Test Acc:0.4053
[Epoch:760] Train Loss:1.3104 | Valid Acc:0.3941 | Test Acc:0.4053
[Epoch:770] Train Loss:1.3218 | Valid Acc:0.3961 | Test Acc:0.4055
[Epoch:780] Train Loss:1.3169 | Valid Acc:0.3967 | Test Acc:0.4055
[Epoch:790] Train Loss:1.3196 | Valid Acc:0.3965 | Test Acc:0.4054
Best results: epoch=658, valid loss=1.3390, valid acc=0.3987, test loss:1.3475,test acc=0.4007
{'activation': 'elu', 'dropout': 4, 'hidden_size': 512, 'learning_rate': 0.0025105500662067843, 'model': 'f2gnn', 'optimizer': 'adam', 'weight_decay': 0.0006116505922708463}
Current Hyperparameter: {'activation': 'elu', 'dropout': 4, 'hidden_size': 512, 'learning_rate': 0.0025105500662067843, 'model': 'f2gnn', 'optimizer': 'adam', 'weight_decay': 0.0006116505922708463}
[Epoch:0] Train Loss:1.6612 | Valid Acc:0.2388 | Test Acc:0.2388
[Epoch:10] Train Loss:1.6227 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:20] Train Loss:1.5764 | Valid Acc:0.2612 | Test Acc:0.2572
[Epoch:30] Train Loss:1.5423 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:40] Train Loss:1.4827 | Valid Acc:0.2618 | Test Acc:0.2579
[Epoch:50] Train Loss:1.5262 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:60] Train Loss:1.4671 | Valid Acc:0.3026 | Test Acc:0.3053
[Epoch:70] Train Loss:1.4255 | Valid Acc:0.3033 | Test Acc:0.2947
[Epoch:80] Train Loss:1.3812 | Valid Acc:0.2770 | Test Acc:0.2691
[Epoch:90] Train Loss:1.4077 | Valid Acc:0.3197 | Test Acc:0.3243
[Epoch:100] Train Loss:1.3838 | Valid Acc:0.3138 | Test Acc:0.3164
[Epoch:110] Train Loss:1.3293 | Valid Acc:0.3849 | Test Acc:0.3967
[Epoch:120] Train Loss:1.3320 | Valid Acc:0.3803 | Test Acc:0.4000
[Epoch:130] Train Loss:1.3021 | Valid Acc:0.3809 | Test Acc:0.3974
[Epoch:140] Train Loss:1.2742 | Valid Acc:0.3395 | Test Acc:0.3605
[Epoch:150] Train Loss:1.2815 | Valid Acc:0.3842 | Test Acc:0.3993
[Epoch:160] Train Loss:1.6131 | Valid Acc:0.2776 | Test Acc:0.2829
[Epoch:170] Train Loss:1.3215 | Valid Acc:0.3395 | Test Acc:0.3447
[Epoch:180] Train Loss:1.2681 | Valid Acc:0.3309 | Test Acc:0.3322
[Epoch:190] Train Loss:1.2483 | Valid Acc:0.2947 | Test Acc:0.2987
[Epoch:200] Train Loss:1.3534 | Valid Acc:0.3750 | Test Acc:0.3967
[Epoch:210] Train Loss:1.2595 | Valid Acc:0.3461 | Test Acc:0.3566
[Epoch:220] Train Loss:1.2399 | Valid Acc:0.3625 | Test Acc:0.3658
[Epoch:230] Train Loss:1.2196 | Valid Acc:0.3664 | Test Acc:0.3743
[Epoch:240] Train Loss:1.1961 | Valid Acc:0.3664 | Test Acc:0.3796
[Epoch:250] Train Loss:1.1767 | Valid Acc:0.3717 | Test Acc:0.3691
[Epoch:260] Train Loss:1.2555 | Valid Acc:0.3164 | Test Acc:0.3099
[Epoch:270] Train Loss:1.2672 | Valid Acc:0.3658 | Test Acc:0.3566
[Epoch:280] Train Loss:1.2037 | Valid Acc:0.3434 | Test Acc:0.3586
[Epoch:290] Train Loss:1.1640 | Valid Acc:0.3789 | Test Acc:0.3783
[Epoch:300] Train Loss:1.1436 | Valid Acc:0.3750 | Test Acc:0.3783
[Epoch:310] Train Loss:1.1195 | Valid Acc:0.3783 | Test Acc:0.3704
[Epoch:320] Train Loss:1.1075 | Valid Acc:0.3605 | Test Acc:0.3757
[Epoch:330] Train Loss:1.0838 | Valid Acc:0.3625 | Test Acc:0.3566
[Epoch:340] Train Loss:1.1413 | Valid Acc:0.3789 | Test Acc:0.3724
[Epoch:350] Train Loss:1.0824 | Valid Acc:0.3789 | Test Acc:0.3868
[Epoch:360] Train Loss:1.0530 | Valid Acc:0.3829 | Test Acc:0.3836
[Epoch:370] Train Loss:1.0328 | Valid Acc:0.3816 | Test Acc:0.3776
[Epoch:380] Train Loss:1.0583 | Valid Acc:0.3776 | Test Acc:0.3829
[Epoch:390] Train Loss:1.0571 | Valid Acc:0.3770 | Test Acc:0.3678
[Epoch:400] Train Loss:1.0752 | Valid Acc:0.3862 | Test Acc:0.3711
[Epoch:410] Train Loss:1.0284 | Valid Acc:0.3539 | Test Acc:0.3599
[Epoch:420] Train Loss:0.9911 | Valid Acc:0.3763 | Test Acc:0.3645
[Epoch:430] Train Loss:0.9619 | Valid Acc:0.3757 | Test Acc:0.3638
[Epoch:440] Train Loss:0.9448 | Valid Acc:0.3671 | Test Acc:0.3632
[Epoch:450] Train Loss:0.9160 | Valid Acc:0.3671 | Test Acc:0.3638
[Epoch:460] Train Loss:0.8967 | Valid Acc:0.3618 | Test Acc:0.3664
[Epoch:470] Train Loss:0.8901 | Valid Acc:0.3579 | Test Acc:0.3658
[Epoch:480] Train Loss:0.8557 | Valid Acc:0.3605 | Test Acc:0.3704
[Epoch:490] Train Loss:0.8356 | Valid Acc:0.3691 | Test Acc:0.3605
[Epoch:500] Train Loss:0.8119 | Valid Acc:0.3559 | Test Acc:0.3664
[Epoch:510] Train Loss:0.8011 | Valid Acc:0.3507 | Test Acc:0.3566
[Epoch:520] Train Loss:0.7942 | Valid Acc:0.3461 | Test Acc:0.3632
[Epoch:530] Train Loss:0.7741 | Valid Acc:0.3559 | Test Acc:0.3592
[Epoch:540] Train Loss:0.7622 | Valid Acc:0.3572 | Test Acc:0.3586
[Epoch:550] Train Loss:0.7287 | Valid Acc:0.3513 | Test Acc:0.3546
[Epoch:560] Train Loss:0.7303 | Valid Acc:0.3526 | Test Acc:0.3546
[Epoch:570] Train Loss:0.7209 | Valid Acc:0.3461 | Test Acc:0.3533
[Epoch:580] Train Loss:0.6943 | Valid Acc:0.3487 | Test Acc:0.3467
[Epoch:590] Train Loss:0.6856 | Valid Acc:0.3487 | Test Acc:0.3513
[Epoch:600] Train Loss:0.6706 | Valid Acc:0.3500 | Test Acc:0.3513
[Epoch:610] Train Loss:0.6696 | Valid Acc:0.3461 | Test Acc:0.3539
[Epoch:620] Train Loss:0.6506 | Valid Acc:0.3480 | Test Acc:0.3539
[Epoch:630] Train Loss:0.6529 | Valid Acc:0.3447 | Test Acc:0.3520
[Epoch:640] Train Loss:0.6451 | Valid Acc:0.3408 | Test Acc:0.3553
[Epoch:650] Train Loss:0.6421 | Valid Acc:0.3434 | Test Acc:0.3572
[Epoch:660] Train Loss:0.6356 | Valid Acc:0.3441 | Test Acc:0.3474
[Epoch:670] Train Loss:0.6316 | Valid Acc:0.3414 | Test Acc:0.3539
[Epoch:680] Train Loss:0.6201 | Valid Acc:0.3355 | Test Acc:0.3526
[Epoch:690] Train Loss:0.6190 | Valid Acc:0.3428 | Test Acc:0.3553
[Epoch:700] Train Loss:0.6211 | Valid Acc:0.3408 | Test Acc:0.3546
[Epoch:710] Train Loss:0.6202 | Valid Acc:0.3454 | Test Acc:0.3566
[Epoch:720] Train Loss:0.6097 | Valid Acc:0.3414 | Test Acc:0.3513
[Epoch:730] Train Loss:0.6155 | Valid Acc:0.3447 | Test Acc:0.3553
[Epoch:740] Train Loss:0.6040 | Valid Acc:0.3421 | Test Acc:0.3546
[Epoch:750] Train Loss:0.6083 | Valid Acc:0.3434 | Test Acc:0.3539
[Epoch:760] Train Loss:0.6066 | Valid Acc:0.3441 | Test Acc:0.3546
[Epoch:770] Train Loss:0.6085 | Valid Acc:0.3447 | Test Acc:0.3546
[Epoch:780] Train Loss:0.6073 | Valid Acc:0.3441 | Test Acc:0.3546
[Epoch:790] Train Loss:0.6122 | Valid Acc:0.3461 | Test Acc:0.3553
Best results: epoch=143, valid loss=1.3640, valid acc=0.4007, test loss:1.3692,test acc=0.3776
{'activation': 'relu', 'dropout': 6, 'hidden_size': 256, 'learning_rate': 0.005879402236481791, 'model': 'f2gnn', 'optimizer': 'adagrad', 'weight_decay': 0.00016868864800359453}
Current Hyperparameter: {'activation': 'relu', 'dropout': 6, 'hidden_size': 256, 'learning_rate': 0.005879402236481791, 'model': 'f2gnn', 'optimizer': 'adagrad', 'weight_decay': 0.00016868864800359453}
[Epoch:0] Train Loss:1.7022 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:10] Train Loss:1.5910 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:20] Train Loss:1.5861 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:30] Train Loss:1.5801 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:40] Train Loss:1.5762 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:50] Train Loss:1.5706 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:60] Train Loss:1.5538 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:70] Train Loss:1.5445 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:80] Train Loss:1.5386 | Valid Acc:0.2474 | Test Acc:0.2513
[Epoch:90] Train Loss:1.5070 | Valid Acc:0.2730 | Test Acc:0.2757
[Epoch:100] Train Loss:1.5000 | Valid Acc:0.2928 | Test Acc:0.2947
[Epoch:110] Train Loss:1.4907 | Valid Acc:0.2803 | Test Acc:0.2809
[Epoch:120] Train Loss:1.4671 | Valid Acc:0.3132 | Test Acc:0.3105
[Epoch:130] Train Loss:1.4509 | Valid Acc:0.3454 | Test Acc:0.3592
[Epoch:140] Train Loss:1.4526 | Valid Acc:0.2954 | Test Acc:0.3000
[Epoch:150] Train Loss:1.4378 | Valid Acc:0.3026 | Test Acc:0.3007
[Epoch:160] Train Loss:1.4254 | Valid Acc:0.3217 | Test Acc:0.3191
[Epoch:170] Train Loss:1.4305 | Valid Acc:0.3112 | Test Acc:0.3105
[Epoch:180] Train Loss:1.4230 | Valid Acc:0.2993 | Test Acc:0.3000
[Epoch:190] Train Loss:1.4109 | Valid Acc:0.3132 | Test Acc:0.3099
[Epoch:200] Train Loss:1.3875 | Valid Acc:0.3197 | Test Acc:0.3250
[Epoch:210] Train Loss:1.3966 | Valid Acc:0.3158 | Test Acc:0.3178
[Epoch:220] Train Loss:1.3795 | Valid Acc:0.3250 | Test Acc:0.3283
[Epoch:230] Train Loss:1.3869 | Valid Acc:0.3237 | Test Acc:0.3342
[Epoch:240] Train Loss:1.3728 | Valid Acc:0.3211 | Test Acc:0.3257
[Epoch:250] Train Loss:1.3679 | Valid Acc:0.3322 | Test Acc:0.3434
[Epoch:260] Train Loss:1.3666 | Valid Acc:0.3289 | Test Acc:0.3309
[Epoch:270] Train Loss:1.3646 | Valid Acc:0.3217 | Test Acc:0.3276
[Epoch:280] Train Loss:1.3525 | Valid Acc:0.3388 | Test Acc:0.3434
[Epoch:290] Train Loss:1.3471 | Valid Acc:0.3467 | Test Acc:0.3507
[Epoch:300] Train Loss:1.3462 | Valid Acc:0.3559 | Test Acc:0.3651
[Epoch:310] Train Loss:1.3412 | Valid Acc:0.3434 | Test Acc:0.3599
[Epoch:320] Train Loss:1.3436 | Valid Acc:0.3605 | Test Acc:0.3684
[Epoch:330] Train Loss:1.3398 | Valid Acc:0.3408 | Test Acc:0.3480
[Epoch:340] Train Loss:1.3305 | Valid Acc:0.3368 | Test Acc:0.3355
[Epoch:350] Train Loss:1.3297 | Valid Acc:0.3526 | Test Acc:0.3651
[Epoch:360] Train Loss:1.3238 | Valid Acc:0.3520 | Test Acc:0.3605
[Epoch:370] Train Loss:1.3285 | Valid Acc:0.3553 | Test Acc:0.3599
[Epoch:380] Train Loss:1.3233 | Valid Acc:0.3474 | Test Acc:0.3618
[Epoch:390] Train Loss:1.3131 | Valid Acc:0.3572 | Test Acc:0.3638
[Epoch:400] Train Loss:1.3137 | Valid Acc:0.3513 | Test Acc:0.3461
[Epoch:410] Train Loss:1.3168 | Valid Acc:0.3349 | Test Acc:0.3336
[Epoch:420] Train Loss:1.3170 | Valid Acc:0.3414 | Test Acc:0.3375
[Epoch:430] Train Loss:1.3071 | Valid Acc:0.3329 | Test Acc:0.3283
[Epoch:440] Train Loss:1.3110 | Valid Acc:0.3375 | Test Acc:0.3303
[Epoch:450] Train Loss:1.3108 | Valid Acc:0.3401 | Test Acc:0.3296
[Epoch:460] Train Loss:1.3099 | Valid Acc:0.3428 | Test Acc:0.3270
[Epoch:470] Train Loss:1.3095 | Valid Acc:0.3474 | Test Acc:0.3283
[Epoch:480] Train Loss:1.3121 | Valid Acc:0.3421 | Test Acc:0.3270
[Epoch:490] Train Loss:1.3012 | Valid Acc:0.3474 | Test Acc:0.3316
[Epoch:500] Train Loss:1.3063 | Valid Acc:0.3638 | Test Acc:0.3329
[Epoch:510] Train Loss:1.3014 | Valid Acc:0.3592 | Test Acc:0.3408
[Epoch:520] Train Loss:1.3009 | Valid Acc:0.3480 | Test Acc:0.3270
[Epoch:530] Train Loss:1.3038 | Valid Acc:0.3474 | Test Acc:0.3283
[Epoch:540] Train Loss:1.2977 | Valid Acc:0.3507 | Test Acc:0.3316
[Epoch:550] Train Loss:1.3054 | Valid Acc:0.3533 | Test Acc:0.3362
[Epoch:560] Train Loss:1.3030 | Valid Acc:0.3507 | Test Acc:0.3336
[Epoch:570] Train Loss:1.2966 | Valid Acc:0.3520 | Test Acc:0.3349
[Epoch:580] Train Loss:1.2994 | Valid Acc:0.3507 | Test Acc:0.3355
[Epoch:590] Train Loss:1.3011 | Valid Acc:0.3546 | Test Acc:0.3401
[Epoch:600] Train Loss:1.3014 | Valid Acc:0.3559 | Test Acc:0.3395
[Epoch:610] Train Loss:1.2950 | Valid Acc:0.3553 | Test Acc:0.3362
[Epoch:620] Train Loss:1.2968 | Valid Acc:0.3533 | Test Acc:0.3375
[Epoch:630] Train Loss:1.2929 | Valid Acc:0.3546 | Test Acc:0.3355
[Epoch:640] Train Loss:1.2992 | Valid Acc:0.3533 | Test Acc:0.3342
[Epoch:650] Train Loss:1.3003 | Valid Acc:0.3546 | Test Acc:0.3349
[Epoch:660] Train Loss:1.2998 | Valid Acc:0.3526 | Test Acc:0.3329
[Epoch:670] Train Loss:1.2951 | Valid Acc:0.3539 | Test Acc:0.3355
[Epoch:680] Train Loss:1.2906 | Valid Acc:0.3507 | Test Acc:0.3342
[Epoch:690] Train Loss:1.2997 | Valid Acc:0.3513 | Test Acc:0.3342
[Epoch:700] Train Loss:1.2958 | Valid Acc:0.3533 | Test Acc:0.3355
[Epoch:710] Train Loss:1.2931 | Valid Acc:0.3539 | Test Acc:0.3368
[Epoch:720] Train Loss:1.3042 | Valid Acc:0.3533 | Test Acc:0.3368
[Epoch:730] Train Loss:1.2962 | Valid Acc:0.3526 | Test Acc:0.3355
[Epoch:740] Train Loss:1.2957 | Valid Acc:0.3533 | Test Acc:0.3368
[Epoch:750] Train Loss:1.2971 | Valid Acc:0.3526 | Test Acc:0.3368
[Epoch:760] Train Loss:1.2912 | Valid Acc:0.3526 | Test Acc:0.3368
[Epoch:770] Train Loss:1.2970 | Valid Acc:0.3526 | Test Acc:0.3368
[Epoch:780] Train Loss:1.2973 | Valid Acc:0.3526 | Test Acc:0.3368
[Epoch:790] Train Loss:1.2988 | Valid Acc:0.3526 | Test Acc:0.3368
Best results: epoch=193, valid loss=1.4308, valid acc=0.3737, test loss:1.4333,test acc=0.3770
Best space is  {'activation': 'relu', 'dropout': 2, 'hidden_size': 128, 'learning_rate': 0.002116974073439665, 'model': 'f2gnn', 'optimizer': 'adam', 'weight_decay': 0.0001037664689191369}
Best args from space is  {'gpu': 6, 'data': 'actor', 'arch_filename': 'exp_res/actor-searched_res-20241021-212059-eps0.0-reg0.0005.txt', 'arch': 'GATConv||GATConv||GATConv||zero||zero||zero||zero||identity||zero||identity||identity||identity||identity||mean||sum||sum||sum', 'num_layers': 3, 'hyper_epoch': 30, 'epochs': 800, 'cos_lr': True, 'std_times': 5, 'batch_size': 256, 'layer_norm': True, 'batch_norm': False, 'with_linear': False, 'min_lr': 0.0, 'rm_feature': False, 'NA_PRIMITIVES': ['GATConv'], 'FEATURE_ENGINEERING': ['GCNNorm', 'AddSelfLoops', 'RemoveIsolatedNodes'], 'FF_PRIMITIVES': ['sum', 'mean'], 'ACTIVATION': ['relu', 'elu'], 'activation': 'relu', 'dropout': 0.2, 'hidden_size': 128, 'learning_rate': 0.002116974073439665, 'model': 'f2gnn', 'optimizer': 'adam', 'weight_decay': 0.0001037664689191369, 'rnd_num': 1, 'seed': 2, 'grad_clip': 5, 'momentum': 0.9}
[Epoch:0] Train Loss:1.6525 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:10] Train Loss:1.5717 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:20] Train Loss:1.5600 | Valid Acc:0.2671 | Test Acc:0.2612
[Epoch:30] Train Loss:1.4940 | Valid Acc:0.2711 | Test Acc:0.2737
[Epoch:40] Train Loss:1.4064 | Valid Acc:0.3592 | Test Acc:0.3625
[Epoch:50] Train Loss:1.3611 | Valid Acc:0.4007 | Test Acc:0.4072
[Epoch:60] Train Loss:1.3109 | Valid Acc:0.3842 | Test Acc:0.3684
[Epoch:70] Train Loss:1.2811 | Valid Acc:0.3967 | Test Acc:0.3855
[Epoch:80] Train Loss:1.2503 | Valid Acc:0.3928 | Test Acc:0.3862
[Epoch:90] Train Loss:1.2164 | Valid Acc:0.3757 | Test Acc:0.3849
[Epoch:100] Train Loss:1.2007 | Valid Acc:0.3671 | Test Acc:0.3750
[Epoch:110] Train Loss:1.1641 | Valid Acc:0.3717 | Test Acc:0.3651
[Epoch:120] Train Loss:1.1382 | Valid Acc:0.3763 | Test Acc:0.3822
[Epoch:130] Train Loss:1.1052 | Valid Acc:0.3789 | Test Acc:0.3737
[Epoch:140] Train Loss:1.0826 | Valid Acc:0.3737 | Test Acc:0.3638
[Epoch:150] Train Loss:1.0415 | Valid Acc:0.3789 | Test Acc:0.3566
[Epoch:160] Train Loss:1.0153 | Valid Acc:0.3763 | Test Acc:0.3658
[Epoch:170] Train Loss:0.9638 | Valid Acc:0.3711 | Test Acc:0.3586
[Epoch:180] Train Loss:0.9176 | Valid Acc:0.3632 | Test Acc:0.3638
[Epoch:190] Train Loss:0.8763 | Valid Acc:0.3691 | Test Acc:0.3671
[Epoch:200] Train Loss:0.8332 | Valid Acc:0.3645 | Test Acc:0.3539
[Epoch:210] Train Loss:0.7854 | Valid Acc:0.3691 | Test Acc:0.3533
[Epoch:220] Train Loss:0.7343 | Valid Acc:0.3651 | Test Acc:0.3461
[Epoch:230] Train Loss:0.6899 | Valid Acc:0.3592 | Test Acc:0.3408
[Epoch:240] Train Loss:0.6612 | Valid Acc:0.3599 | Test Acc:0.3474
[Epoch:250] Train Loss:0.6064 | Valid Acc:0.3664 | Test Acc:0.3401
[Epoch:260] Train Loss:0.5772 | Valid Acc:0.3599 | Test Acc:0.3336
[Epoch:270] Train Loss:0.5458 | Valid Acc:0.3645 | Test Acc:0.3408
[Epoch:280] Train Loss:0.5195 | Valid Acc:0.3664 | Test Acc:0.3447
[Epoch:290] Train Loss:0.4798 | Valid Acc:0.3586 | Test Acc:0.3421
[Epoch:300] Train Loss:0.4769 | Valid Acc:0.3520 | Test Acc:0.3408
[Epoch:310] Train Loss:0.4377 | Valid Acc:0.3526 | Test Acc:0.3388
[Epoch:320] Train Loss:0.4273 | Valid Acc:0.3612 | Test Acc:0.3408
[Epoch:330] Train Loss:0.4198 | Valid Acc:0.3566 | Test Acc:0.3414
[Epoch:340] Train Loss:0.4045 | Valid Acc:0.3625 | Test Acc:0.3441
[Epoch:350] Train Loss:0.3979 | Valid Acc:0.3513 | Test Acc:0.3368
[Epoch:360] Train Loss:0.3893 | Valid Acc:0.3539 | Test Acc:0.3454
[Epoch:370] Train Loss:0.3792 | Valid Acc:0.3559 | Test Acc:0.3467
[Epoch:380] Train Loss:0.3751 | Valid Acc:0.3507 | Test Acc:0.3428
[Epoch:390] Train Loss:0.3661 | Valid Acc:0.3461 | Test Acc:0.3454
[Epoch:400] Train Loss:0.3682 | Valid Acc:0.3500 | Test Acc:0.3461
[Epoch:410] Train Loss:0.3579 | Valid Acc:0.3500 | Test Acc:0.3500
[Epoch:420] Train Loss:0.3548 | Valid Acc:0.3513 | Test Acc:0.3493
[Epoch:430] Train Loss:0.3528 | Valid Acc:0.3513 | Test Acc:0.3526
[Epoch:440] Train Loss:0.3499 | Valid Acc:0.3441 | Test Acc:0.3487
[Epoch:450] Train Loss:0.3415 | Valid Acc:0.3487 | Test Acc:0.3487
[Epoch:460] Train Loss:0.3407 | Valid Acc:0.3480 | Test Acc:0.3487
[Epoch:470] Train Loss:0.3487 | Valid Acc:0.3500 | Test Acc:0.3461
[Epoch:480] Train Loss:0.3343 | Valid Acc:0.3480 | Test Acc:0.3441
[Epoch:490] Train Loss:0.3306 | Valid Acc:0.3520 | Test Acc:0.3480
[Epoch:500] Train Loss:0.3277 | Valid Acc:0.3461 | Test Acc:0.3480
[Epoch:510] Train Loss:0.3325 | Valid Acc:0.3461 | Test Acc:0.3539
[Epoch:520] Train Loss:0.3206 | Valid Acc:0.3513 | Test Acc:0.3520
[Epoch:530] Train Loss:0.3248 | Valid Acc:0.3520 | Test Acc:0.3467
[Epoch:540] Train Loss:0.3158 | Valid Acc:0.3493 | Test Acc:0.3461
[Epoch:550] Train Loss:0.3150 | Valid Acc:0.3507 | Test Acc:0.3447
[Epoch:560] Train Loss:0.3283 | Valid Acc:0.3487 | Test Acc:0.3447
[Epoch:570] Train Loss:0.3148 | Valid Acc:0.3500 | Test Acc:0.3467
[Epoch:580] Train Loss:0.3158 | Valid Acc:0.3480 | Test Acc:0.3507
[Epoch:590] Train Loss:0.3183 | Valid Acc:0.3487 | Test Acc:0.3447
[Epoch:600] Train Loss:0.3113 | Valid Acc:0.3487 | Test Acc:0.3480
[Epoch:610] Train Loss:0.3148 | Valid Acc:0.3474 | Test Acc:0.3474
[Epoch:620] Train Loss:0.3149 | Valid Acc:0.3487 | Test Acc:0.3461
[Epoch:630] Train Loss:0.3121 | Valid Acc:0.3533 | Test Acc:0.3447
[Epoch:640] Train Loss:0.3124 | Valid Acc:0.3487 | Test Acc:0.3480
[Epoch:650] Train Loss:0.3108 | Valid Acc:0.3487 | Test Acc:0.3480
[Epoch:660] Train Loss:0.3038 | Valid Acc:0.3447 | Test Acc:0.3507
[Epoch:670] Train Loss:0.3029 | Valid Acc:0.3487 | Test Acc:0.3461
[Epoch:680] Train Loss:0.3079 | Valid Acc:0.3467 | Test Acc:0.3461
[Epoch:690] Train Loss:0.3071 | Valid Acc:0.3487 | Test Acc:0.3441
[Epoch:700] Train Loss:0.3087 | Valid Acc:0.3467 | Test Acc:0.3454
[Epoch:710] Train Loss:0.3068 | Valid Acc:0.3461 | Test Acc:0.3493
[Epoch:720] Train Loss:0.3116 | Valid Acc:0.3474 | Test Acc:0.3467
[Epoch:730] Train Loss:0.3067 | Valid Acc:0.3487 | Test Acc:0.3461
[Epoch:740] Train Loss:0.3081 | Valid Acc:0.3480 | Test Acc:0.3467
[Epoch:750] Train Loss:0.3097 | Valid Acc:0.3474 | Test Acc:0.3493
[Epoch:760] Train Loss:0.3126 | Valid Acc:0.3467 | Test Acc:0.3480
[Epoch:770] Train Loss:0.3051 | Valid Acc:0.3461 | Test Acc:0.3487
[Epoch:780] Train Loss:0.3079 | Valid Acc:0.3467 | Test Acc:0.3480
[Epoch:790] Train Loss:0.3112 | Valid Acc:0.3467 | Test Acc:0.3480
Best results: epoch=59, valid loss=1.3353, valid acc=0.4092, test loss:1.3428,test acc=0.4039
Calculate standard: times:0, valid_Acc:0.40921052631578947, test_acc:0.4039473684210526
[Epoch:0] Train Loss:1.6525 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:10] Train Loss:1.5717 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:20] Train Loss:1.5601 | Valid Acc:0.2664 | Test Acc:0.2599
[Epoch:30] Train Loss:1.5008 | Valid Acc:0.3112 | Test Acc:0.3013
[Epoch:40] Train Loss:1.4177 | Valid Acc:0.3612 | Test Acc:0.3862
[Epoch:50] Train Loss:1.3513 | Valid Acc:0.3961 | Test Acc:0.4112
[Epoch:60] Train Loss:1.3102 | Valid Acc:0.4072 | Test Acc:0.4007
[Epoch:70] Train Loss:1.2742 | Valid Acc:0.4026 | Test Acc:0.3954
[Epoch:80] Train Loss:1.2483 | Valid Acc:0.3974 | Test Acc:0.3974
[Epoch:90] Train Loss:1.2112 | Valid Acc:0.3789 | Test Acc:0.3875
[Epoch:100] Train Loss:1.2021 | Valid Acc:0.3789 | Test Acc:0.3816
[Epoch:110] Train Loss:1.1531 | Valid Acc:0.3658 | Test Acc:0.3645
[Epoch:120] Train Loss:1.1450 | Valid Acc:0.3750 | Test Acc:0.3750
[Epoch:130] Train Loss:1.1136 | Valid Acc:0.3770 | Test Acc:0.3717
[Epoch:140] Train Loss:1.0871 | Valid Acc:0.3816 | Test Acc:0.3664
[Epoch:150] Train Loss:1.0497 | Valid Acc:0.3789 | Test Acc:0.3579
[Epoch:160] Train Loss:1.0367 | Valid Acc:0.3763 | Test Acc:0.3605
[Epoch:170] Train Loss:0.9818 | Valid Acc:0.3809 | Test Acc:0.3612
[Epoch:180] Train Loss:0.9354 | Valid Acc:0.3684 | Test Acc:0.3539
[Epoch:190] Train Loss:0.9014 | Valid Acc:0.3717 | Test Acc:0.3599
[Epoch:200] Train Loss:0.8539 | Valid Acc:0.3730 | Test Acc:0.3612
[Epoch:210] Train Loss:0.8216 | Valid Acc:0.3711 | Test Acc:0.3546
[Epoch:220] Train Loss:0.7660 | Valid Acc:0.3711 | Test Acc:0.3592
[Epoch:230] Train Loss:0.7265 | Valid Acc:0.3678 | Test Acc:0.3480
[Epoch:240] Train Loss:0.6870 | Valid Acc:0.3612 | Test Acc:0.3408
[Epoch:250] Train Loss:0.6399 | Valid Acc:0.3612 | Test Acc:0.3454
[Epoch:260] Train Loss:0.6074 | Valid Acc:0.3671 | Test Acc:0.3539
[Epoch:270] Train Loss:0.5819 | Valid Acc:0.3612 | Test Acc:0.3520
[Epoch:280] Train Loss:0.5455 | Valid Acc:0.3697 | Test Acc:0.3605
[Epoch:290] Train Loss:0.5141 | Valid Acc:0.3638 | Test Acc:0.3461
[Epoch:300] Train Loss:0.5021 | Valid Acc:0.3632 | Test Acc:0.3500
[Epoch:310] Train Loss:0.4643 | Valid Acc:0.3520 | Test Acc:0.3454
[Epoch:320] Train Loss:0.4564 | Valid Acc:0.3559 | Test Acc:0.3546
[Epoch:330] Train Loss:0.4368 | Valid Acc:0.3572 | Test Acc:0.3533
[Epoch:340] Train Loss:0.4224 | Valid Acc:0.3605 | Test Acc:0.3605
[Epoch:350] Train Loss:0.4113 | Valid Acc:0.3599 | Test Acc:0.3566
[Epoch:360] Train Loss:0.4064 | Valid Acc:0.3599 | Test Acc:0.3546
[Epoch:370] Train Loss:0.3948 | Valid Acc:0.3487 | Test Acc:0.3579
[Epoch:380] Train Loss:0.3900 | Valid Acc:0.3480 | Test Acc:0.3579
[Epoch:390] Train Loss:0.3828 | Valid Acc:0.3487 | Test Acc:0.3546
[Epoch:400] Train Loss:0.3776 | Valid Acc:0.3507 | Test Acc:0.3513
[Epoch:410] Train Loss:0.3746 | Valid Acc:0.3493 | Test Acc:0.3539
[Epoch:420] Train Loss:0.3598 | Valid Acc:0.3454 | Test Acc:0.3507
[Epoch:430] Train Loss:0.3587 | Valid Acc:0.3480 | Test Acc:0.3500
[Epoch:440] Train Loss:0.3610 | Valid Acc:0.3533 | Test Acc:0.3566
[Epoch:450] Train Loss:0.3524 | Valid Acc:0.3434 | Test Acc:0.3572
[Epoch:460] Train Loss:0.3488 | Valid Acc:0.3493 | Test Acc:0.3612
[Epoch:470] Train Loss:0.3544 | Valid Acc:0.3467 | Test Acc:0.3546
[Epoch:480] Train Loss:0.3512 | Valid Acc:0.3467 | Test Acc:0.3493
[Epoch:490] Train Loss:0.3373 | Valid Acc:0.3428 | Test Acc:0.3480
[Epoch:500] Train Loss:0.3340 | Valid Acc:0.3414 | Test Acc:0.3526
[Epoch:510] Train Loss:0.3415 | Valid Acc:0.3480 | Test Acc:0.3467
[Epoch:520] Train Loss:0.3310 | Valid Acc:0.3408 | Test Acc:0.3533
[Epoch:530] Train Loss:0.3303 | Valid Acc:0.3493 | Test Acc:0.3474
[Epoch:540] Train Loss:0.3316 | Valid Acc:0.3467 | Test Acc:0.3507
[Epoch:550] Train Loss:0.3213 | Valid Acc:0.3513 | Test Acc:0.3500
[Epoch:560] Train Loss:0.3291 | Valid Acc:0.3493 | Test Acc:0.3461
[Epoch:570] Train Loss:0.3228 | Valid Acc:0.3487 | Test Acc:0.3500
[Epoch:580] Train Loss:0.3268 | Valid Acc:0.3454 | Test Acc:0.3507
[Epoch:590] Train Loss:0.3286 | Valid Acc:0.3480 | Test Acc:0.3493
[Epoch:600] Train Loss:0.3219 | Valid Acc:0.3454 | Test Acc:0.3454
[Epoch:610] Train Loss:0.3258 | Valid Acc:0.3474 | Test Acc:0.3507
[Epoch:620] Train Loss:0.3225 | Valid Acc:0.3507 | Test Acc:0.3474
[Epoch:630] Train Loss:0.3182 | Valid Acc:0.3493 | Test Acc:0.3474
[Epoch:640] Train Loss:0.3214 | Valid Acc:0.3553 | Test Acc:0.3467
[Epoch:650] Train Loss:0.3155 | Valid Acc:0.3507 | Test Acc:0.3467
[Epoch:660] Train Loss:0.3143 | Valid Acc:0.3526 | Test Acc:0.3480
[Epoch:670] Train Loss:0.3136 | Valid Acc:0.3526 | Test Acc:0.3500
[Epoch:680] Train Loss:0.3160 | Valid Acc:0.3520 | Test Acc:0.3487
[Epoch:690] Train Loss:0.3121 | Valid Acc:0.3513 | Test Acc:0.3480
[Epoch:700] Train Loss:0.3184 | Valid Acc:0.3533 | Test Acc:0.3461
[Epoch:710] Train Loss:0.3170 | Valid Acc:0.3546 | Test Acc:0.3500
[Epoch:720] Train Loss:0.3150 | Valid Acc:0.3493 | Test Acc:0.3507
[Epoch:730] Train Loss:0.3176 | Valid Acc:0.3493 | Test Acc:0.3480
[Epoch:740] Train Loss:0.3200 | Valid Acc:0.3520 | Test Acc:0.3493
[Epoch:750] Train Loss:0.3137 | Valid Acc:0.3520 | Test Acc:0.3493
[Epoch:760] Train Loss:0.3114 | Valid Acc:0.3513 | Test Acc:0.3500
[Epoch:770] Train Loss:0.3077 | Valid Acc:0.3513 | Test Acc:0.3500
[Epoch:780] Train Loss:0.3184 | Valid Acc:0.3507 | Test Acc:0.3500
[Epoch:790] Train Loss:0.3113 | Valid Acc:0.3507 | Test Acc:0.3500
Best results: epoch=57, valid loss=1.3320, valid acc=0.4138, test loss:1.3378,test acc=0.4066
Calculate standard: times:1, valid_Acc:0.4138157894736842, test_acc:0.40657894736842104
[Epoch:0] Train Loss:1.6525 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:10] Train Loss:1.5717 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:20] Train Loss:1.5601 | Valid Acc:0.2704 | Test Acc:0.2632
[Epoch:30] Train Loss:1.5037 | Valid Acc:0.3197 | Test Acc:0.3151
[Epoch:40] Train Loss:1.4215 | Valid Acc:0.3730 | Test Acc:0.3743
[Epoch:50] Train Loss:1.3472 | Valid Acc:0.3882 | Test Acc:0.4099
[Epoch:60] Train Loss:1.3108 | Valid Acc:0.4039 | Test Acc:0.4000
[Epoch:70] Train Loss:1.2745 | Valid Acc:0.4046 | Test Acc:0.3993
[Epoch:80] Train Loss:1.2424 | Valid Acc:0.3849 | Test Acc:0.3934
[Epoch:90] Train Loss:1.2093 | Valid Acc:0.3770 | Test Acc:0.3875
[Epoch:100] Train Loss:1.1926 | Valid Acc:0.3757 | Test Acc:0.3816
[Epoch:110] Train Loss:1.1548 | Valid Acc:0.3664 | Test Acc:0.3651
[Epoch:120] Train Loss:1.1336 | Valid Acc:0.3651 | Test Acc:0.3697
[Epoch:130] Train Loss:1.0956 | Valid Acc:0.3638 | Test Acc:0.3671
[Epoch:140] Train Loss:1.0598 | Valid Acc:0.3743 | Test Acc:0.3605
[Epoch:150] Train Loss:1.0259 | Valid Acc:0.3711 | Test Acc:0.3618
[Epoch:160] Train Loss:0.9986 | Valid Acc:0.3757 | Test Acc:0.3592
[Epoch:170] Train Loss:0.9559 | Valid Acc:0.3711 | Test Acc:0.3539
[Epoch:180] Train Loss:0.9123 | Valid Acc:0.3684 | Test Acc:0.3605
[Epoch:190] Train Loss:0.8654 | Valid Acc:0.3678 | Test Acc:0.3645
[Epoch:200] Train Loss:0.8195 | Valid Acc:0.3618 | Test Acc:0.3566
[Epoch:210] Train Loss:0.7834 | Valid Acc:0.3651 | Test Acc:0.3579
[Epoch:220] Train Loss:0.7352 | Valid Acc:0.3632 | Test Acc:0.3461
[Epoch:230] Train Loss:0.6922 | Valid Acc:0.3553 | Test Acc:0.3401
[Epoch:240] Train Loss:0.6492 | Valid Acc:0.3612 | Test Acc:0.3382
[Epoch:250] Train Loss:0.6142 | Valid Acc:0.3559 | Test Acc:0.3447
[Epoch:260] Train Loss:0.5818 | Valid Acc:0.3632 | Test Acc:0.3467
[Epoch:270] Train Loss:0.5430 | Valid Acc:0.3638 | Test Acc:0.3493
[Epoch:280] Train Loss:0.5117 | Valid Acc:0.3638 | Test Acc:0.3375
[Epoch:290] Train Loss:0.4863 | Valid Acc:0.3579 | Test Acc:0.3421
[Epoch:300] Train Loss:0.4794 | Valid Acc:0.3651 | Test Acc:0.3461
[Epoch:310] Train Loss:0.4504 | Valid Acc:0.3526 | Test Acc:0.3421
[Epoch:320] Train Loss:0.4353 | Valid Acc:0.3599 | Test Acc:0.3474
[Epoch:330] Train Loss:0.4211 | Valid Acc:0.3533 | Test Acc:0.3507
[Epoch:340] Train Loss:0.4032 | Valid Acc:0.3586 | Test Acc:0.3493
[Epoch:350] Train Loss:0.3968 | Valid Acc:0.3533 | Test Acc:0.3467
[Epoch:360] Train Loss:0.3901 | Valid Acc:0.3539 | Test Acc:0.3539
[Epoch:370] Train Loss:0.3763 | Valid Acc:0.3566 | Test Acc:0.3507
[Epoch:380] Train Loss:0.3779 | Valid Acc:0.3526 | Test Acc:0.3546
[Epoch:390] Train Loss:0.3701 | Valid Acc:0.3474 | Test Acc:0.3559
[Epoch:400] Train Loss:0.3709 | Valid Acc:0.3520 | Test Acc:0.3507
[Epoch:410] Train Loss:0.3651 | Valid Acc:0.3539 | Test Acc:0.3493
[Epoch:420] Train Loss:0.3484 | Valid Acc:0.3559 | Test Acc:0.3507
[Epoch:430] Train Loss:0.3541 | Valid Acc:0.3553 | Test Acc:0.3467
[Epoch:440] Train Loss:0.3550 | Valid Acc:0.3586 | Test Acc:0.3454
[Epoch:450] Train Loss:0.3404 | Valid Acc:0.3546 | Test Acc:0.3493
[Epoch:460] Train Loss:0.3406 | Valid Acc:0.3559 | Test Acc:0.3513
[Epoch:470] Train Loss:0.3465 | Valid Acc:0.3500 | Test Acc:0.3559
[Epoch:480] Train Loss:0.3357 | Valid Acc:0.3539 | Test Acc:0.3454
[Epoch:490] Train Loss:0.3298 | Valid Acc:0.3480 | Test Acc:0.3461
[Epoch:500] Train Loss:0.3261 | Valid Acc:0.3447 | Test Acc:0.3520
[Epoch:510] Train Loss:0.3328 | Valid Acc:0.3487 | Test Acc:0.3533
[Epoch:520] Train Loss:0.3282 | Valid Acc:0.3474 | Test Acc:0.3487
[Epoch:530] Train Loss:0.3213 | Valid Acc:0.3493 | Test Acc:0.3546
[Epoch:540] Train Loss:0.3180 | Valid Acc:0.3434 | Test Acc:0.3513
[Epoch:550] Train Loss:0.3216 | Valid Acc:0.3461 | Test Acc:0.3454
[Epoch:560] Train Loss:0.3260 | Valid Acc:0.3461 | Test Acc:0.3507
[Epoch:570] Train Loss:0.3133 | Valid Acc:0.3454 | Test Acc:0.3487
[Epoch:580] Train Loss:0.3163 | Valid Acc:0.3467 | Test Acc:0.3500
[Epoch:590] Train Loss:0.3194 | Valid Acc:0.3500 | Test Acc:0.3507
[Epoch:600] Train Loss:0.3161 | Valid Acc:0.3500 | Test Acc:0.3533
[Epoch:610] Train Loss:0.3167 | Valid Acc:0.3461 | Test Acc:0.3520
[Epoch:620] Train Loss:0.3142 | Valid Acc:0.3526 | Test Acc:0.3553
[Epoch:630] Train Loss:0.3129 | Valid Acc:0.3507 | Test Acc:0.3513
[Epoch:640] Train Loss:0.3105 | Valid Acc:0.3553 | Test Acc:0.3553
[Epoch:650] Train Loss:0.3118 | Valid Acc:0.3526 | Test Acc:0.3526
[Epoch:660] Train Loss:0.3057 | Valid Acc:0.3526 | Test Acc:0.3566
[Epoch:670] Train Loss:0.3066 | Valid Acc:0.3487 | Test Acc:0.3526
[Epoch:680] Train Loss:0.3078 | Valid Acc:0.3520 | Test Acc:0.3533
[Epoch:690] Train Loss:0.3110 | Valid Acc:0.3526 | Test Acc:0.3480
[Epoch:700] Train Loss:0.3121 | Valid Acc:0.3480 | Test Acc:0.3520
[Epoch:710] Train Loss:0.3097 | Valid Acc:0.3513 | Test Acc:0.3546
[Epoch:720] Train Loss:0.3137 | Valid Acc:0.3526 | Test Acc:0.3526
[Epoch:730] Train Loss:0.3141 | Valid Acc:0.3526 | Test Acc:0.3513
[Epoch:740] Train Loss:0.3124 | Valid Acc:0.3487 | Test Acc:0.3520
[Epoch:750] Train Loss:0.3090 | Valid Acc:0.3526 | Test Acc:0.3526
[Epoch:760] Train Loss:0.3128 | Valid Acc:0.3513 | Test Acc:0.3520
[Epoch:770] Train Loss:0.3088 | Valid Acc:0.3520 | Test Acc:0.3520
[Epoch:780] Train Loss:0.3141 | Valid Acc:0.3526 | Test Acc:0.3520
[Epoch:790] Train Loss:0.3107 | Valid Acc:0.3526 | Test Acc:0.3520
Best results: epoch=57, valid loss=1.3337, valid acc=0.4112, test loss:1.3393,test acc=0.4118
Calculate standard: times:2, valid_Acc:0.41118421052631576, test_acc:0.4118421052631579
[Epoch:0] Train Loss:1.6525 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:10] Train Loss:1.5717 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:20] Train Loss:1.5600 | Valid Acc:0.2671 | Test Acc:0.2612
[Epoch:30] Train Loss:1.5025 | Valid Acc:0.3138 | Test Acc:0.3059
[Epoch:40] Train Loss:1.4095 | Valid Acc:0.3658 | Test Acc:0.3737
[Epoch:50] Train Loss:1.3510 | Valid Acc:0.4092 | Test Acc:0.4118
[Epoch:60] Train Loss:1.3077 | Valid Acc:0.4026 | Test Acc:0.4000
[Epoch:70] Train Loss:1.2756 | Valid Acc:0.3901 | Test Acc:0.3836
[Epoch:80] Train Loss:1.2450 | Valid Acc:0.3875 | Test Acc:0.3961
[Epoch:90] Train Loss:1.2129 | Valid Acc:0.3730 | Test Acc:0.3803
[Epoch:100] Train Loss:1.2102 | Valid Acc:0.3822 | Test Acc:0.3803
[Epoch:110] Train Loss:1.1471 | Valid Acc:0.3697 | Test Acc:0.3691
[Epoch:120] Train Loss:1.1404 | Valid Acc:0.3842 | Test Acc:0.3829
[Epoch:130] Train Loss:1.1001 | Valid Acc:0.3816 | Test Acc:0.3704
[Epoch:140] Train Loss:1.0704 | Valid Acc:0.3704 | Test Acc:0.3605
[Epoch:150] Train Loss:1.0298 | Valid Acc:0.3770 | Test Acc:0.3671
[Epoch:160] Train Loss:1.0012 | Valid Acc:0.3730 | Test Acc:0.3724
[Epoch:170] Train Loss:0.9473 | Valid Acc:0.3704 | Test Acc:0.3559
[Epoch:180] Train Loss:0.9051 | Valid Acc:0.3763 | Test Acc:0.3684
[Epoch:190] Train Loss:0.8619 | Valid Acc:0.3645 | Test Acc:0.3566
[Epoch:200] Train Loss:0.8240 | Valid Acc:0.3638 | Test Acc:0.3480
[Epoch:210] Train Loss:0.7631 | Valid Acc:0.3697 | Test Acc:0.3566
[Epoch:220] Train Loss:0.7215 | Valid Acc:0.3605 | Test Acc:0.3487
[Epoch:230] Train Loss:0.6730 | Valid Acc:0.3579 | Test Acc:0.3454
[Epoch:240] Train Loss:0.6303 | Valid Acc:0.3671 | Test Acc:0.3375
[Epoch:250] Train Loss:0.5804 | Valid Acc:0.3592 | Test Acc:0.3487
[Epoch:260] Train Loss:0.5596 | Valid Acc:0.3645 | Test Acc:0.3408
[Epoch:270] Train Loss:0.5197 | Valid Acc:0.3579 | Test Acc:0.3454
[Epoch:280] Train Loss:0.4946 | Valid Acc:0.3539 | Test Acc:0.3474
[Epoch:290] Train Loss:0.4690 | Valid Acc:0.3500 | Test Acc:0.3428
[Epoch:300] Train Loss:0.4649 | Valid Acc:0.3586 | Test Acc:0.3428
[Epoch:310] Train Loss:0.4302 | Valid Acc:0.3474 | Test Acc:0.3388
[Epoch:320] Train Loss:0.4172 | Valid Acc:0.3493 | Test Acc:0.3447
[Epoch:330] Train Loss:0.4081 | Valid Acc:0.3421 | Test Acc:0.3395
[Epoch:340] Train Loss:0.3998 | Valid Acc:0.3461 | Test Acc:0.3395
[Epoch:350] Train Loss:0.3844 | Valid Acc:0.3454 | Test Acc:0.3487
[Epoch:360] Train Loss:0.3807 | Valid Acc:0.3368 | Test Acc:0.3454
[Epoch:370] Train Loss:0.3688 | Valid Acc:0.3382 | Test Acc:0.3507
[Epoch:380] Train Loss:0.3656 | Valid Acc:0.3401 | Test Acc:0.3493
[Epoch:390] Train Loss:0.3631 | Valid Acc:0.3414 | Test Acc:0.3467
[Epoch:400] Train Loss:0.3645 | Valid Acc:0.3382 | Test Acc:0.3434
[Epoch:410] Train Loss:0.3532 | Valid Acc:0.3414 | Test Acc:0.3421
[Epoch:420] Train Loss:0.3465 | Valid Acc:0.3401 | Test Acc:0.3434
[Epoch:430] Train Loss:0.3514 | Valid Acc:0.3414 | Test Acc:0.3500
[Epoch:440] Train Loss:0.3455 | Valid Acc:0.3474 | Test Acc:0.3467
[Epoch:450] Train Loss:0.3388 | Valid Acc:0.3447 | Test Acc:0.3421
[Epoch:460] Train Loss:0.3347 | Valid Acc:0.3414 | Test Acc:0.3487
[Epoch:470] Train Loss:0.3420 | Valid Acc:0.3447 | Test Acc:0.3447
[Epoch:480] Train Loss:0.3350 | Valid Acc:0.3454 | Test Acc:0.3388
[Epoch:490] Train Loss:0.3305 | Valid Acc:0.3382 | Test Acc:0.3493
[Epoch:500] Train Loss:0.3247 | Valid Acc:0.3408 | Test Acc:0.3408
[Epoch:510] Train Loss:0.3265 | Valid Acc:0.3408 | Test Acc:0.3480
[Epoch:520] Train Loss:0.3252 | Valid Acc:0.3362 | Test Acc:0.3434
[Epoch:530] Train Loss:0.3193 | Valid Acc:0.3368 | Test Acc:0.3428
[Epoch:540] Train Loss:0.3191 | Valid Acc:0.3382 | Test Acc:0.3480
[Epoch:550] Train Loss:0.3150 | Valid Acc:0.3414 | Test Acc:0.3414
[Epoch:560] Train Loss:0.3290 | Valid Acc:0.3414 | Test Acc:0.3428
[Epoch:570] Train Loss:0.3111 | Valid Acc:0.3414 | Test Acc:0.3447
[Epoch:580] Train Loss:0.3176 | Valid Acc:0.3401 | Test Acc:0.3434
[Epoch:590] Train Loss:0.3160 | Valid Acc:0.3441 | Test Acc:0.3408
[Epoch:600] Train Loss:0.3155 | Valid Acc:0.3461 | Test Acc:0.3441
[Epoch:610] Train Loss:0.3138 | Valid Acc:0.3368 | Test Acc:0.3421
[Epoch:620] Train Loss:0.3149 | Valid Acc:0.3454 | Test Acc:0.3467
[Epoch:630] Train Loss:0.3082 | Valid Acc:0.3441 | Test Acc:0.3454
[Epoch:640] Train Loss:0.3091 | Valid Acc:0.3467 | Test Acc:0.3408
[Epoch:650] Train Loss:0.3133 | Valid Acc:0.3441 | Test Acc:0.3474
[Epoch:660] Train Loss:0.3049 | Valid Acc:0.3428 | Test Acc:0.3434
[Epoch:670] Train Loss:0.3053 | Valid Acc:0.3401 | Test Acc:0.3454
[Epoch:680] Train Loss:0.3044 | Valid Acc:0.3408 | Test Acc:0.3434
[Epoch:690] Train Loss:0.3039 | Valid Acc:0.3428 | Test Acc:0.3454
[Epoch:700] Train Loss:0.3113 | Valid Acc:0.3487 | Test Acc:0.3421
[Epoch:710] Train Loss:0.3069 | Valid Acc:0.3434 | Test Acc:0.3454
[Epoch:720] Train Loss:0.3140 | Valid Acc:0.3434 | Test Acc:0.3454
[Epoch:730] Train Loss:0.3092 | Valid Acc:0.3434 | Test Acc:0.3441
[Epoch:740] Train Loss:0.3100 | Valid Acc:0.3414 | Test Acc:0.3421
[Epoch:750] Train Loss:0.3034 | Valid Acc:0.3401 | Test Acc:0.3441
[Epoch:760] Train Loss:0.3117 | Valid Acc:0.3434 | Test Acc:0.3447
[Epoch:770] Train Loss:0.3044 | Valid Acc:0.3434 | Test Acc:0.3461
[Epoch:780] Train Loss:0.3095 | Valid Acc:0.3434 | Test Acc:0.3461
[Epoch:790] Train Loss:0.3054 | Valid Acc:0.3434 | Test Acc:0.3461
Best results: epoch=54, valid loss=1.3340, valid acc=0.4145, test loss:1.3388,test acc=0.4112
Calculate standard: times:3, valid_Acc:0.4144736842105263, test_acc:0.41118421052631576
[Epoch:0] Train Loss:1.6525 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:10] Train Loss:1.5718 | Valid Acc:0.2586 | Test Acc:0.2586
[Epoch:20] Train Loss:1.5603 | Valid Acc:0.2651 | Test Acc:0.2605
[Epoch:30] Train Loss:1.5002 | Valid Acc:0.2796 | Test Acc:0.2776
[Epoch:40] Train Loss:1.4083 | Valid Acc:0.3678 | Test Acc:0.3586
[Epoch:50] Train Loss:1.3460 | Valid Acc:0.3875 | Test Acc:0.3888
[Epoch:60] Train Loss:1.3081 | Valid Acc:0.3980 | Test Acc:0.4033
[Epoch:70] Train Loss:1.2686 | Valid Acc:0.3987 | Test Acc:0.3967
[Epoch:80] Train Loss:1.2470 | Valid Acc:0.3842 | Test Acc:0.3901
[Epoch:90] Train Loss:1.2219 | Valid Acc:0.3868 | Test Acc:0.3980
[Epoch:100] Train Loss:1.2060 | Valid Acc:0.3783 | Test Acc:0.3816
[Epoch:110] Train Loss:1.1533 | Valid Acc:0.3704 | Test Acc:0.3592
[Epoch:120] Train Loss:1.1412 | Valid Acc:0.3691 | Test Acc:0.3704
[Epoch:130] Train Loss:1.1100 | Valid Acc:0.3651 | Test Acc:0.3645
[Epoch:140] Train Loss:1.0776 | Valid Acc:0.3763 | Test Acc:0.3632
[Epoch:150] Train Loss:1.0451 | Valid Acc:0.3704 | Test Acc:0.3592
[Epoch:160] Train Loss:1.0191 | Valid Acc:0.3743 | Test Acc:0.3572
[Epoch:170] Train Loss:0.9730 | Valid Acc:0.3763 | Test Acc:0.3553
[Epoch:180] Train Loss:0.9314 | Valid Acc:0.3711 | Test Acc:0.3605
[Epoch:190] Train Loss:0.8914 | Valid Acc:0.3625 | Test Acc:0.3559
[Epoch:200] Train Loss:0.8432 | Valid Acc:0.3684 | Test Acc:0.3592
[Epoch:210] Train Loss:0.8157 | Valid Acc:0.3599 | Test Acc:0.3480
[Epoch:220] Train Loss:0.7700 | Valid Acc:0.3638 | Test Acc:0.3500
[Epoch:230] Train Loss:0.7198 | Valid Acc:0.3618 | Test Acc:0.3526
[Epoch:240] Train Loss:0.6698 | Valid Acc:0.3632 | Test Acc:0.3441
[Epoch:250] Train Loss:0.6282 | Valid Acc:0.3625 | Test Acc:0.3507
[Epoch:260] Train Loss:0.5932 | Valid Acc:0.3605 | Test Acc:0.3447
[Epoch:270] Train Loss:0.5570 | Valid Acc:0.3612 | Test Acc:0.3428
[Epoch:280] Train Loss:0.5209 | Valid Acc:0.3612 | Test Acc:0.3461
[Epoch:290] Train Loss:0.4980 | Valid Acc:0.3612 | Test Acc:0.3395
[Epoch:300] Train Loss:0.4866 | Valid Acc:0.3605 | Test Acc:0.3408
[Epoch:310] Train Loss:0.4490 | Valid Acc:0.3651 | Test Acc:0.3408
[Epoch:320] Train Loss:0.4380 | Valid Acc:0.3651 | Test Acc:0.3467
[Epoch:330] Train Loss:0.4264 | Valid Acc:0.3605 | Test Acc:0.3507
[Epoch:340] Train Loss:0.4104 | Valid Acc:0.3553 | Test Acc:0.3520
[Epoch:350] Train Loss:0.4024 | Valid Acc:0.3539 | Test Acc:0.3454
[Epoch:360] Train Loss:0.3957 | Valid Acc:0.3546 | Test Acc:0.3487
[Epoch:370] Train Loss:0.3875 | Valid Acc:0.3513 | Test Acc:0.3539
[Epoch:380] Train Loss:0.3836 | Valid Acc:0.3500 | Test Acc:0.3572
[Epoch:390] Train Loss:0.3741 | Valid Acc:0.3500 | Test Acc:0.3533
[Epoch:400] Train Loss:0.3725 | Valid Acc:0.3520 | Test Acc:0.3513
[Epoch:410] Train Loss:0.3702 | Valid Acc:0.3507 | Test Acc:0.3533
[Epoch:420] Train Loss:0.3590 | Valid Acc:0.3461 | Test Acc:0.3586
[Epoch:430] Train Loss:0.3563 | Valid Acc:0.3480 | Test Acc:0.3513
[Epoch:440] Train Loss:0.3601 | Valid Acc:0.3546 | Test Acc:0.3513
[Epoch:450] Train Loss:0.3487 | Valid Acc:0.3493 | Test Acc:0.3480
[Epoch:460] Train Loss:0.3451 | Valid Acc:0.3507 | Test Acc:0.3513
[Epoch:470] Train Loss:0.3518 | Valid Acc:0.3539 | Test Acc:0.3480
[Epoch:480] Train Loss:0.3382 | Valid Acc:0.3553 | Test Acc:0.3520
[Epoch:490] Train Loss:0.3348 | Valid Acc:0.3500 | Test Acc:0.3474
[Epoch:500] Train Loss:0.3347 | Valid Acc:0.3553 | Test Acc:0.3513
[Epoch:510] Train Loss:0.3367 | Valid Acc:0.3539 | Test Acc:0.3553
[Epoch:520] Train Loss:0.3276 | Valid Acc:0.3572 | Test Acc:0.3487
[Epoch:530] Train Loss:0.3282 | Valid Acc:0.3493 | Test Acc:0.3480
[Epoch:540] Train Loss:0.3195 | Valid Acc:0.3539 | Test Acc:0.3500
[Epoch:550] Train Loss:0.3238 | Valid Acc:0.3533 | Test Acc:0.3474
[Epoch:560] Train Loss:0.3320 | Valid Acc:0.3579 | Test Acc:0.3480
[Epoch:570] Train Loss:0.3191 | Valid Acc:0.3500 | Test Acc:0.3500
[Epoch:580] Train Loss:0.3222 | Valid Acc:0.3559 | Test Acc:0.3474
[Epoch:590] Train Loss:0.3211 | Valid Acc:0.3520 | Test Acc:0.3461
[Epoch:600] Train Loss:0.3168 | Valid Acc:0.3500 | Test Acc:0.3461
[Epoch:610] Train Loss:0.3184 | Valid Acc:0.3487 | Test Acc:0.3467
[Epoch:620] Train Loss:0.3173 | Valid Acc:0.3533 | Test Acc:0.3507
[Epoch:630] Train Loss:0.3138 | Valid Acc:0.3493 | Test Acc:0.3461
[Epoch:640] Train Loss:0.3153 | Valid Acc:0.3572 | Test Acc:0.3474
[Epoch:650] Train Loss:0.3140 | Valid Acc:0.3553 | Test Acc:0.3461
[Epoch:660] Train Loss:0.3142 | Valid Acc:0.3520 | Test Acc:0.3467
[Epoch:670] Train Loss:0.3094 | Valid Acc:0.3520 | Test Acc:0.3461
[Epoch:680] Train Loss:0.3096 | Valid Acc:0.3507 | Test Acc:0.3500
[Epoch:690] Train Loss:0.3122 | Valid Acc:0.3520 | Test Acc:0.3520
[Epoch:700] Train Loss:0.3139 | Valid Acc:0.3520 | Test Acc:0.3467
[Epoch:710] Train Loss:0.3120 | Valid Acc:0.3546 | Test Acc:0.3474
[Epoch:720] Train Loss:0.3171 | Valid Acc:0.3526 | Test Acc:0.3467
[Epoch:730] Train Loss:0.3148 | Valid Acc:0.3513 | Test Acc:0.3487
[Epoch:740] Train Loss:0.3097 | Valid Acc:0.3493 | Test Acc:0.3461
[Epoch:750] Train Loss:0.3100 | Valid Acc:0.3507 | Test Acc:0.3467
[Epoch:760] Train Loss:0.3170 | Valid Acc:0.3500 | Test Acc:0.3493
[Epoch:770] Train Loss:0.3073 | Valid Acc:0.3500 | Test Acc:0.3480
[Epoch:780] Train Loss:0.3134 | Valid Acc:0.3493 | Test Acc:0.3487
[Epoch:790] Train Loss:0.3132 | Valid Acc:0.3493 | Test Acc:0.3487
Best results: epoch=52, valid loss=1.3364, valid acc=0.4125, test loss:1.3419,test acc=0.4105
Calculate standard: times:4, valid_Acc:0.4125, test_acc:0.4105263157894737
Test results 5 times:0.4088+-0.0030. Valid results 5 times:0.4122+-0.0019
Finsh tunining 1 archs, saved in tuned_res/actor_res_20241021-212104_res-20241021-212059-eps0.0-reg0.0005.pkl
************************* TUNE DONE *************************

========================= EVALUATION AGENT END =========================


========================= RESPONSE AGENT =========================

{"predict_results": "the gnn achieved a test accuracy of 0.4088 with a standard deviation of 0.0030, and a validation accuracy of 0.4122 with a standard deviation of 0.0019.", "searched_gnn": "the searched gnn architecture consists of 3 gatconv layers followed by a combination of zero, identity, and mean/sum aggregations.","hyper_parameters": "the optimized hyper-parameters for the gnn model include an activation function of 'relu', a dropout rate of 0.2, a hidden size of 128, a learning rate of 0.002116974073439665, using the 'adam' optimizer with weight decay of 0.0001037664689191369.", "resource_consuming": "during the learning process, the gnn consumed approximately 709.81 seconds of computational resources."}

========================= RESPONSE AGENT END =========================


========================= REVISION LOOP =========================

{
    "answer": "no",
    "reason": "The evaluation metrics from the experiments indicate that the accuracy is significantly high, particularly around 40.88% for the actor dataset and higher for others, suggesting that the current architecture and hyperparameters are likely well-suited to the task. Therefore, further revision loops may not be necessary."
}
*** No revision loop needed ***

========================= REVISION LOOP END =========================
