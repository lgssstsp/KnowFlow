{
    "name": "GATConv",
    "description": "The graph attentional operator from the \"Graph Attention Networks\" paper.",
    "link": "../generated/torch_geometric.nn.conv.GATConv.html#torch_geometric.nn.conv.GATConv",
    "paper_link": "https://arxiv.org/abs/1710.10903",
    "paper_name": "\"Graph Attention Networks\"",
    "Local Paper PDF Path": "knowledge_base\\pyg_info\\nn\\GATConv.pdf",
    "Paper Summary": "{    \"GNN_Design\": {        \"agg_ops\": \"Attention mechanism applied to node neighborhoods. Uses a linear transformation and attention coefficients are computed using a single-layer feedforward neural network with a LeakyReLU nonlinearity.\",        \"skip_connections\": \"Skip connections applied across intermediate attention layers in the inductive setup.\",        \"layer_info_fusion\": \"Multi-head attention with features concatenated or averaged depending on the layer. Final layer uses averaging.\",        \"num_layers\": \"Models use either two or three layers, depending on the experiment.\",        \"hyperparameters\": \"Multi-head attention: K=8 heads for the first layer; L2 regularization Î»=0.0005; Dropout rate p=0.6.\",        \"activation\": \"Exponential Linear Unit (ELU) activation function applied after the first layer.\"    },    \"Experimental_Setup\": {        \"datasets\": \"Cora, Citeseer, Pubmed, and PPI.\",        \"dataset_summary\": {            \"Cora\": {                \"task\": \"Transductive\",                \"nodes\": 2708,                \"edges\": 5429,                \"features_per_node\": 1433,                \"classes\": 7,                \"training_nodes\": 140,                \"validation_nodes\": 500,                \"test_nodes\": 1000            },            \"Citeseer\": {                \"task\": \"Transductive\",                \"nodes\": 3327,                \"edges\": 4732,                \"features_per_node\": 3703,                \"classes\": 6,                \"training_nodes\": 120,                \"validation_nodes\": 500,                \"test_nodes\": 1000            },            \"Pubmed\": {                \"task\": \"Transductive\",                \"nodes\": 19717,                \"edges\": 44338,                \"features_per_node\": 500,                \"classes\": 3,                \"training_nodes\": 60,                \"validation_nodes\": 500,                \"test_nodes\": 1000            },            \"PPI\": {                \"task\": \"Inductive\",                \"nodes\": 56944 (across 24 graphs),                \"edges\": 818716,                \"features_per_node\": 50,                \"classes\": 121,                \"training_nodes\": 44906 (20 graphs),                \"validation_nodes\": 6514 (2 graphs),                \"test_nodes\": 5524 (2 graphs)            }        },        \"baseline\": {            \"Transductive\": [                \"Label Propagation\",                \"Semi-supervised Embedding\",                \"Manifold Regularization\",                \"DeepWalk\",                \"Iterative Classification Algorithm\",                \"Planetoid\",                \"GCN\",                \"Chebyshev filters\",                \"MoNet\"            ],            \"Inductive\": [                \"GraphSAGE-GCN\",                \"GraphSAGE-mean\",                \"GraphSAGE-LSTM\",                \"GraphSAGE-pool\"            ]        },        \"performance_comparisons\": {            \"Transductive\": {                \"Cora\": {                    \"results\": \"GAT: 83.0% accuracy, improving over GCN: 81.5%\"                },                \"Citeseer\": {                    \"results\": \"GAT: 72.5% accuracy, improving over GCN: 70.3%\"                },                \"Pubmed\": {                    \"results\": \"GAT: 79.0% accuracy, matching GCN\"                }            },            \"Inductive\": {                \"PPI\": {                    \"results\": \"GAT: 0.973 F1 score; improves over GraphSAGE: 0.612 max F1 score\"                }            }        }    }}"
}