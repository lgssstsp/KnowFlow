{
    "name": "GINConv",
    "description": "The graph isomorphism operator from the \"How Powerful are Graph Neural Networks?\" paper.",
    "link": "../generated/torch_geometric.nn.conv.GINConv.html#torch_geometric.nn.conv.GINConv",
    "paper_link": "https://arxiv.org/abs/1810.00826",
    "paper_name": "\"How Powerful are Graph Neural Networks?\"",
    "Local Paper PDF Path": "knowledge_base\\pyg_info\\nn\\GINConv.pdf",
    "Paper Summary": "{  \"GNN_Design\": {    \"agg_ops\": \"The aggregation operators explored include sum, mean, and max-pooling. The sum aggregator was found to be the most powerful as it can represent injective, universal functions over multisets. Mean and max-pooling were found to be less expressive, capturing only distributions and sets of distinct elements, respectively.\",    \"skip_connections\": \"The architecture details do not specify explicit use of skip-connections. However, GIN's approach of considering all layers' outputs through concatenation in the readout phase can implicitly retain features from previous layers.\",    \"layer_info_fusion\": \"Information fusion across layers is achieved through the aggregation and combination steps in each layer, where the node features are recursively updated by aggregating the features of neighboring nodes. The final layer's node embeddings are aggregated through a READOUT function to produce the graph-level embedding.\",    \"num_layers\": \"The models generally used 5 GNN layers, including the input layer.\",    \"hyperparameters\": \"Hyperparameters varied across datasets and included: number of hidden units (16, 32, or 64 based on the dataset), batch size (32 or 128), dropout ratio (0 or 0.5), learning rate (initially 0.01, decayed by 0.5 every 50 epochs). Additional model-specific parameters such as learnable or fixed epsilon (ε) in GIN.\",    \"activation\": \"ReLU activation functions were used following the linear transformations in the 1-layer perceptrons or multi-layer perceptrons (MLPs).\"  },  \"Experimental_Setup\": {    \"datasets\": [      \"MUTAG\",      \"PTC\",      \"NCI1\",      \"PROTEINS\",      \"COLLAB\",      \"IMDB-BINARY\",      \"IMDB-MULTI\",      \"REDDIT-BINARY\",      \"REDDIT-MULTI5K\"    ],    \"dataset_summary\": {      \"MUTAG\": \"A dataset of 188 mutagenic aromatic and heteroaromatic nitro compounds with 7 discrete labels.\",      \"PTC\": \"A dataset of 344 chemical compounds reporting carcinogenicity for male and female rats with 19 discrete labels.\",      \"NCI1\": \"A dataset from the National Cancer Institute consisting of chemical compounds screened for their ability to inhibit the growth of human tumor cell lines, with 37 discrete labels.\",      \"PROTEINS\": \"Nodes are secondary structure elements (SSEs) and edges represent spatial or sequential neighbors in amino-acid sequences, with 3 discrete labels (helix, sheet, or turn).\",      \"COLLAB\": \"A scientific collaboration dataset involving ego-networks of researchers from High Energy Physics, Condensed Matter Physics, and Astro Physics, classified into their respective fields.\",      \"IMDB-BINARY\": \"Ego-networks of actors/actresses classified based on the genre of movies they appear in, with each graph representing a different genre.\",      \"IMDB-MULTI\": \"Similar to IMDB-BINARY but with three classes corresponding to different movie genres.\",      \"REDDIT-BINARY\": \"Graphs representing online discussion threads with users as nodes and edges if one user responds to another, classified into different communities or subreddits.\",      \"REDDIT-MULTI5K\": \"Similar to REDDIT-BINARY but with five classes.\"    },    \"baseline\": [      \"Weisfeiler-Lehman (WL) subtree kernel\",      \"DCNN (Diffusion-convolutional neural networks)\",      \"PATCHY-SAN\",      \"DGCNN (Deep Graph Convolutional Neural Networks)\",      \"AWL (Anonymous Walk Embeddings)\"    ],    \"performance_comparisons\": {      \"MUTAG\": {        \"WL subtree kernel\": \"90.4%\",        \"GIN-0\": \"89.4%\",        \"GIN-ε\": \"89.0%\",        \"GCN\": \"85.6%\",        \"GraphSAGE\": \"85.1%\"      },      \"PTC\": {        \"WL subtree kernel\": \"59.9%\",        \"GIN-0\": \"64.6%\",        \"GIN-ε\": \"63.7%\",        \"GCN\": \"64.2%\",        \"GraphSAGE\": \"63.9%\"      },      \"NCI1\": {        \"WL subtree kernel\": \"86.0%\",        \"GIN-0\": \"82.7%\",        \"GIN-ε\": \"82.7%\",        \"GCN\": \"80.2%\",        \"GraphSAGE\": \"77.7%\"      },      \"PROTEINS\": {        \"WL subtree kernel\": \"75.0%\",        \"GIN-0\": \"76.2%\",        \"GIN-ε\": \"75.9%\",        \"GCN\": \"76.0%\",        \"GraphSAGE\": \"75.9%\"      },      \"COLLAB\": {        \"WL subtree kernel\": \"78.9%\",        \"GIN-0\": \"80.2%\",        \"GIN-ε\": \"80.1%\",        \"GCN\": \"79.0%\"      },      \"IMDB-BINARY\": {        \"WL subtree kernel\": \"73.8%\",        \"GIN-0\": \"75.1%\",        \"GIN-ε\": \"74.3%\",        \"GCN\": \"74.0%\"      },      \"IMDB-MULTI\": {        \"WL subtree kernel\": \"50.9%\",        \"GIN-0\": \"52.3%\",        \"GIN-ε\": \"52.1%\",        \"GCN\": \"51.9%\"      },      \"REDDIT-BINARY\": {        \"WL subtree kernel\": \"81.0%\",        \"GIN-0\": \"92.4%\",        \"GIN-ε\": \"92.2%\",        \"GCN\": \"50.0%\"      },      \"REDDIT-MULTI5K\": {        \"WL subtree kernel\": \"52.5%\",        \"GIN-0\": \"57.5%\",        \"GIN-ε\": \"57.0%\",        \"GCN\": \"20.0%\"      }    }  }}"
}