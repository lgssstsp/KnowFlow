{
    "name": "EGConv",
    "description": "The Efficient Graph Convolution from the \"Adaptive Filters and Aggregator Fusion for Efficient Graph Convolutions\" paper.",
    "link": "../generated/torch_geometric.nn.conv.EGConv.html#torch_geometric.nn.conv.EGConv",
    "paper_link": "https://arxiv.org/abs/2104.01481",
    "paper_name": "\"Adaptive Filters and Aggregator Fusion for Efficient Graph Convolutions\"",
    "Local Paper PDF Path": "knowledge_base\\pyg_info\\nn\\EGConv.pdf",
    "Paper Summary": "{    \"GNN_Design\": {        \"agg_ops\": \"EGC employs adaptive filters with symmetric normalization as the primary aggregation approach. The adaptive filters combine multiple learned basis filters using spatially varying coefficients, providing a localized signal processing perspective.\",        \"skip_connections\": \"No explicit skip connections are mentioned in the EGC design.\",        \"layer_info_fusion\": \"EGC effectively combines information through a basis matrix scheme where weights vary per node and are integrated using learned combination coefficients. The multi-aggregator version (EGC-M) uses several distinct aggregators and fuses them inline at inference time to reduce overhead.\",        \"num_layers\": \"The depth used for effective evaluation isn't explicitly mentioned, but typical GNN models are often evaluated with four layers in literature.\",        \"hyperparameters\": \"EGC uses H = B = 8 for EGC-S and H = B = 4 for EGC-M across experiments, with a consistent parameter count for fairness. The number of heads and bases is tuned to balance parameter efficiency and model expressivity.\",        \"activation\": \"EGC layers benefit from no activation on combination weightings for better performance.\"    },    \"Experimental_Setup\": {        \"datasets\": [            \"ZINC\",            \"CIFAR-10 Superpixels\",            \"Arxiv\",            \"MolHIV\",            \"OGB Code-V2\"        ],        \"dataset_summary\": \"The datasets cover different domains and tasks such as molecular property prediction (ZINC), image classification from superpixels (CIFAR-10 Superpixels), node classification in citation networks (Arxiv), molecular property prediction (MolHIV), and source code processing (OGB Code-V2).\",        \"baseline\": [            \"GCN\",            \"GIN\",            \"GraphSAGE\",            \"GAT\",            \"GATv2\",            \"MPNN-Sum\",            \"MPNN-Max\",            \"PNA\"        ],        \"performance_comparisons\": \"EGC-C and EGC-M outperform recent isotropic and anisotropic models like GAT, PNA on tasks across datasets. EGC showed strong parameter efficiency, consistently outperforming in accuracy or being competitive while utilizing significantly lower resources (memory, inference time). On the Arxiv dataset, EGC-S achieves comparable accuracy with lower parameter count compared to the enlarged baseline models.\"    }}"
}