{
    "name": "CuGraphGATConv",
    "description": "The graph attentional operator from the \"Graph Attention Networks\" paper.",
    "link": "../generated/torch_geometric.nn.conv.CuGraphGATConv.html#torch_geometric.nn.conv.CuGraphGATConv",
    "paper_link": "https://arxiv.org/abs/1710.10903",
    "paper_name": "\"Graph Attention Networks\"",
    "Local Paper PDF Path": "knowledge_base\\pyg_info\\nn\\CuGraphGATConv.pdf",
    "Paper Summary": "{    \"GNN_Design\": {        \"agg_ops\": \"The aggregation operator used in the GAT model is a self-attention mechanism. Specifically, it leverages a shared attentional mechanism to compute attention coefficients which indicate the importance of neighboring nodes.\",        \"skip_connections\": \"Skip connections are observed to be utilized across the intermediate attentional layer, particularly in the inductive learning task. This design helps to stabilize the learning process and aids in training deeper models.\",        \"layer_info_fusion\": \"Information is fused across layers using multi-head attention mechanisms. Each layer’s output is a combination of features from multiple attention heads, either concatenated or averaged depending on the layer position in the network.\",        \"num_layers\": \"The number of layers used varies by task: a two-layer model for transductive learning tasks and a three-layer model for inductive learning tasks.\",        \"hyperparameters\": \"For transductive learning tasks: the first layer has 8 attention heads each computing 8 features, and the second layer has a single head computing class logits. For inductive learning tasks: the first two layers have 4 attention heads each computing 256 features, and the final layer has 6 attention heads computing 121 logits.\",        \"activation\": \"The model applies the exponential linear unit (ELU) nonlinearity after the first layer in the transductive task and utilizes a logistic sigmoid activation after the final layer in the inductive task. LeakyReLU activation with a negative input slope of 0.2 is also used in the attentional mechanism.\"    },    \"Experimental_Setup\": {        \"datasets\": [            \"Cora\",            \"Citeseer\",            \"Pubmed\",            \"PPI\"        ],        \"dataset_summary\": {            \"Cora\": {                \"type\": \"Transductive\",                \"nodes\": 2708,                \"edges\": 5429,                \"features_per_node\": 1433,                \"classes\": 7,                \"training_nodes\": 140,                \"validation_nodes\": 500,                \"test_nodes\": 1000            },            \"Citeseer\": {                \"type\": \"Transductive\",                \"nodes\": 3327,                \"edges\": 4732,                \"features_per_node\": 3703,                \"classes\": 6,                \"training_nodes\": 120,                \"validation_nodes\": 500,                \"test_nodes\": 1000            },            \"Pubmed\": {                \"type\": \"Transductive\",                \"nodes\": 19717,                \"edges\": 44338,                \"features_per_node\": 500,                \"classes\": 3,                \"training_nodes\": 60,                \"validation_nodes\": 500,                \"test_nodes\": 1000            },            \"PPI\": {                \"type\": \"Inductive\",                \"nodes\": 56944 across 24 graphs,                \"edges\": 818716,                \"features_per_node\": 50,                \"classes\": 121 (multi-label),                \"training_nodes\": 44906 across 20 graphs,                \"validation_nodes\": 6514 across 2 graphs,                \"test_nodes\": 5524 across 2 graphs            }        },        \"baseline\": [            \"MLP\",            \"ManiReg\",            \"SemiEmb\",            \"LP\",            \"DeepWalk\",            \"ICA\",            \"Planetoid\",            \"Chebyshev\",            \"GCN\",            \"MoNet\",            \"GraphSAGE\"        ],        \"performance_comparisons\": {            \"Cora\": {                \"GAT\": \"83.0±0.7%\",                \"GCN\": \"81.5%\",                \"MoNet\": \"81.7±0.5%\",                \"Chebyshev\": \"81.2%\",                \"Planetoid\": \"75.7%\",                \"ICA\": \"75.1%\",                \"DeepWalk\": \"67.2%\",                \"LP\": \"68.0%\",                \"MLP\": \"55.1%\",                \"SemiEmb\": \"59.0%\",                \"ManiReg\": \"59.5%\"            },            \"Citeseer\": {                \"GAT\": \"72.5±0.7%\",                \"GCN\": \"70.3%\",                \"MoNet\": \"—\",                \"Chebyshev\": \"69.8%\",                \"Planetoid\": \"64.7%\",                \"ICA\": \"69.1%\",                \"DeepWalk\": \"43.2%\",                \"LP\": \"45.3%\",                \"MLP\": \"46.5%\",                \"SemiEmb\": \"59.6%\",                \"ManiReg\": \"60.1%\"            },            \"Pubmed\": {                \"GAT\": \"79.0±0.3%\",                \"GCN\": \"79.0%\",                \"MoNet\": \"78.8±0.3%\",                \"Chebyshev\": \"74.4%\",                \"Planetoid\": \"77.2%\",                \"ICA\": \"73.9%\",                \"DeepWalk\": \"65.3%\",                \"LP\": \"63.0%\",                \"MLP\": \"71.4%\",                \"SemiEmb\": \"71.7%\",                \"ManiReg\": \"70.7%\"            },            \"PPI\": {                \"GAT\": \"97.3±0.2%\",                \"Const-GAT\": \"93.4±0.6%\",                \"GraphSAGE\": \"76.8%\",                \"GraphSAGE-GCN\": \"50.0%\",                \"GraphSAGE-mean\": \"59.8%\",                \"GraphSAGE-LSTM\": \"61.2%\",                \"GraphSAGE-pool\": \"60.0%\",                \"MLP\": \"42.2%\",                \"Random\": \"39.6%\"            }        }    }}"
}