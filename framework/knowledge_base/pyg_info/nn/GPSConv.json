{
    "name": "GPSConv",
    "description": "The general, powerful, scalable (GPS) graph transformer layer from the \"Recipe for a General, Powerful, Scalable Graph Transformer\" paper.",
    "link": "../generated/torch_geometric.nn.conv.GPSConv.html#torch_geometric.nn.conv.GPSConv",
    "paper_link": "https://arxiv.org/abs/2205.12454",
    "paper_name": "\"Recipe for a General, Powerful, Scalable Graph Transformer\"",
    "Local Paper PDF Path": "knowledge_base\\pyg_info\\nn\\GPSConv.pdf",
    "Paper Summary": "{    \"GNN_Design\": {        \"agg_ops\": \"The model uses a combination of MPNN sum aggregations and Global Attention mechanisms without edge features.\",        \"skip_connections\": \"Residual connections with batch normalization apply after the MPNN and global attention blocks; GINE and PNA include an inner skip connection.\",        \"layer_info_fusion\": \"Node features are encoded by local MPNN layers first, followed by implicit feature inference via global attention layers using fused node representations.\",        \"num_layers\": \"Configurable, with examples using 3 to 10 layers across datasets.\",        \"hyperparameters\": \"Includes learning rate (0.0001 to 0.001), weight decay (1e-5), dropout ratios and batch sizes that vary per dataset.\",        \"activation\": \"ReLU activations within the 2-layer MLP block with inner hidden dimension twice the layer-input feature dimensionality.\"    },    \"Experimental_Setup\": {        \"datasets\": [            \"ZINC\",            \"MNIST\",            \"CIFAR10\",            \"PATTERN\",            \"CLUSTER\",            \"ogbg-molhiv\",            \"ogbg-molpcba\",            \"ogbg-ppa\",            \"ogbg-code2\",            \"PCQM4Mv2\",            \"MalNet-Tiny\",            \"PascalVOC-SP\",            \"COCO-SP\",            \"PCQM-Contact\",            \"Peptides-func\",            \"Peptides-struct\"        ],        \"dataset_summary\": {            \"ZINC\": \"Molecular graph dataset for regression of constrained solubility.\",            \"MNIST\": \"10-class superpixel graph classification dataset from MNIST images.\",            \"CIFAR10\": \"10-class superpixel graph classification from CIFAR-10 images.\",            \"PATTERN\": \"Node classification with stochastic block model patterns.\",            \"CLUSTER\": \"Node classification with nodes belonging to defined clusters.\",            \"ogbg-molhiv\": \"Binary classification of molecular graphs for HIV inhibition activity.\",            \"ogbg-molpcba\": \"Large-scale multi-task classification for bioassay activity against 128 targets.\",            \"ogbg-ppa\": \"Classification of taxonomic group of protein-protein association networks.\",            \"ogbg-code2\": \"Sequence prediction of first 5 subtokens of function names from source code ASTs.\",            \"PCQM4Mv2\": \"Regression of quantum property (HOMO-LUMO gap) on molecular graphs.\",            \"MalNet-Tiny\": \"Classification of function call graphs representing different software types.\",            \"PascalVOC-SP\": \"Superpixel object class prediction in the Pascal VOC dataset.\",            \"COCO-SP\": \"Superpixel object class prediction in the COCO dataset.\",            \"PCQM-Contact\": \"Link prediction in 3D molecular contact graphs.\",            \"Peptides-func\": \"Graph classification for peptide functional classes.\",            \"Peptides-struct\": \"Regression of structural properties of peptides.\"        },        \"baseline\": [            \"GCN\",            \"GIN\",            \"PNA\",            \"SAN\",            \"Graphormer\",            \"K-Subgraph SAT\",            \"EGT\"        ],        \"performance_comparisons\": {            \"results\": {                \"ZINC\": \"GPS achieved an MAE of 0.070, outperforming all baselines.\",                \"MNIST\": \"98.05% accuracy, second overall.\",                \"CIFAR10\": \"72.30% accuracy, first overall.\",                \"PATTERN\": \"Comparable accuracy to state-of-the-art\",                \"CLUSTER\": \"78.02% accuracy, second overall.\",                \"ogbg-molhiv\": \"AUROC of 0.7880, consistent with top models.\",                \"ogbg-molpcba\": \"Average precision of 0.2907, top tier performance.\",                \"ogbg-ppa\": \"Accuracy of 0.8015, best result.\",                \"ogbg-code2\": \"F1 score of 0.1894, competitive with best models.\"            }        }    }}"
}