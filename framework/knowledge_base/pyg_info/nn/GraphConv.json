{
    "name": "GraphConv",
    "description": "The graph neural network operator from the \"Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks\" paper.",
    "link": "../generated/torch_geometric.nn.conv.GraphConv.html#torch_geometric.nn.conv.GraphConv",
    "paper_link": "https://arxiv.org/abs/1810.02244",
    "paper_name": "\"Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks\"",
    "Local Paper PDF Path": "knowledge_base\\pyg_info\\nn\\GraphConv.pdf",
    "Paper Summary": "{    'GNN_Design': {        'agg_ops': 'The aggregation operations are based on message passing frameworks similar to neural message passing from Gilmer et al. (2017). It is mentioned that one can replace the sum over the neighborhood with a permutation-invariant, differentiable function.',        'skip_connections': 'There is no explicit mention of skip connections within the layers of the k-GNN architecture.',        'layer_info_fusion': 'The architecture supports hierarchical message passing where k-GNNs use features from (k-1)-dimensional GNNs, allowing them to capture graph structures at multiple scales. This is achieved by using the features learned by (k-1)-GNNs as inputs.',        'num_layers': 'For the experiments, three layers were used for 1-GNN, two layers for local 2-GNN and 3-GNN.',        'hyperparameters': 'Hidden dimension size of 64 for each layer. No other specific hyperparameters like learning rate or regularization parameters are mentioned for the model setup.',        'activation': 'The k-GNN uses ReLU as the activation function, though the proof uses a sign-based activation function.'    },    'Experimental_Setup': {        'datasets': 'PROTEINS, IMDB-BINARY, IMDB-MULTI, PTC-FM, NCI1, MUTAG, PTC-MR, and QM9 datasets.',        'dataset_summary': 'Each dataset varies in size and domain. PROTEINS, IMDB-BINARY, and IMDB-MULTI are graph classification datasets. QM9 is a quantum chemistry dataset containing about 133,000 small molecules with regression targets for molecular properties.',        'baseline': 'The baselines include various graph kernels such as Graphlet Kernel, Shortest-Path Kernel, 1-WL, 2-WL, 3-WL, and Weisfeiler-Lehman Optimal Assignment (WL-OA) kernel. Neural baselines include DCNN, PATCHYSAN, DGCNN, and standard 1-GNN.',        'performance_comparisons': 'The k-GNN architecture showed consistent improvements over the 1-GNN architecture across twelve QM9 regression tasks, reducing the mean absolute error by significant percentages such as 65.3%, 85.5%, 98.5%, and others for different tasks. The hierarchical k-GNNs also performed better or on par with state-of-the-art graph kernels on various benchmark datasets.'    }}"
}