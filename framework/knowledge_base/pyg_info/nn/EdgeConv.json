{
    "name": "EdgeConv",
    "description": "The edge convolutional operator from the \"Dynamic Graph CNN for Learning on Point Clouds\" paper.",
    "link": "../generated/torch_geometric.nn.conv.EdgeConv.html#torch_geometric.nn.conv.EdgeConv",
    "paper_link": "https://arxiv.org/abs/1801.07829",
    "paper_name": "\"Dynamic Graph CNN for Learning on Point Clouds\"",
    "Local Paper PDF Path": "knowledge_base\\pyg_info\\nn\\EdgeConv.pdf",
    "Paper Summary": "{    \"GNN_Design\": {        \"agg_ops\": \"EdgeConv uses channel-wise symmetric aggregation, typically sum or max.\",        \"skip_connections\": \"Shortcut connections are included to extract multi-scale features.\",        \"layer_info_fusion\": \"Fusion occurs through concatenation in the final layer and uses global max/sum pooling for global feature aggregation.\",        \"num_layers\": \"Four EdgeConv layers are used for feature extraction.\",        \"hyperparameters\": \"Number of nearest neighbors (k) is typically 20, or 40 for higher point counts. Batch size is 32.\",        \"activation\": \"Leaky ReLU is used in all layers with batch normalization. Dropout is applied with a keep probability of 0.5.\"    },    \"Experimental_Setup\": {        \"datasets\": [\"ModelNet40\", \"ShapeNetPart\", \"S3DIS\"],        \"dataset_summary\": {            \"ModelNet40\": \"12,311 3D CAD models across 40 categories; 9,843 for training and 2,468 for testing.\",            \"ShapeNetPart\": \"16,881 3D shapes from 16 categories, annotated with 50 parts, with 2,048 points sampled per shape.\",            \"S3DIS\": \"Stanford 3D Indoor Spaces dataset with 272 rooms across 6 areas, labeled with 13 categories.\"        },        \"baseline\": [\"PointNet\", \"PointNet++\", \"Kd-Net\", \"PCNN\", \"PointCNN\"],        \"performance_comparisons\": {            \"ModelNet40\": {                \"Ours\": \"92.9% accuracy with dynamic graph recomputation outperforming PointNet++ (90.7%) and PCNN (92.3%).\",                \"Baseline\": \"91.7% accuracy with fixed graph.\"            },            \"ShapeNetPart\": {                \"Ours\": \"86.7% mIoU, competitive with PointCNN (86.1%).\"            },            \"S3DIS\": {                \"Ours\": \"56.1% mIoU and 84.1% accuracy, surpassing PointNet (47.6% mIoU) and achieving near-PointCNN performance.\"            }        }    }}"
}