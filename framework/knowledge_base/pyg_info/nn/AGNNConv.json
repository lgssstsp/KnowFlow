{
    "name": "AGNNConv",
    "description": "The graph attentional propagation layer from the \"Attention-based Graph Neural Network for Semi-Supervised Learning\" paper.",
    "link": "../generated/torch_geometric.nn.conv.AGNNConv.html#torch_geometric.nn.conv.AGNNConv",
    "paper_link": "https://arxiv.org/abs/1803.03735",
    "paper_name": "\"Attention-based Graph Neural Network for Semi-Supervised Learning\"",
    "Local Paper PDF Path": "knowledge_base\\pyg_info\\nn\\AGNNConv.pdf",
    "Paper Summary": "{    \"GNN_Design\": {        \"agg_ops\": \"Dynamic and adaptive propagation layers using an attention mechanism to weigh contributions from neighbors. Propagation matrix P(t) uses a softmax over cosine similarity with a shared scalar parameter β(t) to dynamically adjust relevance.\",        \"skip_connections\": \"No explicit mention of standard skip connections, but self-loops are added to ensure node features and hidden states are retained during propagation.\",        \"layer_info_fusion\": \"Information is fused across layers using attention-guided propagation with dynamic updates to the propagation matrix based on cosine similarities.\",        \"num_layers\": \"Models use up to 4 propagation layers depending on the dataset, e.g., 4-layer models for CiteSeer and PubMed, and 2-layer model for Cora.\",        \"hyperparameters\": \"Propagator layers (l = 4 for CiteSeer, PubMed; l = 3 for random splits in Cora), scalar parameter β(t), dropout rate at first and last layers, varying learning rates for datasets (e.g., 0.005 for CiteSeer, 0.008 for PubMed).\",        \"activation\": \"ReLU activation used after embedding in the initial layer, softmax used in propagation layers to ensure attention row-sums to one.\"    },    \"Experimental_Setup\": {        \"datasets\": [\"CiteSeer\", \"Cora\", \"PubMed\"],        \"dataset_summary\": \"Citation network datasets consist of documents as nodes with topics, and citation links as edges. Feature vectors vary—binary features for CiteSeer and Cora, TF-IDF for PubMed.\",        \"baseline\": [            \"Single/Multilayer Perceptrons\",            \"T-SVM\",            \"DeepWalk\",            \"node2vec\",            \"LP\",            \"ICA\",            \"ManiReg\",            \"SemiEmb\",            \"Planetoid\",            \"GCN\",            \"Graph-CNN\",            \"DynamicFilter\",            \"Bootstrap\"        ],        \"performance_comparisons\": \"AGNN outperformed baselines with higher accuracies on fixed splits: CiteSeer 71.7%, Cora 83.1%, PubMed 79.9%. For random splits: CiteSeer 69.8%, Cora 81.0%, PubMed 78.0%. In larger training set (Cora), AGNN achieved 89.07% on 3-fold cross validation, outperforming Graph-CNN.\"    }}"
}