{
    "name": "FeaStConv",
    "description": "The (translation-invariant) feature-steered convolutional operator from the \"FeaStNet: Feature-Steered Graph Convolutions for 3D Shape Analysis\" paper.",
    "link": "../generated/torch_geometric.nn.conv.FeaStConv.html#torch_geometric.nn.conv.FeaStConv",
    "paper_link": "https://arxiv.org/abs/1706.05206",
    "paper_name": "\"FeaStNet: Feature-Steered Graph Convolutions for 3D Shape Analysis\"",
    "Local Paper PDF Path": "knowledge_base\\pyg_info\\nn\\FeaStConv.pdf",
    "Paper Summary": "{    \"GNN_Design\": {        \"agg_ops\": \"Dynamic generation of local graph filters using a learned function over preceding layer features; soft-assignment q_m based on comparison of node features.\",        \"skip_connections\": \"No explicit mention of skip-connections, but multi-scale architecture implies some level of feature reuse across scales.\",        \"layer_info_fusion\": \"Single-scale architecture with sequence of linear and graph convolution layers; multi-scale architecture using graph pooling inspired by U-Net to increase the field of view while maintaining resolution.\",        \"num_layers\": \"Single-scale: Lin16 + Conv32 + Conv64 + Conv128 + Lin256 + Lin6890; Multi-scale: Additional pooling/unpooling layers with Graclus clustering for max-pooling.\",        \"hyperparameters\": \"Learning rate: 10^-2, weight decay: 10^-4, k-nearest neighbors graph with k=16, number of weight matrices (M): 32.\",        \"activation\": \"Standard non-linear activations applied point-wise after linear transformations. No specific activation function mentioned, likely ReLU based on common practices.\"    },    \"Experimental_Setup\": {        \"datasets\": [\"FAUST\", \"ShapeNet Part\"],        \"dataset_summary\": {            \"FAUST\": \"100 watertight meshes with 6,890 vertices each, representing 10 shapes in 10 different poses. Ground truth correspondences available. Split: 80 training shapes, 20 test shapes.\",            \"ShapeNet Part\": \"16,881 shapes from 16 categories labeled with 50 parts in total, applied on k-nearest neighbor graphs over sampled 3D points. Standard point cloud protocol.\"        },        \"baseline\": [\"Logistic Regression\", \"PointNet\", \"Anisotropic CNN (ACNN)\", \"Geodesic CNN (GCNN)\", \"Mixure Model Network (MoNet)\"],        \"performance_comparisons\": {            \"Shape Correspondence (FAUST)\": {                \"Ours (single-scale, w/o refinement)\": \"88.1% (XYZ inputs)\",                \"Ours (single-scale, w/ refinement)\": \"92.2% (XYZ inputs)\",                \"Ours (multi-scale, w/o refinement)\": \"98.6% (XYZ inputs)\",                \"Ours (multi-scale, w/ refinement)\": \"98.7% (XYZ inputs)\",                \"Ours (multi-scale, w/o refinement)\": \"90.9% (SHOT inputs)\",                \"MoNet (w/ refinement)\": \"88.2% (SHOT inputs)\",                \"MoNet (w/o refinement)\": \"73.8% (SHOT inputs)\",                \"Prior Methods (other)\": \"Varied performance with PointNet at 49.7% and other methods without detailed results.\"            },            \"Part Labeling (ShapeNet)\": {                \"Ours\": \"81.5% overall mIoU, with specific results across categories e.g., Chair: 87.5%, Laptop: 94.7%\",                \"PointNet\": \"83.7% overall mIoU, best in categories like Motor: 95.3%\",                \"Kd-network\": \"82.3% overall mIoU, best in categories like Motor: 94.9%\",                \"Various\": \"Ranges of performance across classes, with some methods like Yi showing up to 81.0% in categories like aeroplane.\"            }        }    }}"
}