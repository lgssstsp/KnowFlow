{
    "name": "APPNP",
    "description": "The approximate personalized propagation of neural predictions layer from the \"Predict then Propagate: Graph Neural Networks meet Personalized PageRank\" paper.",
    "link": "../generated/torch_geometric.nn.conv.APPNP.html#torch_geometric.nn.conv.APPNP",
    "paper_link": "https://arxiv.org/abs/1810.05997",
    "paper_name": "\"Predict then Propagate: Graph Neural Networks meet Personalized PageRank\"",
    "Local Paper PDF Path": "knowledge_base\\pyg_info\\nn\\APPNP.pdf",
    "Model design and experimental setup": "{    \"GNN_Design\": {        \"agg_ops\": \"The model leverages a new propagation scheme based on personalized PageRank, allowing for infinitely many propagation steps without oversmoothing.\",        \"skip_connections\": \"Not explicitly used in the described model, but the separation of propagation and prediction intrinsically allows flexibility.\",        \"layer_info_fusion\": \"Information is fused using a propagation scheme where predictions generated from node features are propagated via personalized PageRank.\",        \"num_layers\": \"The neural network uses two layers with 64 hidden units.\",        \"hyperparameters\": {            \"teleport_probability\": 0.1 to 0.2 depending on the dataset,            \"dropout_rate\": 0.5 for both layers and adjacency matrix,            \"L2_regularization\": 0.005,            \"number_of_power_iterations\": 10 for APPNP\"        },        \"activation\": \"ReLU is used as the activation function, followed by softmax for final predictions.\"    },    \"Experimental_Setup\": {        \"datasets\": [\"CITESEER\", \"CORA-ML\", \"PUBMED\", \"MICROSOFT ACADEMIC\"],        \"dataset_summary\": {            \"CITESEER\": {                \"type\": \"Citation\",                \"classes\": 6,                \"features\": 3703,                \"nodes\": 2110,                \"edges\": 3668,                \"label_rate\": 0.036,                \"avg_shortest_path\": 9.31            },            \"CORA-ML\": {                \"type\": \"Citation\",                \"classes\": 7,                \"features\": 2879,                \"nodes\": 2810,                \"edges\": 7981,                \"label_rate\": 0.047,                \"avg_shortest_path\": 5.27            },            \"PUBMED\": {                \"type\": \"Citation\",                \"classes\": 3,                \"features\": 500,                \"nodes\": 19717,                \"edges\": 44324,                \"label_rate\": 0.003,                \"avg_shortest_path\": 6.34            },            \"MICROSOFT ACADEMIC\": {                \"type\": \"Co-author\",                \"classes\": 15,                \"features\": 6805,                \"nodes\": 18333,                \"edges\": 81894,                \"label_rate\": 0.016,                \"avg_shortest_path\": 5.43            }        },        \"baseline\": [\"GCN\", \"GAT\", \"N-GCN\", \"Jumping Knowledge (JK) Networks\", \"Bootstrapped Feature Propagation (Bt.FP)\"],        \"performance_comparisons\": {            \"CITESEER\": {                \"V.GCN\": \"73.51 \u00b1 0.48\",                \"GCN\": \"75.40 \u00b1 0.30\",                \"N-GCN\": \"74.25 \u00b1 0.40\",                \"GAT\": \"75.39 \u00b1 0.27\",                \"JK\": \"73.03 \u00b1 0.47\",                \"Bt.FP\": \"73.55 \u00b1 0.57\",                \"PPNP\": \"75.83 \u00b1 0.27\",                \"APPNP\": \"75.73 \u00b1 0.30\"            },            \"CORA-ML\": {                \"V.GCN\": \"82.30 \u00b1 0.34\",                \"GCN\": \"83.41 \u00b1 0.39\",                \"N-GCN\": \"82.25 \u00b1 0.30\",                \"GAT\": \"84.37 \u00b1 0.24\",                \"JK\": \"82.69 \u00b1 0.35\",                \"Bt.FP\": \"80.84 \u00b1 0.97\",                \"PPNP\": \"85.29 \u00b1 0.25\",                \"APPNP\": \"85.09 \u00b1 0.25\"            },            \"PUBMED\": {                \"V.GCN\": \"77.65 \u00b1 0.40\",                \"GCN\": \"78.68 \u00b1 0.38\",                \"N-GCN\": \"77.43 \u00b1 0.42\",                \"GAT\": \"77.76 \u00b1 0.44\",                \"JK\": \"77.88 \u00b1 0.38\",                \"Bt.FP\": \"72.94 \u00b1 1.00\",                \"PPNP\": \"-\",                \"APPNP\": \"79.73 \u00b1 0.31\"            },            \"MICROSOFT ACADEMIC\": {                \"V.GCN\": \"91.65 \u00b1 0.09\",                \"GCN\": \"92.10 \u00b1 0.08\",                \"N-GCN\": \"92.86 \u00b1 0.11\",                \"GAT\": \"91.22 \u00b1 0.07\",                \"JK\": \"91.71 \u00b1 0.10\",                \"Bt.FP\": \"91.61 \u00b1 0.24\",                \"PPNP\": \"-\",                \"APPNP\": \"93.27 \u00b1 0.08\"            }        }    }}",
    "Paper Summary": "The paper discusses a novel model called Personalized Propagation of Neural Predictions (PPNP) and its fast approximation, APPNP, which aim to address the limitations of existing message passing algorithms like Graph Convolutional Networks (GCNs) in semi-supervised node classification tasks. \n\n### Model Design Aspects:\n\n1. **Connection to PageRank**: The authors leverage the connection between GCNs and PageRank to create a new propagation scheme, drawing upon the personalization aspect of PageRank, which allows for a teleported return to the root node, thus retaining local neighborhood information while aggregating data from a larger neighborhood.\n\n2. **Propagation Scheme**: PPNP utilizes a propagation mechanism derived from personalized PageRank. The model's key equation involves solving a recurrent equation that incorporates a teleport vector to preserve the influence dynamics of the root node within its local context:\n   \\[\n   \\pi^{(i)} = (1 - \\alpha) \\hat{A} \\pi^{(i)} + \\alpha i\n   \\]\n   By choosing different values of the teleport probability (\\(\\alpha\\)), the model can adjust how much local versus distant neighborhood information is utilized.\n\n3. **Model Decoupling**: PPNP separates the neural network used for prediction from the propagation scheme. The predictions for each node are first generated from its features using a neural network, which allows for the application of different neural architectures:\n   \\[\n   Z = \\text{softmax}\\left(\\alpha \\left(I - (1 - \\alpha) \\hat{A}\\right)^{-1} H\\right)\n   \\]\n   This design means additional propagation steps do not necessitate increasing the depth of the neural network, enabling the use of infinitely many neighborhood aggregation layers without facing the oversmoothing problem.\n\n4. **Computational Efficiency**: The authors highlight that PPNP methods are computationally efficient compared to na\u00efve implementations, achieving linear complexity relative to the number of edges in the graph. They accomplish this by avoiding the need to compute a dense \\( R^{n \\times n} \\) personalized PageRank matrix, instead approximating it via iterative updates, similar to topic-sensitive PageRank, which retains the graph's sparsity.\n\n5. **Approximate Personalized Propagation**: The APPNP model implements an iterative approach to the propagation of predictions that allows for computational scalability. The iterative update rule calculates the influence of node predictions on the resulting decision set, ensuring logarithmic complexity by leveraging the sparse structure of graphs:\n   \\[\n   Z^{(k+1)} = (1 - \\alpha) \\hat{A} Z^{(k)} + \\alpha H\n   \\]\n\n6. **Parameter Efficiency**: Both PPNP and APPNP are designed to utilize fewer parameters than traditional GCNs, as the propagation scheme does not require additional learnable parameters for each aggregation step, enhancing training speed and model scalability.\n\n7. **Hyperparameter Flexibility**: The teleport probability (\\(\\alpha\\)) acts as a hyperparameter allowing fine-tuning of the model for various graph structures, making it adaptable to the distinct characteristics of different datasets or problem domains. \n\nThis approach not only improves performance on semi-supervised classification tasks but also provides a framework for combining with various advanced neural network models without being constrained by traditional message passing architectures."
}